<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Solon Barocas">
  <meta name="author" content="Moritz Hardt">
  <meta name="author" content="Arvind Narayanan">
  <title>Classification</title>
  <link rel="stylesheet" href="style.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
    var mathElements = document.getElementsByClassName("math");
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") { katex.render(texText.data, mathElements[i], { displayMode: mathElements[i].classList.contains("display"), throwOnError: false } );
    }}});</script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article>
<header>
<div id="chapter">Chapter 2</div>
<h1 class="title">Classification</h1>
</header>
<nav id="TOC">
<div id="menuToggle">
<a class="toggle" tabindex="0" ><img src="assets/hamburger_icon.svg" alt="hamburger" id="hamburger"></a>
<a id="index" href="index.html"><img src="assets/home.png" alt="home" id="home"></a>
<ul>
<li><a href="#supervised-learning">Supervised learning</a><ul>
<li><a href="#statistical-classification-criteria">Statistical classification criteria</a></li>
<li><a href="#score-functions">Score functions</a></li>
<li><a href="#the-conditional-expectation">The conditional expectation</a></li>
<li><a href="#from-scores-to-classifiers">From scores to classifiers</a></li>
</ul></li>
<li><a href="#sensitive-characteristics">Sensitive characteristics</a><ul>
<li><a href="#no-fairness-through-unawareness">No fairness through unawareness</a></li>
</ul></li>
<li><a href="#formal-non-discrimination-criteria">Formal non-discrimination criteria</a><ul>
<li><a href="#independence">Independence</a></li>
<li><a href="#limitations-of-independence">Limitations of independence</a></li>
<li><a href="#interlude-how-to-satisfy-fairness-criteria">Interlude: How to satisfy fairness criteria</a></li>
<li><a href="#separation">Separation</a></li>
<li><a href="#achieving-separation">Achieving separation</a></li>
<li><a href="#sufficiency">Sufficiency</a></li>
</ul></li>
<li><a href="#calibration-and-sufficiency">Calibration and sufficiency</a><ul>
<li><a href="#calibration-by-group">Calibration by group</a></li>
<li><a href="#calibration-by-group-as-a-consequence-of-unconstrained-learning">Calibration by group as a consequence of unconstrained learning</a></li>
</ul></li>
<li><a href="#relationships-between-criteria">Relationships between criteria</a><ul>
<li><a href="#independence-versus-sufficiency">Independence versus Sufficiency</a></li>
<li><a href="#independence-versus-separation">Independence versus Separation</a></li>
<li><a href="#separation-versus-sufficiency">Separation versus Sufficiency</a></li>
</ul></li>
<li><a href="#inherent-limitations-of-observational-criteria">Inherent limitations of observational criteria</a><ul>
<li><a href="#scenario-i">Scenario I</a></li>
<li><a href="#scenario-ii">Scenario II</a></li>
<li><a href="#different-interpretations">Different interpretations</a></li>
<li><a href="#indistinguishability">Indistinguishability</a></li>
<li><a href="#a-forced-perspective-problem">A forced perspective problem</a></li>
</ul></li>
<li><a href="#case-study-credit-scoring">Case study: Credit scoring</a><ul>
<li><a href="#score-distribution">Score distribution</a></li>
<li><a href="#performance-variables-and-roc-curves">Performance variables and ROC curves</a></li>
<li><a href="#comparison-of-different-criteria">Comparison of different criteria</a></li>
<li><a href="#calibration-values">Calibration values</a></li>
</ul></li>
<li><a href="#problem-set-criminal-justice-case-study">Problem set: Criminal justice case study</a><ul>
<li><a href="#calibrationsufficiency">Calibration/sufficiency</a></li>
<li><a href="#error-ratesseparation">Error rates/separation</a></li>
<li><a href="#risk-factors-and-interventions">Risk factors and interventions</a></li>
</ul></li>
<li><a href="#problem-set-data-modeling-of-traffic-stops">Problem set: Data modeling of traffic stops</a><ul>
<li><a href="#stop-rates">Stop rates</a></li>
<li><a href="#post-stop-outcomes">Post-stop outcomes</a></li>
<li><a href="#data-quality">Data quality</a></li>
</ul></li>
<li><a href="#what-is-the-purpose-of-a-fairness-criterion">What is the purpose of a fairness criterion?</a></li>
<li><a href="#bibliographic-notes-and-further-reading">Bibliographic notes and further reading</a><ul>
<li><a href="#a-dictionary-of-criteria">A dictionary of criteria</a></li>
</ul></li>
<li><a href="#bibliography">References</a></li>
</ul>
</div>
</nav>
<a class="cheatsheet" tabindex="1"><img src="assets/cheatsheet_icon.png" alt="cheatsheet" id="cheatsheet_icon"></a>
<div class="onclick-menu" id="content">
    <div class="onclick-menu-content" id="variables"> <span class="math inline">\(R\)</span>: score &ensp; <span class="math inline">\(Y\)</span>: target variable &ensp; <br />
    <span class="math inline">\(A\)</span>: sensitive attribute &ensp; <span class="math inline">\(\hat Y\)</span>: prediction </div>
    <table class="onclick-menu-content">
    <thead>
    <tr class="header">
    <th style="text-align: center;">Independence</th>
    <th style="text-align: center;">Separation</th>
    <th style="text-align: center;">Sufficiency</th>
    </tr>
    </thead>
    <tbody>
    <tr class="odd">
    <td style="text-align: center;"><span class="math inline">\(R\bot A\)</span></td>
    <td style="text-align: center;"><span class="math inline">\(R\bot A \mid Y\)</span></td>
    <td style="text-align: center;"><span class="math inline">\(Y\bot A\mid R\)</span></td>
    </tr>
    </tbody>
    </table>
    <ul class="onclick-menu-content">
      <li>Notion = <span class="math inline">\(\mathbb{P}\{\mathrm{event}\mid \mathrm{condition}\}\)</span></li>
    </ul>
    <table class="onclick-menu-content">
    <thead>
    <tr class="header">
    <th style="text-align: center;">Event</th>
    <th style="text-align: center;">Condition</th>
    <th style="text-align: left;">Notion</th>
    </tr>
    </thead>
    <tbody>
    <tr class="odd">
    <td style="text-align: center;"><span class="math inline">\(\hat Y=1\)</span></td>
    <td style="text-align: center;"><span class="math inline">\(Y=1\)</span></td>
    <td style="text-align: left;">True positive rate, recall</td>
    </tr>
    <tr class="even">
    <td style="text-align: center;"><span class="math inline">\(\hat Y=0\)</span></td>
    <td style="text-align: center;"><span class="math inline">\(Y=1\)</span></td>
    <td style="text-align: left;">False negative rate</td>
    </tr>
    <tr class="odd">
    <td style="text-align: center;"><span class="math inline">\(\hat Y=1\)</span></td>
    <td style="text-align: center;"><span class="math inline">\(Y=0\)</span></td>
    <td style="text-align: left;">False positive rate</td>
    </tr>
    <tr class="even">
    <td style="text-align: center;"><span class="math inline">\(\hat Y=0\)</span></td>
    <td style="text-align: center;"><span class="math inline">\(Y=0\)</span></td>
    <td style="text-align: left;">True negative rate</td>
    </tr>
    <tr class="odd">
    <td style="text-align: center;"><span class="math inline">\(Y=1\)</span></td>
    <td style="text-align: center;"><span class="math inline">\(\hat Y=1\)</span></td>
    <td style="text-align: left;">Positive predictive value, precision</td>
    </tr>
    <tr class="even">
    <td style="text-align: center;"><span class="math inline">\(Y=0\)</span></td>
    <td style="text-align: center;"><span class="math inline">\(\hat Y=0\)</span></td>
    <td style="text-align: left;">Negative predictive value</td>
    </tr>
    </tbody>
    </table>
    <br/>
    </div>
</div>
<p>Simply put, the goal of classification is to determine a plausible value for an unknown variable <span class="math inline">Y</span> given an observed variable <span class="math inline">X</span>. For example, we might try to <em>predict</em> whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable <span class="math inline">Y</span> does not refer to an event that lies in the future. For example, we can try to determine if an image contains a <em>cat</em> by looking at the set of pixels encoding the image. This practice is also called <em>object recognition</em> or <em>image classification</em>. Object recognition might not even seem like a statistical problem, yet statistical methods came to be the method of choice for many important pattern recognition tasks in computer vision.</p>
<section id="supervised-learning" class="level1">
<h1>Supervised learning</h1>
<p>A classifier is a mapping from the space of possible values for <span class="math inline">X</span> to the space of values that the target variable <span class="math inline">Y</span> can assume. <em>Supervised learning</em> is the prevalent method for constructing classifiers from observed data. The essential idea is very simple. Suppose we have labeled data, also called <em>training examples</em>, of the form <span class="math inline">(x_1,y_1), ..., (x_n, y_n),</span> where each <em>example</em> is a pair <span class="math inline">(x_i,y_i)</span> of an <em>instance</em> <span class="math inline">x_i</span> and a <em>label</em> <span class="math inline">y_i.</span></p>
<p>Instances are usually arranged as vectors of some dimension. You can think of them as arrays with numbers in them. In a classification problem, labels typically come from a discrete set such as <span class="math inline">\{-1,1\}</span> in the case of binary classification. We interpret these labels as partitioning the set of instances into positive and negative instances depending on their label.<span><label for="sn-0" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-0" class="margin-toggle" /><span class="sidenote">Multi-class prediction is the generalization to label sets with more than two values.</span></span> We can interpret such a classifier as a <em>decision rule</em> by equating a positive label with <em>acceptance</em> and a negative label with <em>rejection</em>.</p>
<p>In a <em>regression</em> problem, the label <span class="math inline">y</span> is typically a real number. The goal is no longer to predict the exact value of <span class="math inline">y</span> but rather to be close to it. The tools to solve classification and regression problems in practice are very similar. In both cases, roughly the same optimization approach is used to find a classifier <span class="math inline">f</span> that maps an instance <span class="math inline">x</span> to a label <span class="math inline">\hat y=f(x)</span> that we hope agrees with the correct label. This optimization process is often called <em>training</em>; its specifics are irrelevant for this chapter.</p>
<p>To turn supervised learning into a statistical problem, we assume that there is an underlying distribution from which the data were drawn. The distribution is fixed and each example is drawn independently of the others. We can express this underlying distribution as a pair of random variables <span class="math inline">(X, Y)</span>. For example, our training examples might be responses from a survey. Each survey participant is chosen independently at random from a fixed sampling frame that represents an underlying population. As we discussed in the introduction, the goal of supervised learning is to identify meaningful patterns in the population that aren’t just artifacts of the sample.</p>
<p>At the population level, we can interpret our classifier as a random variable by considering <span class="math inline">\hat Y=f(X)</span>. In doing so, we overload our terminology slightly by using the word <em>classifier</em> for both the random variable <span class="math inline">\hat Y</span> and mapping <span class="math inline">f.</span> The distinction is mostly irrelevant for this chapter as we will focus on the statistical properties of the joint distribution of the data and the classifier, which we denote as a tuple of three random variables <span class="math inline">(X,Y,\hat Y)</span>. For now, we ignore how <span class="math inline">\hat Y</span> was learned from a finite sample, what the functional form of the classifier is, and how we estimate various statistical quantities from finite samples. While finite sample considerations are fundamental to machine learning, they are often not specific to the conceptual and technical questions around fairness that we will discuss.</p>
<section id="statistical-classification-criteria" class="level2">
<h2>Statistical classification criteria</h2>
<p>What makes a classifier <em>good</em> for an application and how do we choose one out of many possible classifiers? This question often does not have a fully satisfying answer, but some formal criteria can help highlight different qualities of a classifier that can inform our choice.</p>
<p>Perhaps the most well known property of a classifier <span class="math inline">\hat Y</span> is its <em>accuracy</em> defined as <span class="math inline">\mathbb{P}\{Y=\hat Y\},</span> the probability of correctly predicting the target variable. It is common practice to apply the classifier that achieves highest accuracy among those available to us.<span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle" /><span class="sidenote">We typically don’t know the classifier that maximizes accuracy among all possible classifiers, but rather we only have access to those that we can find with effective training procedures.</span></span></p>
<p>Accuracy is easy to define, but misses some important aspects. A classifier that always predicts <em>no traffic fatality in the next year</em> might have high accuracy, simply because individual accidents are highly unlikely. However, it’s a constant function that has no value in assessing the risk that an individual experiences a fatal traffic accident.</p>
<p>Many other formal classification criteria highlight different aspects of a classifier. In a binary classification setting, we can consider the conditional probability <span class="math inline">\mathbb{P}\{\mathrm{event}\mid \mathrm{condition}\}</span> for various different settings.</p>
<table>
<caption>Common classification criteria <span label="table:classification-1"></span></caption>
<thead>
<tr class="header">
<th style="text-align: center;">Event</th>
<th style="text-align: center;">Condition</th>
<th style="text-align: left;">Resulting notion (<span class="math inline">\mathbb{P}\{\mathrm{event}\mid \mathrm{condition}\}</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\hat Y=1</span></td>
<td style="text-align: center;"><span class="math inline">Y=1</span></td>
<td style="text-align: left;">True positive rate, recall</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\hat Y=0</span></td>
<td style="text-align: center;"><span class="math inline">Y=1</span></td>
<td style="text-align: left;">False negative rate</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\hat Y=1</span></td>
<td style="text-align: center;"><span class="math inline">Y=0</span></td>
<td style="text-align: left;">False positive rate</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\hat Y=0</span></td>
<td style="text-align: center;"><span class="math inline">Y=0</span></td>
<td style="text-align: left;">True negative rate</td>
</tr>
</tbody>
</table>
<p>To be clear, the true positive rate corresponds to the frequency with which the classifier correctly assigns a positive label to a positive instance. We call this a <em>true positive</em>. The other terms <em>false positive</em>, <em>false negative</em>, and <em>true negative</em> derive analogously from the respective definitions.</p>
<p>It is not important to memorize all these terms. They do, however, come up regularly in the classification setting so the table might come in handy.</p>
<p>Another family of classification criteria arises from swapping event and condition. We’ll only highlight two of the four possible notions.</p>
<table>
<caption>Additional classification criteria<span label="table:classification-2"></span></caption>
<thead>
<tr class="header">
<th style="text-align: center;">Event</th>
<th style="text-align: center;">Condition</th>
<th style="text-align: left;">Resulting notion (<span class="math inline">\mathbb{P}\{\mathrm{event}\mid\mathrm{condition}\}</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">Y=1</span></td>
<td style="text-align: center;"><span class="math inline">\hat Y=1</span></td>
<td style="text-align: left;">Positive predictive value, precision</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">Y=0</span></td>
<td style="text-align: center;"><span class="math inline">\hat Y=0</span></td>
<td style="text-align: left;">Negative predictive value</td>
</tr>
</tbody>
</table>
<p>We’ll return to these criteria later on when we explore some of their properties and relationships.</p>
</section>
<section id="score-functions" class="level2">
<h2>Score functions</h2>
<p>Classification is often attacked by first solving a regression problem to summarize the data in a single real-valued variable. We will refer to such a variable as <em>score</em>. We can turn a score into a classifier by thresholding it somewhere on the real line.</p>
<p>For an illustrative example consider the well-known <a href="https://en.wikipedia.org/wiki/Body_mass_index">body mass index</a> which summarizes <em>weight</em> and <em>height</em> of a person into a single real number. In our formal notation, the features are <span class="math inline">X=(H, W)</span> where <span class="math inline">H</span> denotes height in meters and <span class="math inline">W</span> denotes weight in kilograms. The body mass index corresponds to the score function <span class="math inline">R=W/H^2.</span></p>
<figure>
<img src="assets/bmi1.svg" style="width:75.0%" alt="" /><figcaption>Plot of the body mass index.</figcaption>
</figure>
<p>We could interpret the body mass index as measuring risk of heart disease. Thresholding it at the value <em>27</em>, we might decide that individuals with a body mass index above this value are at risk of developing heart disease while others are not. It does not take a medical degree to suspect that the resulting classifier may not be very accurate<span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle" /><span class="sidenote">In fact, it seems to be <a href="https://www.thelancet.com/article/S0140-6736(06)69251-9/abstract">quite poor</a>.</span></span>. The body mass index has a number of known issues leading to errors when used for classification. We won’t go into detail, but it’s worth noting that these classification errors can systematically align with certain demographic groups. For instance, the body mass index tends to be inflated as a risk measure for taller people (due to its <a href="https://en.wikipedia.org/wiki/Body_mass_index#Scaling">scaling issues</a>).</p>
<p>Score functions need not follow simple algebraic formulas such as the body mass index. In most cases, score functions are built by fitting regression models against historical data. Think of a credit score, as is common in some countries, which can be used to accept or deny loan applicants based on the score value. We will revisit this example in detail later.</p>
</section>
<section id="the-conditional-expectation" class="level2">
<h2>The conditional expectation</h2>
<p>A natural score function is the expectation of the target variable <span class="math inline">Y</span> conditional on the features <span class="math inline">X</span> we have observed. We can write this score as <span class="math inline">R=r(X)</span> where <span class="math inline">r(x)=\mathbb{E}[Y\mid X=x],</span> or more succinctly, <span class="math inline">R=\mathbb{E}[Y\mid X].</span> In a sense, this score function gives us the <em>best guess</em> for the target variable given the observations we have. We can think of the conditional expectation as a <em>lookup table</em> that gives us for each setting of features the frequency of positive outcomes given these features.<span><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle" /><span class="sidenote">We can make this statement more precise. This score is sometimes called the <em>Bayes optimal score</em> or <em>Bayes optimal score</em> as it minimizes the squared error <span class="math inline">\mathbb{E}(g(X)-R)^2</span> among all functions <span class="math inline">g(X)</span>.</span></span></p>
<p>Such lookup tables have a long and fascinating history in applications of risk assessment such as insurance pricing.<span><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle" /><span class="sidenote">D. Bouk, <em>How Our Days Became Numbered: Risk and the Rise of the Statistical Individual</em> (University of Chicago Press, 2015).</span></span> One of the earliest examples is Halley’s <em>life table</em> from 1693 that was used to estimate the life expectancy an individual in order to accurately price certain annuities.</p>
<figure>
<img src="assets/halley_life_table_1693.png" style="width:75.0%" alt="" /><figcaption>Halley’s life table (1693)</figcaption>
</figure>
<p>The conditional expectation also makes sense for our example of scoring risk of heart disease. What it would do here is to tell us for every setting of weight (say, rounded to the nearest kg unit) and every physical height (rounded to the nearest cm unit), the incidence rate of heart disease among individuals with these values of weight and height. The target variable in this case is a binary indicator of heart disease. So, <span class="math inline">r((176, 68))</span> would be the incidence rate of heart disease among individuals who are 1.76m tall and weigh 68kg. Intuitively, we can think of the conditional expectation as a big lookup table of incidence rates given some setting of characteristics.</p>
<p>The conditional expectation is likely more useful as a risk measure of heart disease than the body mass index we saw earlier. After all, the conditional expectation directly reflects the incidence rate of heart disease given the observed characteristics, while the body mass index is a general-purpose summary statistic.</p>
<p>That said, we can still spot a few issues with this score function. First, our definition of target variable was a bit fuzzy, lumping together all sorts of different kinds of heart disease with different characteristics. Second, in order to actually compute the conditional expectation in practice, we would have to collect incidence rate statistics by height and weight. These data points would only tell us about historical incidence rates. The extent to which they can tell us about future cases of heart disease is somewhat unclear. If our data comes from a time where people generally smoked more cigarettes, our statistics might overestimate future incidence rates. There are numerous other features that are relevant for the prediction of heart disease, including age and gender, but they are neglected in our data. We could include these additional features in our data; but as we increase the number of features, estimating the conditional expectation becomes increasingly difficult. Any feature set partitions the population into demographics. The more features we include, the fewer data points we can collect in each subgroup. As a result, the conditional expectation is generally hard to estimate in <em>high-dimensional</em> settings, where we have many attributes.</p>
</section>
<section id="from-scores-to-classifiers" class="level2">
<h2>From scores to classifiers</h2>
<p>We just saw how we can turn a score function into a discrete classifier by discretizing its values into buckets. In the case of a binary classifier, this corresponds to choosing a threshold <span class="math inline">t</span> so that when the score is above <span class="math inline">t</span> our classifier outputs 1 (<em>accept</em>) and otherwise -1 (<em>reject</em>).<span><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle" /><span class="sidenote">The choice of the values 1 and -1 is arbitrary. Any two distinct values will do.</span></span> Each choice of the threshold defines one binary classifier. Which threshold should we choose?</p>
<p>The answer to this question is surprisingly subtle. Roughly speaking, which threshold we choose depends on our notion of utility for the resulting classifier and the problem we’re trying to solve. Our notion of utility could be complex and depend on many different considerations.</p>
<p>In classification, it is common to oversimplify the problem quite a bit by summarizing all considerations of utility with just two numbers: a cost for accepting a negative instance (false positive) and a cost for rejecting a positive instance (false negative). If in our problem we face a high cost for false positives, we want to choose a higher threshold than in other applications where false negatives are costly.</p>
<p>The choice of a threshold and its resulting trade-off between true positive rate and false positive rate can be neatly visualized with the help of an <em>ROC curve</em><span><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle" /><span class="sidenote">ROC stands for <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">receiver operating characteristic</a>.</span></span>. Note that true positive rate equals 1 - false negative rate.</p>
<figure>
<img src="assets/roc_curve_1.svg" style="width:75.0%" alt="" /><figcaption>Example of an ROC curve. Each point on the solid curve is realized by thresholding the score function at some value. The dashed line shows the trade-offs achieved by randomly accepting an instance irrespective of its features with some probability <span class="math inline">p\in[0,1].</span><span label="fig:roc"></span></figcaption>
</figure>
<p>The ROC curve serves another purpose. It can be used to eyeball how predictive our score is of the target variable. A common measure of predictiveness is the area under the curve, which is the probability that a random positive instance gets a score higher than a random negative instance. An area of 1/2 corresponds to random guessing, and an area of 1 corresponds to perfect classification, or more formally, the score equals the target. Known disadvantages<span><label for="sn-7" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-7" class="margin-toggle" /><span class="sidenote">S. Halligan, D.G. Altman, and S. Mallett, “Disadvantages of Using the Area Under the Receiver Operating Characteristic Curve to Assess Imaging Tests: A Discussion and Proposal for an Alternative Approach,” <em>European Radiology</em> 25, no. 4 (April 2015): 932–39.</span></span> make <em>area under the curve</em> a tool that must be interpreted with caution.</p>
</section>
</section>
<section id="sensitive-characteristics" class="level1">
<h1>Sensitive characteristics</h1>
<p>In many classification tasks, the features <span class="math inline">X</span> contain or implicitly encode sensitive characteristics of an individual. We will set aside the letter <span class="math inline">A</span> to designate a discrete random variable that captures one or multiple sensitive characteristics<span><label for="sn-8" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-8" class="margin-toggle" /><span class="sidenote">Note that formally we can always represent any number of discrete sensitive attributes as a single discrete attribute whose support corresponds to each of the possible settings of the original attributes.</span></span>. Different settings of <span class="math inline">A</span> correspond to different groups of the population. This notational choice is not meant to suggest that we can cleanly partition the set of features into two independent categories such as “neutral” and “sensitive”. In fact, we will see shortly that sufficiently many seemingly neutral features can often give high accuracy predictions of sensitive characteristics. This should not be surprising. After all, if we think of <span class="math inline">A</span> as the target variable in a classification problem, there is reason to believe that the remaining features would give a non-trivial classifier for <span class="math inline">A.</span></p>
<p>The choice of sensitive attributes will generally have profound consequences as it decides which groups of the population we highlight, and what conclusions we draw from our investigation. The taxonomy induced by discretization can on its own be a source of harm if it is too coarse, too granular, misleading, or inaccurate. Even the act of introducing a sensitive attribute on its own can be problematic. We will revisit this important discussion in the next chapter.</p>
<section id="no-fairness-through-unawareness" class="level2">
<h2>No fairness through unawareness</h2>
<p>Some have hoped that removing or ignoring sensitive attributes would somehow ensure the impartiality of the resulting classifier. Unfortunately, this practice is usually somewhere on the spectrum between ineffective and harmful.</p>
<p>In a typical data set, we have many features that are slightly correlated with the sensitive attribute. Visiting the website <code>pinterest.com</code>, for example, has a small statistical correlation with being female.<span><label for="sn-9" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-9" class="margin-toggle" /><span class="sidenote">As of August 2017, 58.9% of Pinterest’s users in the United States were female. See <a href="https://www.statista.com/statistics/277759/pinterest-gender-usa/">here</a> (Retrieved 3-27-2018)</span></span></p>
<p>The correlation on its own is too small to predict someone’s gender with high accuracy. However, if numerous such features are available, as is the case in a typical browsing history, the task of predicting gender becomes feasible at high accuracy levels.</p>
<p>In other words, several features that are slightly predictive of the sensitive attribute can be used to build high accuracy classifiers for that attribute.</p>
<figure>
<img src="assets/redundant.svg" alt="" /><figcaption>On the left, we see the distribution of a single feature that differs only very slightly between the two groups. In both groups the feature follows a normal distribution. Only the means are slightly different in each group. Multiple features like this can be used to build a high accuracy group membership classifier. On the right, we see how the accuracy grows as more and more features become available.</figcaption>
</figure>
<p>In large feature spaces sensitive attributes are generally <em>redundant</em> given the other features. If a classifier trained on the original data uses the sensitive attribute and we remove the attribute, the classifier will then find a redundant encoding in terms of the other features. This results in an essentially equivalent classifier, in the sense of implementing the same function.</p>
<p>To further illustrate the issue, consider a fictitious start-up that sets out to predict your income from your genome. At first, this task might seem impossible. How could someone’s DNA reveal their income? However, we know that DNA encodes information about ancestry, which in turn correlates with income in some countries such as the United States. Hence, DNA can likely be used to predict income better than random guessing. The resulting classifier uses ancestry in an entirely implicit manner. Removing redundant encodings of ancestry from the genome is a difficult task that cannot be accomplished by removing a few individual genetic markers. What we learn from this is that machine learning can wind up building classifiers for sensitive attributes without explicitly being asked to, simply because it is an available route to improving accuracy.</p>
<p>Redundant encodings typically abound in large feature spaces. What about small hand-curated feature spaces? In some studies, features are chosen carefully so as to be roughly statistically independent of each other. In such cases, the sensitive attribute may not have good redundant encodings. That does not mean that removing it is a good idea. Medication, for example, sometimes depends on race in legitimate ways if these correlate with underlying causal factors.<span><label for="sn-10" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-10" class="margin-toggle" /><span class="sidenote">V.L. Bonham, S.L. Callier, and C.D. Royal, “Will Precision Medicine Move Us Beyond Race?” <em>The New England Journal of Medicine</em> 374, no. 21 (2016): 2003.</span></span> Forcing medications to be uncorrelated with race in such cases can harm the individual.</p>
</section>
</section>
<section id="formal-non-discrimination-criteria" class="level1">
<h1>Formal non-discrimination criteria</h1>
<p>Many <em>fairness criteria</em> have been proposed over the years, each aiming to formalize different desiderata. We’ll start by jumping directly into the formal definitions of three representative fairness criteria that relate to many of the proposals that have been made.</p>
<p>Once we have acquired familiarity with the technical matter, we’ll have a broader debate around the purpose, scope, and meaning of these fairness criteria in Chapter 3.</p>
<p>Most of the proposed fairness criteria are properties of the joint distribution of the sensitive attribute <span class="math inline">A</span>, the target variable <span class="math inline">Y</span>, and the classifier or score <span class="math inline">R.</span><span><label for="sn-11" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-11" class="margin-toggle" /><span class="sidenote">If all variables are binary, then the joint distribution is specified by 8 non-negative parameters that sum to 1. A non-trivial property of the joint distribution would restrict the way in which we can choose these parameters.</span></span> This means that we can write them as some statement involving properties of these three random variables.</p>
<p>To a first approximation, most of these criteria fall into one of three different categories defined along the lines of different (conditional) independence<span><label for="sn-12" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-12" class="margin-toggle" /><span class="sidenote">Learn more about conditional independence <a href="https://en.wikipedia.org/wiki/Conditional_independence">here</a>.</span></span> statements between the involved random variables.</p>
<table>
<caption>Non-discrimination criteria</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Independence</th>
<th style="text-align: center;">Separation</th>
<th style="text-align: center;">Sufficiency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">R\bot A</span></td>
<td style="text-align: center;"><span class="math inline">R\bot A \mid Y</span></td>
<td style="text-align: center;"><span class="math inline">Y\bot A\mid R</span></td>
</tr>
</tbody>
</table>
<p>Below we will introduce and discuss each of these conditions in detail. Variants of these criteria arise from different ways of relaxing them.</p>
<p>As an exercise, think about why we omitted the conditional independence statement <span class="math inline">R\bot Y\mid A</span> from our discussion here.</p>
<section id="independence" class="level2">
<h2>Independence</h2>
<p>Our first formal criterion simply requires the sensitive characteristic to be statistically independent of the score.</p>
<div class="numenv Definition">
<span class="numenv Definition title">Definition 1.</span>
<p>The random variables <span class="math inline">(A, R)</span> satisfy <em>independence</em> if <span class="math inline">A\bot R.</span></p>
</div>
<p>Independence has been explored through many equivalent terms or variants, referred to as <em>demographic parity</em>, <em>statistical parity</em>, <em>group fairness</em>, <em>disparate impact</em> and others. In the case of binary classification, independence simplifies to the condition <span class="math display">\mathbb{P}\{R=1\mid A=a\}=\mathbb{P}\{R=1\mid A=b\}\,,</span> for all groups <span class="math inline">a, b.</span> Thinking of the event <span class="math inline">R=1</span> as “acceptance”, the condition requires the acceptance rate to be the same in all groups. A relaxation of the constraint introduces a positive amount of slack <span class="math inline">\epsilon&gt;0</span> and requires that <span class="math display">\mathbb{P}\{R=1\mid A=a\}\ge \mathbb{P}\{R=1\mid A=b\}-\epsilon\,.</span></p>
<p>Note that we can swap <span class="math inline">a</span> and <span class="math inline">b</span> to get an inequality in the other direction. An alternative relaxation is to consider a ratio condition, such as, <span class="math display">\frac{\mathbb{P}\{R=1\mid A=a\}}
{\mathbb{P}\{R=1\mid A=b\}}\ge1-\epsilon\,.</span> Some have argued<span><label for="sn-13" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-13" class="margin-toggle" /><span class="sidenote">M. Feldman, S.A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian, “Certifying and Removing Disparate Impact,” in <em>Proc. <span class="math inline">21</span>st SIGKDD</em> (ACM, 2015).</span></span> that, for <span class="math inline">\epsilon=0.2,</span> this condition relates to the <em>80 percent rule</em> in disparate impact law.</p>
<p>Yet another way to state the independence condition in full generality is to require that <span class="math inline">A</span> and <span class="math inline">R</span> must have zero mutual information<span><label for="sn-14" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-14" class="margin-toggle" /><span class="sidenote">Mutual information is defined as <span class="math inline">I(A;R)=H(A)+H(R)-H(A,R),</span> where <span class="math inline">H</span> denotes the entropy.</span></span> <span class="math inline">I(A;R)=0.</span> The characterization in terms of mutual information leads to useful relaxations of the constraint. For example, we could require <span class="math inline">I(A;R)\le\epsilon.</span></p>
</section>
<section id="limitations-of-independence" class="level2">
<h2>Limitations of independence</h2>
<p>Independence is pursued as a criterion in many papers, for several reasons. For example, it may be an expression of a belief about human nature, namely that traits relevant for a job are independent of certain attributes. It also has convenient technical properties.</p>
<p>However, decisions based on a classifier that satisfies independence can have undesirable properties (and similar arguments apply to other statistical critiera). Here is one way in which this can happen, which is easiest to illustrate if we imagine a callous or ill-intentioned decision maker. Imagine a company that in group <span class="math inline">a</span> hires diligently selected applicants at some rate <span class="math inline">p&gt;0.</span> In group <span class="math inline">b</span>, the company hires carelessly selected applicants at the same rate <span class="math inline">p.</span> Even though the acceptance rates in both groups are identical, it is far more likely that unqualified applicants are selected in one group than in the other. As a result, it will appear in hindsight that members of group <span class="math inline">b</span> performed worse than members of group <span class="math inline">a,</span> thus establishing a negative track record for group <span class="math inline">b.</span><span><label for="sn-15" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-15" class="margin-toggle" /><span class="sidenote">This problem was identified and called <em>self-fulfilling prophecy</em> in, C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness Through Awareness,” in <em>Proc. <span class="math inline">3</span>rd ITCS</em>, 2012, 214–26. One might object that enforcing demographic parity in this scenario might still create valuable additional training data which could then improve predictions in the future after re-training the classifier on these additional data points.</span></span></p>
<p>This situation might arise without positing malice: the company might have historically hired employees primarily from group <span class="math inline">a,</span> giving them a better understanding of this group. As a technical matter, the company might have substantially more training data in group <span class="math inline">a,</span> thus potentially leading to lower error rates of a learned classifier within that group. The last point is a bit subtle. After all, if both groups were entirely homogenous in all ways relevant to the classification task, more training data in one group would equally benefit both. Then again, the mere fact that we chose to distinguish these two groups indicates that we believe they might be heterogeneous in relevant aspects.</p>
</section>
<section id="interlude-how-to-satisfy-fairness-criteria" class="level2">
<h2>Interlude: How to satisfy fairness criteria</h2>
<p>A later chapter devoted to algorithmic interventions will go into detail, but we pause for a moment to think about how we can achieve the independence criterion when we actually build a classifier. We distinguish between three different techniques. While they generally apply to all the criteria and their relaxations that we review in this chapter, our discussion here focuses on independence.</p>
<ul>
<li>Pre-processing: Adjust the feature space to be uncorrelated with the sensitive attribute.</li>
<li>At training time: Work the constraint into the optimization process that constructs a classifier from training data.</li>
<li>Post-processing: Adjust a learned classifier so as to be uncorrelated with the sensitive attribute.</li>
</ul>
<p>The three approaches have different strengths and weaknesses.</p>
<p>Pre-processing is a family of techniques to transform a feature space into a representation that as a whole is independent of the sensitive attribute. This approach is generally agnostic to what we do with the new feature space in downstream applications. After the pre-processing transformation ensures independence, any deterministic training process on the new space will also satisfy independence<span><label for="sn-16" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-16" class="margin-toggle" /><span class="sidenote">Formally, this is a consequence of the <a href="https://en.wikipedia.org/wiki/Data_processing_inequality">data processing inequality</a> from information theory.</span></span>.</p>
<p>Achieving independence at training time can lead to the highest utility since we get to optimize the classifier with this criterion in mind. The disadvantage is that we need access to the raw data and training pipeline. We also give up a fair bit of generality as this approach typically applies to specific model classes or optimization problems.</p>
<p>Post-processing refers to the process of taking a trained classifier and adjusting it possibly depending on the sensitive attribute and additional randomness in such a way that independence is achieved. Formally, we say a <em>derived classifier</em> <span class="math inline">\hat Y = F(R, A)</span> is a possibly randomized function of a given score <span class="math inline">R</span> and the sensitive attribute. Given a cost for false negatives and false positives, we can find the derived classifier that minimizes the expected cost of false positive and false negatives subject to the fairness constraint at hand. Post-processing has the advantage that it works for any <em>black-box</em> classifier regardless of its inner workings. There’s no need for re-training, which is useful in cases where the training pipeline is complex. It’s often also the only available option when we have access only to a trained model with no control over the training process. These advantages of post-processing are simultaneously also a weakness as it often leads to a significant loss in utility.</p>
</section>
<section id="separation" class="level2">
<h2>Separation</h2>
<p>Our next criterion acknowledges that in many scenarios, the sensitive characteristic may be correlated with the target variable. For example, one group might have a higher default rate on loans than another. A bank might argue that it is a matter of business necessity to therefore have different lending rates for these groups.</p>
<p>Roughly speaking, the separation criterion allows correlation between the score and the sensitive attribute to the extent that it is <em>justified by the target variable</em>. This intuition can be made precise with a simple conditional independence statement.</p>
<div class="numenv Definition">
<span class="numenv Definition title">Definition 2.</span>
<p>Random variables <span class="math inline">(R, A, Y)</span> satisfy <em>separation</em> if <span class="math inline">R\bot A \mid Y.</span><span><label for="sn-17" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-17" class="margin-toggle" /><span class="sidenote">We can display separation as a graphical model in which <span class="math inline">R</span> is separated from <span class="math inline">A</span> by the target variable <span class="math inline">Y</span>:<br />
<img src="assets/R-Y-A.svg" style="width:40.0%" /><br />
If you haven’t seen graphical models before, don’t worry. All this says is that <span class="math inline">R</span> is conditionally independent of <span class="math inline">A</span> given <span class="math inline">Y</span>.</span></span></p>
</div>
<p>In the case where <span class="math inline">R</span> is a binary classifier, separation is equivalent to requiring for all groups <span class="math inline">a,b</span> the two constraints <span class="math display">\begin{aligned}
\mathbb{P}\{ R=1 \mid Y=1, A=a\} &amp;= \mathbb{P}\{ R=1 \mid Y=1, A=b\}\\
\mathbb{P}\{ R=1 \mid Y=0, A=a\} &amp;= \mathbb{P}\{ R=1 \mid Y=0, A=b\}\,.\end{aligned}</span></p>
<p>Recall that <span class="math inline">\mathbb{P}\{R=1 \mid Y=1\}</span> is called the <em>true positive rate</em> of the classifier. It is the rate at which the classifier correctly recognizes positive instances. The <em>false positive rate</em> <span class="math inline">\mathbb{P}\{R=1 \mid Y=0\}</span> highlights the rate at which the classifier mistakenly assigns positive outcomes to negative instances. What separation therefore requires is that all groups experience the same false negative rate and the same false positive rate.</p>
<p>This interpretation in terms of equality of error rates leads to natural relaxations. For example, we could only require equality of false negative rates. A false negative, intuitively speaking, corresponds to denied opportunity in scenarios where acceptance is desirable, such as in hiring.<span><label for="sn-18" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-18" class="margin-toggle" /><span class="sidenote">In contrast, when the task is to identify high-risk individuals, as in the case of recidivism prediction, it is common to denote the undesirable outcome as the “positive” class. This inverts the meaning of false positives and false negatives, and is a frequent source of terminological confusion.</span></span></p>
</section>
<section id="achieving-separation" class="level2">
<h2>Achieving separation</h2>
<p>As was the case with independence, we can achieve separation by post-processing a given score function without the need for retraining.<span><label for="sn-19" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-19" class="margin-toggle" /><span class="sidenote">Recall, a derived classifier is a possible randomized mapping <span class="math inline">\hat Y=F(R,A).</span></span></span></p>
<p>The post-processing step uses the ROC curve that we saw earlier and it’s illustrative to go into a bit more detail. A binary classifier that satisfies separation must achieve the same true positive rates and the same false positive rates in all groups. This condition corresponds to taking the intersection of all group-level ROC curves. Within this constraint region, we can then choose the classifier that minimizes the given cost.</p>
<figure>
<img src="assets/roc_curve_2.svg" style="width:75.0%" alt="" /><figcaption>ROC curve by group.</figcaption>
</figure>
<p>We see the ROC curves of a score displayed for each group separately. The two groups have different curves indicating that not all trade-offs between true and false positive rate are achievable in both groups. The trade-offs that are achievable in both groups are precisely those that lie under both curves, corresponding to the intersection of the regions enclosed by the curves.</p>
<figure>
<img src="assets/roc_curve_3.svg" style="width:75.0%" alt="" /><figcaption>Intersection of area under the curves.</figcaption>
</figure>
<p>The highlighted region is the <em>feasible region</em> of trade-offs that we can achieve in all groups. There is a subtlety though. Points that are not exactly on the curves, but rather in the interior of the region, require <em>randomization</em>. To understand this point, consider a classifier that accepts everyone corresponding to true and false positive rate 1, the upper right corner of the plot. Consider another classifier that accepts no one, resulting in true and false positive rate 0, the lower left corner of the plot. Now, consider a third classifier that given an instance randomly picks and applies the first classifier with probability <span class="math inline">1-p</span>, and the second with probability <span class="math inline">p</span>. This classifier achieves true and false positive rate <span class="math inline">p</span> thus giving us one point on the dashed line in the plot. In the same manner, we could have picked any other pair of classifiers and randomized between them. We can fill out the entire shaded region in this way, because it is <em>convex</em>, meaning that every point in it lies on a line segment between two classifiers on the boundary.</p>
</section>
<section id="sufficiency" class="level2">
<h2>Sufficiency</h2>
<p>Our third criterion formalizes that the score already subsumes the sensitive characteristic for the purpose of predicting the target. This idea again boils down to a conditional independence statement.</p>
<div class="numenv Definition">
<span class="numenv Definition title">Definition 3.</span>
<p>We say the random variables <span class="math inline">(R, A, Y)</span> satisfy <em>sufficiency</em> if <span class="math inline">Y\bot A \mid R.</span><span><label for="sn-20" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-20" class="margin-toggle" /><span class="sidenote">We can again display sufficiency as a graphical model as we did with separation before:<br />
<img src="assets/Y-R-A.svg" style="width:40.0%" /> If you haven’t seen graphical models before, feel free to ignore this interpretation.</span></span></p>
</div>
<p>We will often just say that <span class="math inline">R</span> satisfies <em>sufficiency</em> when the sensitive attribute <span class="math inline">A</span> and target variable <span class="math inline">Y</span> are clear from the context.</p>
<p>Let us write out the definition more explicitly in the binary case where <span class="math inline">Y\in\{0,1\}.</span> In this case, a random variable <span class="math inline">R</span> is sufficient for <span class="math inline">A</span> if and only if for all groups <span class="math inline">a,b</span> and all values <span class="math inline">r</span> in the support of <span class="math inline">R,</span> we have <span class="math display">\mathbb{P}\{Y=1 \mid R=r, A=a\}=\mathbb{P}\{ Y=1 \mid R=r, A=b\}\,.</span> When <span class="math inline">R</span> has only two values we recognize this condition as requiring a parity of positive/negative predictive values across all groups.</p>
<p>While it is often useful to think of sufficiency in terms of positive and negative predictive values, there’s a useful alternative. Indeed, sufficiency turns out to be closely related to an important notion called <em>calibration</em>, as we will discuss next.</p>
</section>
</section>
<section id="calibration-and-sufficiency" class="level1">
<h1>Calibration and sufficiency</h1>
<p>In some applications it is desirable to be able to interpret the values of the score functions as probabilities. Formally, we say that a score <span class="math inline">R</span> is <em>calibrated</em> if for all score values <span class="math inline">r</span> in the support of <span class="math inline">R,</span> we have</p>
<p><span class="math display">\mathbb{P}\{Y=1 \mid R=r\} = r\,.</span></p>
<p>This condition means that the set of all instances assigned a score value <span class="math inline">r</span> has an <span class="math inline">r</span> fraction of positive instances among them. The condition refers to the group of all individuals receiving a particular score value. It does not mean that at the level of a single individual a score of <span class="math inline">r</span> corresponds to a probability <span class="math inline">r</span> of a positive outcome. The latter is a much stronger property that is satisfied by the conditional expectation <span class="math inline">R=\mathbb{E}[Y\mid X].</span><span><label for="sn-21" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-21" class="margin-toggle" /><span class="sidenote">Formally, we have for every set <span class="math inline">S</span>, <span class="math inline">\mathbb{P}\{Y=1\mid R=r, X\in S\}=r.</span></span></span></p>
<p>In practice, there are various heuristics to achieve calibration. For example, <em>Platt scaling</em> is a popular method that works as follows. Platt scaling takes a possibly uncalibrated score, treats it as a single feature, and fits a one variable regression model against the target variable based on this feature. More formally, given an uncalibrated score <span class="math inline">R,</span> Platt scaling aims to find scalar parameters <span class="math inline">a, b</span> such that the sigmoid function<span><label for="sn-22" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-22" class="margin-toggle" /><span class="sidenote">A plot of the sigmoid function <span class="math inline">1/(1+\exp(-x)).</span><br />
<img src="assets/logit_function.svg" alt="logit function" /></span></span> <span class="math display">S =
\frac{1}{1+\exp(a R+b)}</span> fits the target variable <span class="math inline">Y</span> with respect to the so-called <em>log loss</em> <span class="math display">-\mathbb{E}[Y\log S + (1-Y)\log(1-S)].</span> This objective can be minimized given labeled examples drawn from <span class="math inline">(R, Y)</span> as is standard in supervised learning.</p>
<section id="calibration-by-group" class="level2">
<h2>Calibration by group</h2>
<p>From the definition, we can see that sufficiency is closely related to the idea of calibration. To formalize the connection we say that the score <span class="math inline">R</span> satisfies <em>calibration by group</em> if it satisfies <span class="math display">\mathbb{P}\{Y=1 \mid R=r, A=a\} = r\,,</span> for all score values <span class="math inline">r</span> and groups <span class="math inline">a.</span> Recall that calibration is the same requirement at the population level without the conditioning on <span class="math inline">A.</span></p>
<div class="numenv Fact">
<span class="numenv Fact title">Fact 1.</span>
<p>Calibration by group implies sufficiency.</p>
</div>
<p>Conversely, sufficiency is only slightly weaker than calibration by group in the sense that a simple renaming of score values goes from one property to the other.</p>
<div class="numenv Proposition">
<span class="numenv Proposition title">Proposition 1.</span>
<p>If a score <span class="math inline">R</span> satisfies sufficiency, then there exists a function <span class="math inline">\ell\colon[0,1]\to[0,1]</span> so that <span class="math inline">\ell(R)</span> satisfies calibration by group.</p>
</div>
<div class="numenv Proof">
<span class="numenv Proof title">Proof.</span>
<p>Fix any group <span class="math inline">a</span> and put <span class="math inline">\ell(r) = \mathbb{P}\{Y=1\mid R=r, A=a\}.</span> Since <span class="math inline">R</span> satisfies sufficiency, this probability is the same for all groups <span class="math inline">a</span> and hence this map <span class="math inline">\ell</span> is the same regardless of what value <span class="math inline">a</span> we chose.</p>
<p>Now, consider any two groups <span class="math inline">a,b.</span> We have, <span class="math display">\begin{aligned}
r
&amp;= \mathbb{P}\{Y=1\mid \ell(R)=r, A=a\} \\
&amp;= \mathbb{P}\{Y=1\mid R\in \ell^{-1}(r), A=a\} \\
&amp;= \mathbb{P}\{Y=1\mid R\in \ell^{-1}(r), A=b\} \\
&amp;= \mathbb{P}\{Y=1\mid \ell(R)=r, A=b\}\,,\end{aligned}</span> thus showing that <span class="math inline">\ell(R)</span> is calibrated by group.</p>
</div>
<p>We conclude that sufficiency and calibration by group are essentially equivalent notions. In particular, this gives us a large repertoire of methods for achieving sufficiency. We could, for example, apply Platt scaling for each of the groups defined by the sensitive attribute.</p>
</section>
<section id="calibration-by-group-as-a-consequence-of-unconstrained-learning" class="level2">
<h2>Calibration by group as a consequence of unconstrained learning</h2>
<p>Sufficiency is often satisfied by default without the need for any explicit intervention. Indeed, we generally expect a learned score to satisfy sufficiency in cases where the sensitive attribute can be predicted from the other attributes.</p>
<p>To illustrate this point we look at the calibration values of a standard logistic regression model on the standard UCI adult data set.<span><label for="sn-23" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-23" class="margin-toggle" /><span class="sidenote"><a href="https://archive.ics.uci.edu/ml/datasets/adult">Source</a></span></span></p>
<p>We fit a logistic regression model using Python’s sklearn library on the UCI training data. The model is then applied to the UCI test data<span><label for="sn-24" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-24" class="margin-toggle" /><span class="sidenote">Number of test samples in the UCI data set by group: 1561 Black, 13946 White; 5421 Female, 10860 Male</span></span>. We make no effort to either tune or calibrate the model.</p>
<p>As we can see from the figure below, the model turns out to be fairly well calibrated by <em>gender</em> on its own without any explicit correction.</p>
<figure>
<img src="assets/adult_calibration_gender.svg" style="width:75.0%" alt="" /><figcaption>Calibration by gender on UCI adult data. A straight diagonal line would correspond to perfect calibration.</figcaption>
</figure>
<p>We see some deviation when we look at calibration by <em>race</em>.</p>
<figure>
<img src="assets/adult_calibration_race.svg" style="width:75.0%" alt="" /><figcaption>Calibration by race on UCI adult data.</figcaption>
</figure>
<p>The deviation we see in the mid deciles may be due to the scarcity of the test data in the corresponding group and deciles. For example, the <span class="math inline">6</span>th decile, corresponding to the score range <span class="math inline">(0.5, 0.6],</span> on the test data has only <span class="math inline">34</span> instances with the ‘Race’ attribute set to ‘Black’. As a result, the error bars<span><label for="sn-25" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-25" class="margin-toggle" /><span class="sidenote">The shaded region in the plot indicates a 95% confidence interval for a binomial model.</span></span> in this region are rather large.</p>
<p>Continue to explore the UCI Adult data in this <a href="code/adult.html">code example</a>.</p>
<p>The lesson is that sufficiency often comes for free (at least approximately) as a consequence of standard machine learning practices. The flip side is that imposing sufficiency as a constraint on a classification system may not be much of an intervention. In particular, it would not effect a substantial change in current practices.</p>
</section>
</section>
<section id="relationships-between-criteria" class="level1">
<h1>Relationships between criteria</h1>
<p>The criteria we reviewed constrain the joint distribution in non-trivial ways. We should therefore suspect that imposing any two of them simultaneously over-constrains the space to the point where only degenerate solutions remain. We will now see that this intuition is largely correct.</p>
<p>What this shows is that we cannot impose multiple criteria as hard constraints. This leaves open the possibility that meaningful trade-offs between these different criteria exist.</p>
<section id="independence-versus-sufficiency" class="level2">
<h2>Independence versus Sufficiency</h2>
<p>We begin with a simple proposition that shows how in general independence and sufficiency are mutually exclusive. The only assumption needed here is that the sensitive attribute <span class="math inline">A</span> and the target variable <span class="math inline">Y</span> are <em>not</em> independent. This is a different way of saying that group membership has an effect on the statistics of the target variable. In the binary case, this means one group has a higher rate of positive outcomes than another. Think of this as the typical case.</p>
<div class="numenv Proposition">
<span class="numenv Proposition title">Proposition 2.</span>
<p>Assume that <span class="math inline">A</span> and <span class="math inline">Y</span> are not independent. Then sufficiency and independence cannot both hold.</p>
</div>
<div class="numenv Proof">
<span class="numenv Proof title">Proof.</span>
<p>By the contraction rule for conditional independence,</p>
<p><span class="math display">A\bot R \quad\mathrm{and}\quad A\bot Y \mid R
\quad\Longrightarrow\quad
A\bot (Y, R)
\quad\Longrightarrow\quad
A\bot Y\,.</span></p>
<p>To be clear, <span class="math inline">A\bot (Y, R)</span> means that <span class="math inline">A</span> is independent of the pair of random variables <span class="math inline">(Y,R).</span> Dropping <span class="math inline">R</span> cannot introduce a dependence between <span class="math inline">A</span> and <span class="math inline">Y.</span></p>
<p>In the contrapositive, <span class="math display">A\not\bot Y
\quad\Longrightarrow\quad
A\not\bot R
\quad\mathrm{or}\quad
A\not\bot R \mid Y\,.</span></p>
</div>
</section>
<section id="independence-versus-separation" class="level2">
<h2>Independence versus Separation</h2>
<p>An analogous result of mutual exclusion holds for independence and separation. The statement in this case is a bit more contrived and requires the additional assumption that the target variable <span class="math inline">Y</span> is binary. We also additionally need that the score is not independent of the target. This is a rather mild assumption, since any useful score function should have correlation with the target variable.</p>
<div class="numenv Proposition">
<span class="numenv Proposition title">Proposition 3.</span>
<p>Assume <span class="math inline">Y</span> is binary, <span class="math inline">A</span> is not independent of <span class="math inline">Y</span>, and <span class="math inline">R</span> is not independent of <span class="math inline">Y.</span> Then, independence and separation cannot both hold.</p>
</div>
<div class="numenv Proof">
<span class="numenv Proof title">Proof.</span>
<p>Assume <span class="math inline">Y\in\{0,1\}.</span> In its contrapositive form, the statement we need to show is</p>
<p><span class="math display">
A\bot R
\quad\mathrm{and}\quad
A\bot R \mid Y
\quad\Longrightarrow\quad
A\bot Y
\quad\mathrm{or}\quad
R\bot Y
</span></p>
<p>By the law of total probability,</p>
<p><span class="math display">
\mathbb{P}\{R=r\mid A=a\}
=\sum_y \mathbb{P}\{R=r\mid A=a, Y=y\}\mathbb{P}\{Y=y\mid A=a\}
</span></p>
<p>Applying the assumption <span class="math inline">A\bot R</span> and <span class="math inline">A\bot R\mid Y,</span> this equation simplifies to</p>
<p><span class="math display">
\mathbb{P}\{R=r\}
=\sum_y \mathbb{P}\{R=r\mid Y=y\}\mathbb{P}\{Y=y\mid A=a\}
</span></p>
<p>Applied differently, the law of total probability also gives <span class="math display">
\mathbb{P}\{R=r\}
=\sum_y \mathbb{P}\{R=r\mid Y=y\}\mathbb{P}\{Y=y\}
</span></p>
<p>Combining this with the previous equation, we have <span class="math display">
\sum_y \mathbb{P}\{R=r\mid Y=y\}\mathbb{P}\{Y=y\}
=\sum_y \mathbb{P}\{R=r\mid Y=y\}\mathbb{P}\{Y=y\mid A=a\}
</span></p>
<p>Careful inspection reveals that when <span class="math inline">y</span> ranges over only two values, this equation can only be satisfied if <span class="math inline">A\bot Y</span> or <span class="math inline">R\bot Y.</span></p>
<p>Indeed, we can rewrite the equation more compactly using the symbols <span class="math inline">p=\mathbb{P}\{Y=0\},</span> <span class="math inline">p_a=\mathbb{P}\{Y=0\mid A=a\},</span> <span class="math inline">r_y=\mathbb{P}\{R=r\mid Y=y\},</span> as:</p>
<p><span class="math display">
pr_0 + (1-p)r_1 = p_ar_0 + (1-p_a)r_1.
</span></p>
<p>Equivalently, <span class="math inline">p(r_0 -r_1) = p_a(r_0-r_1).</span></p>
<p>This equation can only be satisfied if <span class="math inline">r_0=r_1,</span> in which case <span class="math inline">R\bot Y</span>, or if <span class="math inline">p = p_a</span> for all <span class="math inline">a,</span> in which case <span class="math inline">Y\bot A.</span></p>
</div>
<p>The claim is not true when the target variable can assume more than two values, which is a natural case to consider.</p>
<div class="numenv Exercise">
<span class="numenv Exercise title">Exercise 1.</span>
<p>Give a counterexample to the claim in the previous proposition where the target variable <span class="math inline">Y</span> assumes three distinct values.</p>
</div>
</section>
<section id="separation-versus-sufficiency" class="level2">
<h2>Separation versus Sufficiency</h2>
<p>Finally, we turn to the relationship between separation and sufficiency. Both ask for a non-trivial conditional independence relationship between the three variables <span class="math inline">A, R, Y.</span> Imposing both simultaneously leads to a degenerate solution space, as our next proposition confirms.</p>
<div class="numenv Proposition">
<span class="numenv Proposition title">Proposition 4.</span>
<p>Assume that all events in the joint distribution of <span class="math inline">(A, R, Y)</span> have positive probability, and assume <span class="math inline">A\not\bot Y.</span> Then, separation and sufficiency cannot both hold.</p>
</div>
<div class="numenv Proof">
<span class="numenv Proof title">Proof.</span>
<p>A standard fact<span><label for="sn-26" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-26" class="margin-toggle" /><span class="sidenote">See Theorem 17.2 in L. Wasserman, <em>All of Statistics: A Concise Course in Statistical Inference</em> (Springer, 2010)</span></span> about conditional independence shows</p>
<p><span class="math display">A\bot R \mid Y\quad\text{and}\quad A\bot Y\mid R
\quad\implies\quad A\bot (R, Y)\,.</span> Moreover, <span class="math display">A\bot (R,Y)\quad\implies\quad A\bot R\quad\text{and}\quad A\bot Y\,.</span> Taking the contrapositive completes the proof.</p>
</div>
<p>For a binary target, the non-degeneracy assumption in the previous proposition states that in all groups, at all score values, we have both positive and negative instances. In other words, the score value never fully resolves uncertainty regarding the outcome.</p>
<p>In case the classifier is also binary, we can weaken the assumption to require only that the classifier is imperfect in the sense of making at least one false positive prediction. What’s appealing about the resulting claim is that its proof essentially only uses a well-known relationship between true positive rate (recall) and positive predictive value (precision). This trade-off is often called <em>precision-recall trade-off</em>.</p>
<div class="numenv Proposition">
<span class="numenv Proposition title">Proposition 5.</span>
<p>Assume <span class="math inline">Y</span> is not independent of <span class="math inline">A</span> and assume <span class="math inline">\hat Y</span> is a binary classifier with nonzero false positive rate. Then, separation and sufficiency cannot both hold.</p>
</div>
<div class="numenv Proof">
<span class="numenv Proof title">Proof.</span>
<p>Since <span class="math inline">Y</span> is not independent of <span class="math inline">A</span> there must be two groups, call them <span class="math inline">0</span> and <span class="math inline">1</span>, such that <span class="math display">p_0=\mathbb{P}\{Y=1\mid A=0\}\ne \mathbb{P}\{Y=1\mid A=1\}=p_1\,.</span> Now suppose that separation holds. Since the classifier is imperfect this means that all groups have the same non-zero false positive rate <span class="math inline">\mathrm{FPR}&gt;0,</span> and the same positive true positive rate <span class="math inline">\mathrm{TPR}&gt;0.</span> We will show that sufficiency does not hold.</p>
<p>Recall that in the binary case, sufficiency implies that all groups have the same positive predictive value. The positive predictive value in group <span class="math inline">a,</span> denoted <span class="math inline">\mathrm{PPV}_a</span> satisfies <span class="math display">\mathrm{PPV_a}
= \frac{\mathrm{TPR}p_a}{\mathrm{TPR}p_a+\mathrm{FPR}(1-p_a)}\,.</span> From the expression we can see that <span class="math inline">\mathrm{PPV}_0=\mathrm{PPV}_1</span> only if <span class="math inline">\mathrm{TPR}=0</span> or <span class="math inline">\mathrm{FPR}=0.</span> The latter is ruled out by assumption. So it must be that <span class="math inline">\mathrm{TPR}=0.</span> However, in this case, we can verify that the negative predictive value <span class="math inline">\mathrm{NPV}_0</span> in group <span class="math inline">0</span> must be different from the negative predictive value <span class="math inline">\mathrm{NPV}_1</span> in group <span class="math inline">1.</span> This follows from the expression <span class="math display">\mathrm{NPV_a}
= \frac{(1-\mathrm{FPR})(1-p_a)}{(1-\mathrm{TPR})p_a+(1-\mathrm{FPR})(1-p_a)}\,.</span> Hence, sufficiency does not hold.</p>
</div>
<p>A good exercise is to derive variants of these trade-offs such as the following.</p>
<div class="numenv Exercise">
<span class="numenv Exercise title">Exercise 2.</span>
<p>Prove the following result: Assume <span class="math inline">Y</span> is not independent of <span class="math inline">A</span> and assume <span class="math inline">\hat Y</span> is a binary classifier with nonzero false positive rate and nonzero true positive rate. Then, if separation holds, there must be two groups with different positive predictive values.</p>
</div>
</section>
</section>
<section id="inherent-limitations-of-observational-criteria" class="level1">
<h1>Inherent limitations of observational criteria</h1>
<p>All criteria we’ve seen so far have one important aspect in common. They are properties of the joint distribution of the score, sensitive attribute, and the target variable. In other words, if we know the joint distribution of the random variables <span class="math inline">(R, A, Y),</span> we can without ambiguity determine whether this joint distribution satisfies one of these criteria or not.<span><label for="sn-27" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-27" class="margin-toggle" /><span class="sidenote">For example, if all variables are binary, there are eight numbers specifying the joint distributions. We can verify the property by looking only at these eight numbers.</span></span></p>
<p>We can broaden this notion a bit and also include all other features, not just the sensitive attribute. So, let’s call a criterion <em>observational</em> if it is a property of the joint distribution of the features <span class="math inline">X,</span> the sensitive attribute <span class="math inline">A,</span> a score function <span class="math inline">R</span> and an outcome variable <span class="math inline">Y.</span><span><label for="sn-28" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-28" class="margin-toggle" /><span class="sidenote">Formally, this means an observational property is defined by set of joint distributions over a given set of variables.</span></span> Informally, a criterion is observational if we can express it using probability statements involving the random variables at hand.</p>
<div class="numenv Exercise">
<span class="numenv Exercise title">Exercise 3.</span>
<p>Convince yourself that independence, separation, and sufficiency are all observational definitions. Come up with a criterion that is <em>not</em> observational.</p>
</div>
<p>Observational definitions have many appealing aspects. They’re often easy to state and require only a lightweight formalism. They make no reference to the inner workings of the classifier, the decision maker’s intent, the impact of the decisions on the population, or any notion of whether and how a feature actually influences the outcome. We can reason about them fairly conveniently as we saw earlier. In principle, observational definitions can always be verified given samples from the joint distribution—subject to statistical sampling error.</p>
<p>At the same time, all observational definitions share inherent limitations that we will explore now. Our starting point are two fictitious worlds with substantively different characteristics. We will see that despite their differences these two worlds can map to identical joint distributions. What follows is that all observational criteria will look the same in either world, thus glossing over whatever differences there are.</p>
<p>To develop these two worlds, we’ll use the case of a fictitious advertising campaign that targets a hiring ad to software engineers. A score function estimates the likelihood that an individual is a software engineer given some available features.</p>
<section id="scenario-i" class="level2">
<h2>Scenario I</h2>
<p>Imagine we introduce the following random variables in our classification problem.</p>
<ul>
<li><span class="math inline">A</span> indicates gender</li>
<li><span class="math inline">X_1</span> indicates whether the user visited <code>pinterest.com</code></li>
<li><span class="math inline">X_2</span> indicates whether the user visited <code>github.com</code></li>
<li><span class="math inline">R^*</span> is the optimal unconstrained score</li>
<li><span class="math inline">\tilde R</span> is the optimal score satisfying separation</li>
<li><span class="math inline">Y</span> indicates whether the user is a software engineer</li>
</ul>
<p>We can summarize the conditional independence relationships between the variables in a <em>directed graphical model</em>.<span><label for="sn-29" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-29" class="margin-toggle" /><span class="sidenote">Learn more about graphical models <a href="https://en.wikipedia.org/wiki/Graphical_model">here</a>.</span></span> The main fact we need is that a node is conditionally independent of any node that is not a direct ancestor given its parents.</p>
<figure>
<img src="assets/scenario1-gm.svg" style="width:40.0%" alt="" /><figcaption>Directed graphical model for the variables in Scenario I</figcaption>
</figure>
<p>Let’s imagine a situation that corresponds to this kind of graphical model. We could argue that gender influences the target variable, since currently software engineers are predominantly male. Gender also influences the first feature, since Pinterest’s user base skews female.<span><label for="sn-30" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-30" class="margin-toggle" /><span class="sidenote">As of August 2017, 58.9% of Pinterest’s users in the United States were female. See <a href="https://www.statista.com/statistics/277759/pinterest-gender-usa/">here</a> (Retrieved 3-27-2018)</span></span> We assume <code>github.com</code> has a male bias. However, this bias is explained by the target variable in the sense that conditional on being a software engineer, all genders are equally likely to visit <code>github.com</code>.</p>
<p>Once we make these assumptions, we can work out what the optimal unconstrained classifier will do. Both features correlate with the target variable and are therefore useful for prediction. The first feature is predictive since (absent other information) visiting <code>pinterest.com</code> suggests female gender, which in turns makes “software engineer” less likely. The second feature is predictive in a more direct sense, as the website is specifically designed for software engineers.</p>
<p>The optimal classifier satisfying separation will refrain from using the first feature (visiting <code>pinterest.com</code>). After all, we can see from the graphical model that this feature is not conditionally independent of the sensitive attribute given the target. This score will only use the directly predictive feature <code>github.com</code>, which is indeed conditionally independent of gender given the target.</p>
</section>
<section id="scenario-ii" class="level2">
<h2>Scenario II</h2>
<p>Our two features are different in Scenario II, but all other variables have the same interpretation.</p>
<ul>
<li><span class="math inline">X_1</span> indicates whether the user studied computer science</li>
<li><span class="math inline">X_2</span> indicates whether the user visited the Grace Hopper conference</li>
</ul>
<p>Although the other variables have the same names and interpretations, we now imagine a very different graphical model.</p>
<figure>
<img src="assets/scenario2-gm.svg" style="width:40.0%" alt="" /><figcaption>Directed graphical model for the variables in Scenario II</figcaption>
</figure>
<p>As before, we assume that gender influences the target variable, but now we assume that the target variable is conditionally independent from gender given the first feature. That is, conditional on having studied computer science, all genders are equally likely to go on to become software engineers.<span><label for="sn-31" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-31" class="margin-toggle" /><span class="sidenote">This may not be true in reality. It’s an assumption we make in this example.</span></span></p>
<p>With these assumptions, we can again work out the optimal unconstrained classifier. This time, the optimal unconstrained classifier will only use one feature, namely the first. The reason is that, given the first feature, all remaining features (including the sensitive attribute) become conditionally independent of the target. Therefore, knowing the second feature does not help in predicting the target, once we have the first.</p>
<p>The optimal classifier under separation turns out to be a bit subtle in Scenario II. The issue is that neither of the two features is conditionally independent from the sensitive attribute given the target. The classifier will therefore actively take the sensitive attribute into account in order to <em>subtract</em> its influence on the other features.</p>
</section>
<section id="different-interpretations" class="level2">
<h2>Different interpretations</h2>
<p>Interpreted in the concrete advertising context, the two scenarios don’t seem very similar. In particular, the inner workings of the optimal unconstrained classifier in each scenario are rather different. In the first scenario it uses <code>pinterest.com</code> as a weak proxy for being <em>female</em>, which it then uses as a proxy for not being a software engineer. Software engineers who visit <em>pinterest.com</em> might be concerned about this kind of stereotyping, as they might miss out on seeing the ad, and hence the job opportunity. In the second scenario, unconstrained score leads to a classifier that is natural in the sense that it only considers the directly predictive educational information. Absent other features, this would seem agreeable.</p>
<p>Similarly, the optimal classifier satisfying separation behaves differently in the two scenarios. In the first, it corresponds to the natural classifier that only uses <code>github.com</code> when predicting <em>software engineer</em>. Since <code>github.com</code> is primarily a website for software engineers, this seems reasonable. In the second scenario, however, the optimal constrained score performs a subtle adjustment procedure that explicitly takes the sensitive attribute into account. These score functions are also not equivalent from a legal standpoint. One uses the sensitive attribute explicitly for an adjustment step, while the other does not.</p>
</section>
<section id="indistinguishability" class="level2">
<h2>Indistinguishability</h2>
<p>Despite all their apparent differences, we can instantiate the random variables in each scenario in such a manner that the two scenarios map to identical joint distributions. This means that no property of the joint distribution will be able to distinguish the two scenarios. Whatever property holds for one scenario, it will inevitably also hold for the other. If by some observational criterion we call one scenario <em>unfair</em>, we will also have to call the other <em>unfair</em>.</p>
<div class="numenv Proposition">
<span class="numenv Proposition title">Proposition 6.</span>
<p>The random variables in Scenario I and II admit identical joint distributions. In particular, no observational criterion distinguishes between the two scenarios.</p>
</div>
<p>The indistinguishability result has nothing to do with sample sizes or sampling errors. No matter how many data points we have, the size of our data does not resolve the indistinguishability.</p>
<p>There’s another interesting consequence of this result. Observational criteria cannot even determine if the sensitive attribute was fed into the classifier or not. To see this, recall that the optimal constrained score in one scenario directly uses <em>gender</em>, in the other it does not.</p>
</section>
<section id="a-forced-perspective-problem" class="level2">
<h2>A forced perspective problem</h2>
<p>To understand the indistinguishability result, it’s useful to draw an analogy with a <em>forced perspective</em> problem. Two different objects can appear identical when looked at from a certain fixed perspective.</p>
<p>A data set always forces a particular perspective on reality. There is a possibility that this perspective makes it difficult to identify certain properties of the real world. Even if we have plenty of data, so long as this data comes from the same distribution, it still represents the same perspective. Having additional data is a bit like increasing the resolution of our camera. It helps with some problems, but it doesn’t change the angle or the position of the camera.</p>
<p>The limitations of observational criteria are fundamentally the limitations of a single perspective. When analyzing a data set through the lens of observational criteria we do not evaluate alternatives to the data we have. Observational criteria do not tell us what is missing from our perspective.</p>
<p>What then is <em>not</em> observational and how do we go beyond observational criteria? This is a profound question that will be the focus of later chapters. In particular, we will introduce the technical repertoire of measurement and causality to augment the classification paradigm. Both measurement and causality give us mechanisms to interrogate, question, and change the perspective suggested by our data.</p>
</section>
</section>
<section id="case-study-credit-scoring" class="level1">
<h1>Case study: Credit scoring</h1>
<p>We now apply some of the notions we saw to credit scoring. Credit scores support lending decisions by giving an estimate of the risk that a loan applicant will default on a loan. Credit scores are widely used in the United States and other countries when allocating credit, ranging from micro loans to jumbo mortgages. In the United States, there are three major credit-reporting agencies that collect data on various lendees. These agencies are for-profit organizations that each offer risk scores based on the data they collected. FICO scores are a well-known family of proprietary scores developed by FICO and sold by the three credit reporting agencies.</p>
<p>Regulation of credit agencies in the United States started with the Fair Credit Reporting Act, first passed in 1970, that aims to promote the accuracy, fairness, and privacy of consumer of information collected by the reporting agencies. The Equal Credit Opportunity Act, a United States law enacted in 1974, makes it unlawful for any creditor to discriminate against any applicant the basis of race, color, religion, national origin, sex, marital status, or age.</p>
<section id="score-distribution" class="level2">
<h2>Score distribution</h2>
<p>Our analysis relies on data published by the Federal Reserve<span><label for="sn-32" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-32" class="margin-toggle" /><span class="sidenote">The Federal Reserve Board, “Report to the Congress on Credit Scoring and Its Effects on the Availability and Affordability of Credit” (<a href="https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/" class="uri">https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/</a>, 2007).</span></span>. The data set provides aggregate statistics from 2003 about a credit score, demographic information (race or ethnicity, gender, marital status), and outcomes (to be defined shortly). We’ll focus on the joint statistics of score, race, and outcome, where the race attributes assume four values detailed below.<span><label for="sn-33" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-33" class="margin-toggle" /><span class="sidenote">These numbers come from the “Estimation sample” column of Table 9 on this <a href="https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/datamodel_tables.htm">web page</a>.</span></span></p>
<table>
<caption>Credit score distribution by ethnicity</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Race or ethnicity</th>
<th style="text-align: center;">Samples with both score and outcome</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">White</td>
<td style="text-align: center;">133,165</td>
</tr>
<tr class="even">
<td style="text-align: center;">Black</td>
<td style="text-align: center;">18,274</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Hispanic</td>
<td style="text-align: center;">14,702</td>
</tr>
<tr class="even">
<td style="text-align: center;">Asian</td>
<td style="text-align: center;">7,906</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Total</td>
<td style="text-align: center;">174,047</td>
</tr>
</tbody>
</table>
<p>The score used in the study is based on the TransUnion TransRisk score. TransUnion is a US credit-reporting agency. The TransRisk score is in turn based on a proprietary model created by FICO, hence often referred to as FICO scores. The Federal Reserve renormalized the scores for the study to vary from 0 to 100, with 0 being <em>least creditworthy</em>.</p>
<p>The information on race was provided by the Social Security Administration, thus relying on self-reported values.</p>
<p>The cumulative distribution of these credit scores strongly depends on the group as the next figure reveals.</p>
<figure>
<img src="assets/credit_cdf.svg" style="width:75.0%" alt="" /><figcaption>Cumulative density of scores by group.</figcaption>
</figure>
<p>For an extensive documentation of the data set see the <a href="https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/">Federal Reserve report</a>.</p>
</section>
<section id="performance-variables-and-roc-curves" class="level2">
<h2>Performance variables and ROC curves</h2>
<p>As is often the case, the outcome variable is a subtle aspect of this data set. Its definition is worth emphasizing. Since the score model is proprietary, it is not clear what target variable was used during the training process. What is it then that the score is trying to predict? In a first reaction, we might say that the goal of a credit score is to predict a <em>default</em> outcome. However, that’s not a clearly defined notion. Defaults vary in the amount of debt recovered, and the amount of time given for recovery. Any single binary performance indicator is typically an oversimplification.</p>
<p>What is available in the Federal Reserve data is a so-called <em>performance</em> variable that measures a <em>serious delinquency in at least one credit line of a certain time period</em>. More specifically,</p>
<blockquote>
<p>(the) measure is based on the performance of new or existing accounts and measures whether individuals have been late 90 days or more on one or more of their accounts or had a public record item or a new collection agency account during the performance period.<span><label for="sn-34" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-34" class="margin-toggle" /><span class="sidenote">Quote from the <a href="https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/performance.htm#toc9.5">Federal Reserve report</a>.</span></span></p>
</blockquote>
<p>With this performance variable at hand, we can look at the ROC curve to get a sense of how predictive the score is in different demographics.</p>
<figure>
<img src="assets/credit_roc_curve_(0,1)_(0,1).svg" style="width:75.0%" alt="" /><figcaption>ROC curve of credit score by group.</figcaption>
</figure>
<p>The meaning of true positive rate is <em>the rate of predicted positive performance given positive performance.</em> Similarly, false positive rate is <em>the rate of predicted negative performance given a positive performance</em>.</p>
<figure>
<img src="assets/credit_roc_curve_(0,0.3)_(0.4,1.0).svg" style="width:75.0%" alt="" /><figcaption>ROC curve of credit score by group zoomed in on region of large differences.</figcaption>
</figure>
<p>We see that the shapes appear roughly visually similar in the groups, although the ‘White’ group encloses a noticeably larger area under the curve than the ‘Black’ group. Also note that even two ROC curves with the same shape can correspond to very different score functions. A particular trade-off between true positive rate and false positive rate achieved at a threshold <span class="math inline">t</span> in one group could require a different threshold <span class="math inline">t&#39;</span> in the other group.</p>
</section>
<section id="comparison-of-different-criteria" class="level2">
<h2>Comparison of different criteria</h2>
<p>With the score data at hand, we compare four different classification strategies:</p>
<ul>
<li><em>Maximum profit:</em> Pick possibly group-dependent score thresholds in a way that maximizes profit.</li>
<li><em>Single threshold:</em> Pick a single uniform score threshold for all groups in a way that maximizes profit.</li>
<li><em>Separation:</em> Achieve an equal true/false positive rate in all groups. Subject to this constraint, maximize profit.</li>
<li><em>Independence:</em> Achieve an equal acceptance rate in all groups. Subject to this constraint, maximize profit.</li>
</ul>
<p>To make sense of maximizing profit, we need to assume a reward for a true positive (correctly predicted positive performance), and a cost for false positives (negative performance predicted as positive). In lending, the cost of a false positive is typically many times greater than the reward for a true positive. In other words, the interest payments resulting from a loan are relatively small compared with the loan amount that could be lost. For illustrative purposes, we imagine that the cost of a false positive is 6 times greater than the return on a true positive. The absolute numbers don’t matter. Only the ratio matters. This simple cost structure glosses over a number of details that are likely relevant for the lender such as the terms of the loan.</p>
<p>There is another major caveat to the kind of analysis we’re about to do. Since we’re only given aggregate statistics, we cannot retrain the score with a particular classification strategy in mind. The only thing we can do is to define a setting of thresholds that achieves a particular criterion. This approach may be overly pessimistic with regards to the profit achieved subject to each constraint. For this reason and the fact that our choice of cost function was rather arbitrary, we do not state the profit numbers. The numbers can be found in the original analysis<span><label for="sn-35" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-35" class="margin-toggle" /><span class="sidenote">M. Hardt, E. Price, and N. Srebro, “Equality of Opportunity in Supervised Learning,” in <em>Proc. <span class="math inline">29</span>th NIPS</em>, 2016, 3315–23.</span></span>, which reports that ‘single threshold’ achieves higher profit than ‘separation’, which in turn achieves higher profit than ‘independence’.</p>
<p>What we do instead is to look at the different trade-offs between true and false positive rate that each criterion achieves in each group.</p>
<figure>
<img src="assets/credit_roc_curve_with_thresholds.svg" style="width:100.0%" alt="" /><figcaption>ROC curves with thresholds induced by different criteria.</figcaption>
</figure>
<p>We can see that even though the ROC curves are somewhat similar, the resulting trade-offs can differ widely by group for some of the criteria. The true positive rate achieved by <em>max profit</em> for the Asian group is twice of what it is for the Black group. The separation criterion, of course, results in the same trade-off in all groups. Independence equalizes acceptance rate, but leads to widely different trade-offs. For instance, the Asian group has a false positive rate more than three times the false positive rate within the Black group.</p>
</section>
<section id="calibration-values" class="level2">
<h2>Calibration values</h2>
<p>Finally, we consider the non-default rate by group. This corresponds to the calibration plot by group.<span><label for="sn-36" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-36" class="margin-toggle" /><span class="sidenote">The error bars on the these plots were omitted as they are generally small except for very low score values (0-5) where few samples are available.</span></span></p>
<figure>
<img src="assets/credit_performance.svg" style="width:100.0%" alt="" /><figcaption>Calibration values of credit score by group.</figcaption>
</figure>
<p>We see that the performance curves by group are reasonably well aligned. This means that a monotonic transformation of the score values would result in a score that is roughly calibrated by group according to our earlier definition. Due to the differences in score distribution by group, it could nonetheless be the case that thresholding the score leads to a classifier with different positive predictive values in each group.</p>
<p>Feel free to continue exploring the data in this <a href="code/creditscore.html">code repository</a>.</p>
</section>
</section>
<section id="problem-set-criminal-justice-case-study" class="level1">
<h1>Problem set: Criminal justice case study</h1>
<p>Risk assessment is an important component of the criminal justice system. In the United States, judges set bail and decide pre-trial detention based on their assessment of the risk that a released defendant would fail to appear at trial or cause harm to the public. While <em>actuarial risk assessment</em> is not new in this domain, there is increasing support for the use of learned risk scores to guide human judges in their decisions. Proponents argue that machine learning could lead to greater efficiency and less biased decisions compared with human judgment. Critical voices raise the concern that such scores can perpetuate inequalities found in historical data, and systematically harm historically disadvantaged groups.</p>
<p>In this problem set<span><label for="sn-37" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-37" class="margin-toggle" /><span class="sidenote">Solutions to these problems are available to course instructors on request.</span></span>, we’ll begin to scratch at the surface of the complex criminal justice domain. Our starting point is an investigation carried out by Propublica<span><label for="sn-38" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-38" class="margin-toggle" /><span class="sidenote">J. Angwin, J. Larson, S. Mattu, and L. Kirchner, “Machine Bias,” <em>ProPublica</em>, May 2016, <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" class="uri">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a>.</span></span> of a proprietary risk score, called COMPAS score. These scores are intended to assess the risk that a defendant will re-offend, a task often called <em>recidivism prediction</em>. Within the academic community, the ProPublica article drew much attention to the trade-off between separation and sufficiency that we saw earlier.</p>
<p>We’ll use data obtained and released by ProPublica as a result of a public records request in Broward Country, Florida, concerning the <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">COMPAS</a> recidivism prediction system. The data is available <a href="https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv">here</a>. Following ProPublica’s <a href="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm">analysis</a>, we’ll filter out rows where <code>days_b_screening_arrest</code> is over 30 or under -30, leaving us with 6,172 rows.</p>
<section id="calibrationsufficiency" class="level2">
<h2>Calibration/sufficiency</h2>
<ul>
<li>Plot the fraction of defendants recidivating within two years (<code>two_year_recid == 1</code>) as a function of risk score (<code>decile_score</code>), for black defendants (<code>race == "African-American"</code>) and white defendants (<code>race == "Caucasian"</code>).</li>
<li>Based on these plots, does the risk score satisfy sufficiency across racial groups in this dataset? This is somewhat subjective, since we want to allow for approximate equality between groups; justify your answer in a sentence or two.</li>
</ul>
</section>
<section id="error-ratesseparation" class="level2">
<h2>Error rates/separation</h2>
<ul>
<li>Plot the distribution of scores received by the positive class (recidivists) and the distribution of scores received by the negative class (non-recidivists) for black defendants and for white defendants.</li>
<li>Based on these plots, does COMPAS achieve separation between the risk score and race?</li>
<li>Report the Positive Predictive Value, False Positive Rate, and False Negative Rate for a risk threshold of 4 (i.e., defendants with <code>decile_score &gt;= 4</code> are classified as high risk), for black defendants and for white defendants.</li>
<li>Can we pick two thresholds (one for black defendants, one for white defendants) such that FPR and FNR are roughly equal for the two groups (say, within 1% of each other)? What is the PPV for the two groups in this case? Note: trivial thresholds of 0 or 11 don’t count.</li>
</ul>
</section>
<section id="risk-factors-and-interventions" class="level2">
<h2>Risk factors and interventions</h2>
<ul>
<li>Report the recidivism rate of defendants aged 25 or lower, and defendants aged 50 or higher. Note the stark difference between the two: younger defendants are far more likely to recidivate.</li>
</ul>
<p>The following questions are best viewed as prompts for a class discussion.</p>
<ul>
<li>Suppose we are interested in taking a data-driven approach to changing the criminal justice system. Under a theory of incarceration as incapacitation (prevention of future crimes by removal of individuals from society), how might we act on the finding that younger defendants are more likely to reoffend?</li>
<li>How might we act on this finding under a rehabilitative approach to justice, in which we seek to find interventions that minimize a defendant’s risk of recidivism?</li>
<li>Under a retributive theory of justice, punishment is based in part on culpability, or blameworthiness; this in turn depends on how much control the defendant had over their actions. Under such a theory, how might we act on the finding that younger defendants are more likely to reoffend (and, more generally, commit offenses at all)?</li>
</ul>
</section>
</section>
<section id="problem-set-data-modeling-of-traffic-stops" class="level1">
<h1>Problem set: Data modeling of traffic stops</h1>
<p>For this problem we’ll use data released by the Stanford Open Policing Project (SOPP) for the state of North Carolina, available <a href="https://stacks.stanford.edu/file/druid:py883nd2578/NC-clean.csv.gz">here</a>. It contains records of 9.6 million police stops in the state between 2000 and 2015.</p>
<p>General notes and hints:</p>
<ul>
<li>The <em>stop rates</em> section of this problem requires linking SOPP data to census data, whereas the rest is based only on SOPP data and no external datasets. So you might want to work on <em>post-stop outcomes</em> and the following sections first, so that you can get familiar with the SOPP data before having to also deal with the census data.</li>
<li>Throughout this problem, report any data cleaning steps (such as dropping some rows) that you took. Also report any ambiguities you encountered and how you resolved them.</li>
</ul>
<section id="stop-rates" class="level2">
<h2>Stop rates</h2>
<p><strong>Part A</strong></p>
<ul>
<li>For each possible group defined by race, age, gender, location, and year, where:
<ul>
<li>race is one of “Asian”, “Black”, “Hispanic”, “White”</li>
<li>age is one of the buckets 15–19, 20–29, 30–39, 40–49, and 50+.</li>
<li>gender is one of “female”, “male”</li>
<li>location is a state patrol troop district</li>
<li>and year is between 2010 and 2015, inclusive</li>
</ul></li>
<li>report the following:
<ul>
<li>the population of the group from census data, and</li>
<li>the number of stops in that group from SOPP data.</li>
</ul></li>
</ul>
<p>The census data is available <a href="https://www2.census.gov/programs-surveys/popest/datasets/2010-2015/counties/asrh/cc-est2015-alldata.csv">here</a> and the fields are explained <a href="https://www2.census.gov/programs-surveys/popest/datasets/2010-2015/counties/asrh/cc-est2015-alldata.pdf">here</a>. Your data should look like the table below.</p>
<table>
<caption>Census data</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Race</th>
<th style="text-align: center;">Age</th>
<th style="text-align: center;">Gender</th>
<th style="text-align: center;">Location</th>
<th style="text-align: center;">Year</th>
<th style="text-align: center;">Population</th>
<th style="text-align: center;">Count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Hispanic</td>
<td style="text-align: center;">30-39</td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">B5</td>
<td style="text-align: center;">2012</td>
<td style="text-align: center;">434</td>
<td style="text-align: center;">76</td>
</tr>
<tr class="even">
<td style="text-align: center;">White</td>
<td style="text-align: center;">40-49</td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">C8</td>
<td style="text-align: center;">2011</td>
<td style="text-align: center;">2053</td>
<td style="text-align: center;">213</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Asian</td>
<td style="text-align: center;">15-19</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">A2</td>
<td style="text-align: center;">2012</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">White</td>
<td style="text-align: center;">20-29</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">A6</td>
<td style="text-align: center;">2011</td>
<td style="text-align: center;">8323</td>
<td style="text-align: center;">1464</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Hispanic</td>
<td style="text-align: center;">20-29</td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">D3</td>
<td style="text-align: center;">2010</td>
<td style="text-align: center;">393</td>
<td style="text-align: center;">56</td>
</tr>
<tr class="even">
<td style="text-align: center;">Black</td>
<td style="text-align: center;">40-49</td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">D7</td>
<td style="text-align: center;">2011</td>
<td style="text-align: center;">1832</td>
<td style="text-align: center;">252</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Asian</td>
<td style="text-align: center;">30-39</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">E6</td>
<td style="text-align: center;">2013</td>
<td style="text-align: center;">503</td>
<td style="text-align: center;">34</td>
</tr>
<tr class="even">
<td style="text-align: center;">Asian</td>
<td style="text-align: center;">15-19</td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">B5</td>
<td style="text-align: center;">2015</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="odd">
<td style="text-align: center;">White</td>
<td style="text-align: center;">20-29</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">A5</td>
<td style="text-align: center;">2012</td>
<td style="text-align: center;">12204</td>
<td style="text-align: center;">1852</td>
</tr>
<tr class="even">
<td style="text-align: center;">Black</td>
<td style="text-align: center;">15-19</td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">H1</td>
<td style="text-align: center;">2011</td>
<td style="text-align: center;">1281</td>
<td style="text-align: center;">55</td>
</tr>
</tbody>
</table>
<p><em>Notes and hints:</em></p>
<ul>
<li>The table is a small sample of rows from the actual answer. You can use it to check your answers. There should be about 13,000 rows in the table in total.</li>
<li>The relevant fields in the census data are <code>AA_[FE]MALE</code>, <code>BA_[FE]MALE</code>, <code>H_[FE]MALE</code>, <code>WA_[FE]MALE</code>.</li>
<li>The relevant fields in the SOPP data are <code>driver_race</code>, <code>driver_age</code>, <code>driver_gender</code>, <code>district</code>, and <code>stop_date</code>.</li>
<li>The census data is grouped by county, which is more granular than district.The mapping from county to district is available from SOPP <a href="https://github.com/5harad/openpolicing/blob/master/resources/dictionaries/districts.csv#L101">here</a>.</li>
</ul>
<p><strong>Part B</strong></p>
<ul>
<li>Fit a negative binomial regression to your data from part (A) as given in page 5 of the <a href="https://5harad.com/papers/traffic-stops.pdf">SOPP paper</a>. Report the coefficients of race, age, and gender, and the overdispersion parameter <span class="math inline">\phi</span>. Based on these coefficients, what is the ratio of stop rates of Hispanic drivers to White drivers, and Black drivers to White drivers, controlling for age, gender, location, and year?</li>
</ul>
<p><em>Notes and hints:</em></p>
<ul>
<li>This and the following tasks will be easier using a data modeling framework such as R or statsmodels rather than an algorithmic modeling framework such as scikit-learn.</li>
<li>The “Population” column in your data corresponds to the “exposure” variable in most frameworks. Equivalently, “offset” is the log of the exposure.</li>
<li>The coefficients of the different values of each variable (e.g. female and male) are not interpretable individually; only the difference is interpretable.</li>
<li>Treat <code>year</code> as a categorical rather than a continuous variable.</li>
</ul>
<p><strong>Part C</strong></p>
<ul>
<li>Give three distinct potential reasons for the racial disparity in stop rate as measured in part B.</li>
</ul>
</section>
<section id="post-stop-outcomes" class="level2">
<h2>Post-stop outcomes</h2>
<p><strong>Part D</strong></p>
<ul>
<li>Controlling for age (bucketed as in parts A &amp; B), gender, year, and location, use logistic regression to estimate impact of race on
<ul>
<li>probability of a search (<code>search_conducted</code>)</li>
<li>probability of arrest (<code>is_arrested</code>),</li>
<li>probability of a citation (<code>stop_outcome == "Citation"</code>)</li>
</ul></li>
<li>For each of the three outcomes, report the coefficients of race, age, and gender along with standard errors of those coefficients. Feel free to sample the data for performance reasons, but if you do, make sure that all standard errors are &lt; 0.1.</li>
</ul>
<p><strong>Part E</strong></p>
<ul>
<li>Interpret the coefficients you reported in part D.
<ul>
<li>What is the ratio of the probability of search of Hispanic drivers to White drivers? Black drivers to White drivers?</li>
<li>Repeat the above for the probability of arrest instead of search.</li>
<li>What is the difference in citation probability between Hispanic drivers and White drivers? Black drivers and White drivers?</li>
<li>Comment on the age and gender coefficients in the regressions.</li>
</ul></li>
</ul>
<p><em>Notes and hints:</em></p>
<ul>
<li>Interpreting the coefficients is slightly subjective. Since the search and arrest rates are low, in those regressions we can approximate the <span class="math inline">1 / (1 + e^{-\beta x})</span> formula in logistic regression as <span class="math inline">e^{\beta x}</span>, and thus we can use differences in <span class="math inline">\beta</span> between groups to calculate approximate ratios of search/arrest probabilities.</li>
<li>This trick doesn’t work for citation rates, since those are not low. However, we can pick “typical” values for the control variables, calculate citation rates, and find the difference in citation rate between groups. The results will have little sensitivity to the values of the control variables that we pick.</li>
</ul>
<p><strong>Part F</strong></p>
<p>Explain in a sentence or two why we control for variables such as gender and location in the regression, and why the results might not be what we want if we don’t control for them. (In other words, explain the idea of a confound in this context.)</p>
<p><strong>Part G</strong></p>
<p>However, decisions about what to control are somewhat subjective. What is one reason we might <em>not</em> want to control for location in testing for discrimination? In other words, how might we underestimate discrimination if we control for location? (Hint: broaden the idea of discrimination from individual officers to the systemic aspects of policing.)</p>
</section>
<section id="data-quality" class="level2">
<h2>Data quality</h2>
<p><strong>Part H</strong></p>
<p>The SOPP authors provide a <a href="https://github.com/5harad/openpolicing/blob/master/DATA-README.md">README</a> file in which they note the incompleteness, errors, and missing values in the data on a state-by-state level. Pick any two items from this list and briefly explain how each could lead to errors or biases in the analyses you performed (or in the other analyses performed in the paper).</p>
<p><em>Notes and hints:</em></p>
<ul>
<li>Here is one example: For North Carolina, stop time is not available for a subset of rows. Suppose we throw out the rows with missing stop time (which we might have to if that variable is one of the controls in our regression). These rows might not be a random subset of rows: they could be correlated with location, because officers in some districts don’t record the stop time. If so, we might incorrectly estimate race coefficients, because officer behavior might also be correlated with location.</li>
</ul>
</section>
</section>
<section id="what-is-the-purpose-of-a-fairness-criterion" class="level1">
<h1>What is the purpose of a fairness criterion?</h1>
<p>There is an important question we have neglected so far. Although we have seen several demographic classification criteria and explored their formal properties and the relationships between them, we haven’t yet clarified the purpose of these criteria. This is a difficult normative question that will be a central concern of the next chapter. Let us address it briefly here.</p>
<p>Take the independence criterion as an example. Some support this criterion based on the belief that certain intrinsic human traits such as intelligence are independent of, say, race or gender. Others argue for independence based on their desire to live in a society where the sensitive attribute is statistically independent of outcomes such as financial well-being. In one case, independence serves as a proxy for a belief about human nature. In the other case, it represents a long-term societal goal. In either case, does it then make sense to impose independence as a constraint on a classification system?</p>
<p>In a lending setting, for example, independence would result in the same rate of lending in all demographic groups defined by the sensitive attribute, regardless of the fact that individuals’ ability to repay might be distributed differently in different groups. This makes it hard to predict the long-term impact of an intervention that imposes independence as a hard classification constraint. It is not clear how to account for the impact of the fact that giving out loans to individuals who cannot repay them impoverishes the individual who defaults (in addition to diminishing profits for the bank).</p>
<p>Without an accurate model of long-term impact it is difficult to foresee the effect that a fairness criterion would have if implemented as a hard classification constraint. However, if such a model of long-term impact model were available, directly optimizing for long-term benefit may be a more effective intervention than to impose a general and crude demographic criterion.<span><label for="sn-39" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-39" class="margin-toggle" /><span class="sidenote">L.T. Liu, S. Dean, E. Rolf, M. Simchowitz, and M. Hardt, “Delayed Impact of Fair Machine Learning,” in <em>Proc. <span class="math inline">35</span>th ICML</em>, 2018, 3156–64.</span></span></p>
<p>If demographic criteria are not useful as direct guides to fairness interventions, how should we use them then? An alternative view is that classification criteria have <em>diagnostic value</em> in highlighting different social costs of the system. Disparities in true positive rates or false positive rates, for example, indicate that two or more demographic groups experience different costs of classification that are not necessarily reflected in the cost function that the decision maker optimized.</p>
<p>At the same time, the diagnostic value of fairness criteria is subject to the fundamental limitations that we saw. In particular, we cannot base a conclusive argument of fairness or unfairness on the value of any observational criterion alone. Furthermore, Corbett-Davies et al.<span><label for="sn-40" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-40" class="margin-toggle" /><span class="sidenote">S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq, “Algorithmic Decision Making and the Cost of Fairness,” <em>arXiv Preprint arXiv:1701.08230</em>, 2017.</span></span> make the important point that statistics such as positive predictive values or false positive rates can be manipulated through external (and possibly harmful) changes to the real world processes reflected in the data. In the context of recidivism prediction in criminal justice, for example, we could artificially lower the false positive rate in one group by arresting innocent people and correctly classifying them as low risk. This external intervention will decrease the false positive rate at the expense of a clearly objectionable practice.</p>
</section>
<section id="bibliographic-notes-and-further-reading" class="level1">
<h1>Bibliographic notes and further reading</h1>
<p>The fairness criteria reviewed in this chapter were already known in the 1960s and 70s, primarily in the education testing and psychometrics literature.<span><label for="sn-41" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-41" class="margin-toggle" /><span class="sidenote">We are greatful to Ben Hutchinson for bringing these to our attention.</span></span> An important fairness criterion is due to Cleary<span><label for="sn-42" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-42" class="margin-toggle" /><span class="sidenote">T.A. Cleary, “Test Bias: Validity of the Scholastic Aptitude Test for Negro and White Students in Integrated Colleges,” <em>ETS Research Bulletin Series</em> 1966, no. 2 (1966): i–23; T.A. Cleary, “Test Bias: Prediction of Grades of Negro and White Students in Integrated Colleges,” <em>Journal of Educational Measurement</em> 5, no. 2 (1968): 115–24.</span></span> and compares regression lines between the test score and the outcome in different groups. A test is considered <em>fair</em> by the Cleary criterion if the slope of these regression lines is the same for each group. This turns out to be equivalent to the sufficiency criterion, since it means that at a given score value all groups have the same rate of positive outcomes.</p>
<p>Einhorn and Bass<span><label for="sn-43" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-43" class="margin-toggle" /><span class="sidenote">H.J. Einhorn and A.R. Bass, “Methodological Considerations Relevant to Discrimination in Employment Testing.” <em>Psychological Bulletin</em> 75, no. 4 (1971): 261.</span></span> considered equality of precision values, which is a relaxation of sufficiency as we saw earlier. Thorndike<span><label for="sn-44" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-44" class="margin-toggle" /><span class="sidenote">R.L. Thorndike, “Concepts of Culture-Fairness,” <em>Journal of Educational Measurement</em> 8, no. 2 (1971): 63–70.</span></span> considered a weak variant of calibration by which the frequency of positive predictions must equal the frequency of positive outcomes in each group, and proposed achieving it via a post-processing step that sets different thresholds in different groups. Thorndike’s criterion is incomparable to sufficiency in general.</p>
<p>Darlington<span><label for="sn-45" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-45" class="margin-toggle" /><span class="sidenote">R.B. Darlington, “Another Look at ‘Cultural Fairness’,” <em>Journal of Educational Measurement</em> 8, no. 2 (1971): 71–82.</span></span> stated four different criteria in terms of succinct expressions involving the correlation coefficients between various pairs of random variables. These criteria include independence, a relaxation of sufficiency, a relaxation of separation, and Thorndike’s criterion. Darlington included an intuitive visual argument showing that the four criteria are incompatible except in degenerate cases.</p>
<p>Lewis<span><label for="sn-46" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-46" class="margin-toggle" /><span class="sidenote">M.A. Lewis, “A Comparison of Three Models for Determining Test Fairness” (Federal Aviation Administration Washington DC Office of Aviation Medicine, 1978).</span></span> reviewed three fairness criteria including equal precision and equal true/false positive rates.</p>
<p>These important early works were re-discovered later in the machine learning and data mining community. Numerous works considered variants of independence as a fairness constraint.<span><label for="sn-47" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-47" class="margin-toggle" /><span class="sidenote">T. Calders, F. Kamiran, and M. Pechenizkiy, “Building Classifiers with Independency Constraints,” in <em>In Proc. IEEE ICDMW</em>, 2009, 13–18; F. Kamiran and T. Calders, “Classifying Without Discriminating,” in <em>Proc. <span class="math inline">2</span>nd International Conference on Computer, Control and Communication</em>, 2009.</span></span> Feldman et al.<span><label for="sn-48" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-48" class="margin-toggle" /><span class="sidenote">Feldman, Friedler, Moeller, Scheidegger, and Venkatasubramanian, “Certifying and Removing Disparate Impact.”</span></span> studied a relaxation of demographic parity in the context of disparate impact law. Zemel et al.<span><label for="sn-49" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-49" class="margin-toggle" /><span class="sidenote">R.S. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, “Learning Fair Representations,” in <em>Proc. <span class="math inline">30</span>th ICML</em>, 2013.</span></span> adopted the mutual information viewpoint and proposed a heuristic pre-processing approach for minimizing mutual information. Dwork et al.<span><label for="sn-50" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-50" class="margin-toggle" /><span class="sidenote">Dwork, Hardt, Pitassi, Reingold, and Zemel, “Fairness Through Awareness.”</span></span> argued that the independence criterion was inadequate as a fairness constraint.</p>
<p>The separation criterion appeared under the name <em>equalized odds</em><span><label for="sn-51" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-51" class="margin-toggle" /><span class="sidenote">Hardt, Price, and Srebro, “Equality of Opportunity in Supervised Learning.”</span></span>, alongside the relaxation to equal false negative rates, called <em>equality of opportunity.</em> These criteria also appeared in an independent work<span><label for="sn-52" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-52" class="margin-toggle" /><span class="sidenote">M.B. Zafar, I. Valera, M. Gómez Rodriguez, and K.P. Gummadi, “Fairness Beyond Disparate Treatment &amp; Disparate Impact: Learning Classification Without Disparate Mistreatment,” in <em>Proc. <span class="math inline">26</span>th WWW</em>, 2017.</span></span> under different names. Woodworth et al.<span><label for="sn-53" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-53" class="margin-toggle" /><span class="sidenote">B.E. Woodworth, S. Gunasekar, M.I. Ohannessian, and N. Srebro, “Learning Non-Discriminatory Predictors,” in <em>Proc. <span class="math inline">30</span>th COLT</em>, 2017, 1920–53.</span></span> studied a relaxation of separation stated in terms of correlation coefficients. This relaxation corresponds to the third criterion studied by Darlington.<span><label for="sn-54" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-54" class="margin-toggle" /><span class="sidenote">Darlington, “Another Look at ‘Cultural Fairness’.”</span></span></p>
<p>ProPublica<span><label for="sn-55" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-55" class="margin-toggle" /><span class="sidenote">Angwin, Larson, Mattu, and Kirchner, “Machine Bias.”</span></span> implicitly adopted equality of false positive rates as a fairness criterion in their article on COMPAS scores. Northpointe, the maker of the COMPAS software, emphasized the importance of calibration by group in their rebuttal<span><label for="sn-56" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-56" class="margin-toggle" /><span class="sidenote">W. Dieterich, C. Mendoza, and T. Brennan, “COMPAS Risk Scales: Demonstrating Accuracy Equity and Predictive Parity,” 2016, <a href="https://www.documentcloud.org/documents/2998391-ProPublica-Commentary-Final-070616.html" class="uri">https://www.documentcloud.org/documents/2998391-ProPublica-Commentary-Final-070616.html</a>.</span></span> to ProPublica’s article. Similar arguments were made quickly after the publication of ProPublica’s article by bloggers including Abe Gong.<span><label for="sn-57" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-57" class="margin-toggle" /><span class="sidenote">See <a href="https://medium.com/@AbeGong/ethics-for-powerful-algorithms-1-of-3-a060054efd84">this</a> and subsequent posts.</span></span> There has been extensive scholarship on the actuarial risk assessment in criminal justice that long predates the ProPublica debate; Berk et al.<span><label for="sn-58" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-58" class="margin-toggle" /><span class="sidenote">R. Berk, H. Heidari, S. Jabbari, M. Kearns, and A. Roth, “Fairness in Criminal Justice Risk Assessments: The State of the Art,” <em>ArXiv E-Prints</em> 1703.09207 (2017).</span></span> provide a survey with commentary.</p>
<p>Variants of the trade-off between separation and sufficiency were shown by Chouldechova<span><label for="sn-59" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-59" class="margin-toggle" /><span class="sidenote">A. Chouldechova, “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments,” in <em>Proc. <span class="math inline">3</span>rd FATML</em>, 2016.</span></span> and Kleinberg et al.<span><label for="sn-60" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-60" class="margin-toggle" /><span class="sidenote">J.M. Kleinberg, S. Mullainathan, and M. Raghavan, “Inherent Trade-Offs in the Fair Determination of Risk Scores,” <em>Proc. <span class="math inline">8</span>th ITCS</em>, 2017.</span></span> Each of them considered somewhat different criteria to trade off. Chouldechova’s argument is very similar to the proof we presented that invokes the relationship between positive predictive value and true positive rate. Subsequent work<span><label for="sn-61" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-61" class="margin-toggle" /><span class="sidenote">G. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, and K.Q. Weinberger, “On Fairness and Calibration,” in <em>Proc. <span class="math inline">30</span>th NIPS</em>, 2017.</span></span> considers trade-offs between relaxed and approximate criteria. The other trade-off results presented in this chapter are new to this book. The proof of the proposition relating separation and independence for binary classifiers, as well as the counterexample for ternary classifiers, is due to Shira Mitchell and Jackie Shadlen, pointed out to us in personal communication.</p>
<p>The unidentifiability result for observational criteria is due to Hardt, Price, and Srebro<span><label for="sn-62" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-62" class="margin-toggle" /><span class="sidenote">Hardt, Price, and Srebro, “Equality of Opportunity in Supervised Learning.”</span></span>, except for minor changes in the choice of graphical models and their interpretation.</p>
<section id="a-dictionary-of-criteria" class="level2">
<h2>A dictionary of criteria</h2>
<p>For convenience we collect some demographic fairness criteria below that have been proposed in the past (not necessarily including the original reference). We’ll match them to their closest relative among the three criteria independence, separation, and sufficiency. This table is meant as a reference only and is not exhaustive. There is no need to memorize these different names.</p>
<div class="numenv fullwidth">
<span class="numenv fullwidth title">fullwidth.</span>
<table>
<caption>List of demographic fairness criteria</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Closest relative</th>
<th style="text-align: center;">Note</th>
<th style="text-align: center;">Reference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Statistical parity</td>
<td style="text-align: center;">Independence</td>
<td style="text-align: center;">Equivalent</td>
<td style="text-align: center;">Dwork et al. (2011)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Group fairness</td>
<td style="text-align: center;">Independence</td>
<td style="text-align: center;">Equivalent</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Demographic parity</td>
<td style="text-align: center;">Independence</td>
<td style="text-align: center;">Equivalent</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Conditional statistical parity</td>
<td style="text-align: center;">Independence</td>
<td style="text-align: center;">Relaxation</td>
<td style="text-align: center;">Corbett-Davies et al. (2017)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Darlington criterion (4)</td>
<td style="text-align: center;">Independence</td>
<td style="text-align: center;">Equivalent</td>
<td style="text-align: center;">Darlington (1971)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Equal opportunity</td>
<td style="text-align: center;">Separation</td>
<td style="text-align: center;">Relaxation</td>
<td style="text-align: center;">Hardt, Price, Srebro (2016)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Equalized odds</td>
<td style="text-align: center;">Separation</td>
<td style="text-align: center;">Equivalent</td>
<td style="text-align: center;">Hardt, Price, Srebro (2016)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conditional procedure accuracy</td>
<td style="text-align: center;">Separation</td>
<td style="text-align: center;">Equivalent</td>
<td style="text-align: center;">Berk et al. (2017)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Avoiding disparate mistreatment</td>
<td style="text-align: center;">Separation</td>
<td style="text-align: center;">Equivalent</td>
<td style="text-align: center;">Zafar et al. (2017)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Balance for the negative class</td>
<td style="text-align: center;">Separation</td>
<td style="text-align: center;">Relaxation</td>
<td style="text-align: center;">Kleinberg, Mullainathan, Raghavan (2016)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Balance for the positive class</td>
<td style="text-align: center;">Separation</td>
<td style="text-align: center;">Relaxation</td>
<td style="text-align: center;">Kleinberg, Mullainathan, Raghavan (2016)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Predictive equality</td>
<td style="text-align: center;">Separation</td>
<td style="text-align: center;">Relaxation</td>
<td style="text-align: center;">Chouldechova (2016)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Equalized correlations</td>
<td style="text-align: center;">Separation</td>
<td style="text-align: center;">Relaxation</td>
<td style="text-align: center;">Woodworth (2017)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Darlington criterion (3)</td>
<td style="text-align: center;">Separation</td>
<td style="text-align: center;">Relaxation</td>
<td style="text-align: center;">Darlington (1971)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Cleary model</td>
<td style="text-align: center;">Sufficiency</td>
<td style="text-align: center;">Equivalent</td>
<td style="text-align: center;">Cleary (1966)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conditional use accuracy</td>
<td style="text-align: center;">Sufficiency</td>
<td style="text-align: center;">Equivalent</td>
<td style="text-align: center;">Berk et al. (2017)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Predictive parity</td>
<td style="text-align: center;">Sufficiency</td>
<td style="text-align: center;">Relaxation</td>
<td style="text-align: center;">Chouldechova (2016)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Calibration within groups</td>
<td style="text-align: center;">Sufficiency</td>
<td style="text-align: center;">Equivalent</td>
<td style="text-align: center;">Chouldechova (2016)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Darlington criterion (1), (2)</td>
<td style="text-align: center;">Sufficiency</td>
<td style="text-align: center;">Relaxation</td>
<td style="text-align: center;">Darlington (1971)</td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-angwin2016machine">
<p>Angwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. “Machine Bias.” <em>ProPublica</em>, May 2016. <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" class="uri">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a>.</p>
</div>
<div id="ref-berk2017fairness">
<p>Berk, Richard, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. “Fairness in Criminal Justice Risk Assessments: The State of the Art.” <em>ArXiv E-Prints</em> 1703.09207 (2017).</p>
</div>
<div id="ref-bonham2016will">
<p>Bonham, Vence L, Shawneequa L Callier, and Charmaine D Royal. “Will Precision Medicine Move Us Beyond Race?” <em>The New England Journal of Medicine</em> 374, no. 21 (2016): 2003.</p>
</div>
<div id="ref-bouk2015how">
<p>Bouk, Dan. <em>How Our Days Became Numbered: Risk and the Rise of the Statistical Individual</em>. University of Chicago Press, 2015.</p>
</div>
<div id="ref-calders2009building">
<p>Calders, Toon, Faisal Kamiran, and Mykola Pechenizkiy. “Building Classifiers with Independency Constraints.” In <em>In Proc. IEEE ICDMW</em>, 13–18, 2009.</p>
</div>
<div id="ref-chouldechova2016fair">
<p>Chouldechova, Alexandra. “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.” In <em>Proc. <span class="math inline">3</span>rd FATML</em>, 2016.</p>
</div>
<div id="ref-cleary1968test">
<p>Cleary, T Anne. “Test Bias: Prediction of Grades of Negro and White Students in Integrated Colleges.” <em>Journal of Educational Measurement</em> 5, no. 2 (1968): 115–24.</p>
</div>
<div id="ref-cleary1966test">
<p>———. “Test Bias: Validity of the Scholastic Aptitude Test for Negro and White Students in Integrated Colleges.” <em>ETS Research Bulletin Series</em> 1966, no. 2 (1966): i–23.</p>
</div>
<div id="ref-corbett2017algorithmic">
<p>Corbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. “Algorithmic Decision Making and the Cost of Fairness.” <em>arXiv Preprint arXiv:1701.08230</em>, 2017.</p>
</div>
<div id="ref-darlington1971another">
<p>Darlington, Richard B. “Another Look at ‘Cultural Fairness’.” <em>Journal of Educational Measurement</em> 8, no. 2 (1971): 71–82.</p>
</div>
<div id="ref-dieterich16compas">
<p>Dieterich, William, Christina Mendoza, and Tim Brennan. “COMPAS Risk Scales: Demonstrating Accuracy Equity and Predictive Parity,” 2016. <a href="https://www.documentcloud.org/documents/2998391-ProPublica-Commentary-Final-070616.html" class="uri">https://www.documentcloud.org/documents/2998391-ProPublica-Commentary-Final-070616.html</a>.</p>
</div>
<div id="ref-dwork2012fairness">
<p>Dwork, Cynthia, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. “Fairness Through Awareness.” In <em>Proc. <span class="math inline">3</span>rd ITCS</em>, 214–26, 2012.</p>
</div>
<div id="ref-einhorn1971methodological">
<p>Einhorn, Hillel J, and Alan R Bass. “Methodological Considerations Relevant to Discrimination in Employment Testing.” <em>Psychological Bulletin</em> 75, no. 4 (1971): 261.</p>
</div>
<div id="ref-feldman2015certifying">
<p>Feldman, Michael, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. “Certifying and Removing Disparate Impact.” In <em>Proc. <span class="math inline">21</span>st SIGKDD</em>. ACM, 2015.</p>
</div>
<div id="ref-halligan2015disadvantages">
<p>Halligan, Steve, Douglas G. Altman, and Susan Mallett. “Disadvantages of Using the Area Under the Receiver Operating Characteristic Curve to Assess Imaging Tests: A Discussion and Proposal for an Alternative Approach.” <em>European Radiology</em> 25, no. 4 (April 2015): 932–39.</p>
</div>
<div id="ref-hardt2016equality">
<p>Hardt, Moritz, Eric Price, and Nati Srebro. “Equality of Opportunity in Supervised Learning.” In <em>Proc. <span class="math inline">29</span>th NIPS</em>, 3315–23, 2016.</p>
</div>
<div id="ref-kamiran2009classifying">
<p>Kamiran, Faisal, and Toon Calders. “Classifying Without Discriminating.” In <em>Proc. <span class="math inline">2</span>nd International Conference on Computer, Control and Communication</em>, 2009.</p>
</div>
<div id="ref-kleinberg2016inherent">
<p>Kleinberg, Jon M., Sendhil Mullainathan, and Manish Raghavan. “Inherent Trade-Offs in the Fair Determination of Risk Scores.” <em>Proc. <span class="math inline">8</span>th ITCS</em>, 2017.</p>
</div>
<div id="ref-lewis1978comparison">
<p>Lewis, Mary A. “A Comparison of Three Models for Determining Test Fairness.” Federal Aviation Administration Washington DC Office of Aviation Medicine, 1978.</p>
</div>
<div id="ref-liu2018delayed">
<p>Liu, Lydia T., Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. “Delayed Impact of Fair Machine Learning.” In <em>Proc. <span class="math inline">35</span>th ICML</em>, 3156–64, 2018.</p>
</div>
<div id="ref-weinberger2017calibration">
<p>Pleiss, Geoff, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. “On Fairness and Calibration.” In <em>Proc. <span class="math inline">30</span>th NIPS</em>, 2017.</p>
</div>
<div id="ref-federalreserve">
<p>The Federal Reserve Board. “Report to the Congress on Credit Scoring and Its Effects on the Availability and Affordability of Credit.” <a href="https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/" class="uri">https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/</a>, 2007.</p>
</div>
<div id="ref-thorndike1971concepts">
<p>Thorndike, Robert L. “Concepts of Culture-Fairness.” <em>Journal of Educational Measurement</em> 8, no. 2 (1971): 63–70.</p>
</div>
<div id="ref-wasserman2010all">
<p>Wasserman, Larry. <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer, 2010.</p>
</div>
<div id="ref-woodworth2017learning">
<p>Woodworth, Blake E., Suriya Gunasekar, Mesrob I. Ohannessian, and Nathan Srebro. “Learning Non-Discriminatory Predictors.” In <em>Proc. <span class="math inline">30</span>th COLT</em>, 1920–53, 2017.</p>
</div>
<div id="ref-zafar2017fairnessbeyond">
<p>Zafar, Muhammad Bilal, Isabel Valera, Manuel Gómez Rodriguez, and Krishna P. Gummadi. “Fairness Beyond Disparate Treatment &amp; Disparate Impact: Learning Classification Without Disparate Mistreatment.” In <em>Proc. <span class="math inline">26</span>th WWW</em>, 2017.</p>
</div>
<div id="ref-zemel2013learning">
<p>Zemel, Richard S., Yu Wu, Kevin Swersky, Toniann Pitassi, and Cynthia Dwork. “Learning Fair Representations.” In <em>Proc. <span class="math inline">30</span>th ICML</em>, 2013.</p>
</div>
</div>
</section>
<div id="lastupdate">
Last updated: Sun Dec  1 09:38:40 PST 2019
</div>
</article>
</body>
</html>
