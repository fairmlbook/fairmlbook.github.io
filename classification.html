<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Classification</title>
  <link rel="stylesheet" href="style.css">
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article>
<header>
<div id="chapter">3</div>
<h1 class="title">Classification</h1>
</header>



<div id="collapsiblemenu">
  <button class="collapsible">
    <div class="shortthickbar"></div>
    <div class="shortthickbar"></div>
    <div class="shortthickbar"></div>
  </button>
  <div class="content">
    <ul>
    <li><a href="index.html">Home</a></li>
    </ul>
    <ul>
    <li><a
    href="#modeling-populations-as-probability-distributions">Modeling
    populations as probability distributions</a></li>
    <li><a href="#formalizing-classification">Formalizing
    classification</a>
    <ul>
    <li><a href="#optimal-classification">Optimal
    classification</a></li>
    <li><a href="#risk-scores">Risk scores</a></li>
    <li><a href="#varying-thresholds-and-roc-curves">Varying thresholds
    and ROC curves</a></li>
    </ul></li>
    <li><a href="#supervised-learning">Supervised learning</a></li>
    <li><a href="#protected-categories">Protected categories</a>
    <ul>
    <li><a href="#no-fairness-through-unawareness">No fairness through
    unawareness</a></li>
    </ul></li>
    <li><a href="#statistical-non-discrimination-criteria">Statistical
    non-discrimination criteria</a></li>
    <li><a href="#independence">Independence</a>
    <ul>
    <li><a href="#limitations-of-independence">Limitations of
    independence</a></li>
    </ul></li>
    <li><a href="#separation">Separation</a>
    <ul>
    <li><a href="#why-equalize-error-rates">Why equalize error
    rates?</a></li>
    <li><a href="#visualizing-separation">Visualizing
    separation</a></li>
    <li><a href="#conditional-acceptance-rates">Conditional acceptance
    rates</a></li>
    </ul></li>
    <li><a href="#sufficiency">Sufficiency</a>
    <ul>
    <li><a href="#calibration-and-sufficiency">Calibration and
    sufficiency</a></li>
    <li><a
    href="#calibration-by-group-as-a-consequence-of-unconstrained-learning">Calibration
    by group as a consequence of unconstrained learning</a></li>
    </ul></li>
    <li><a href="#how-to-achieve-a-non-discrimination-criterion">How to
    achieve a non-discrimination criterion</a></li>
    <li><a href="#relationships-between-criteria">Relationships between
    criteria</a>
    <ul>
    <li><a href="#independence-versus-sufficiency">Independence versus
    sufficiency</a></li>
    <li><a href="#independence-versus-separation">Independence versus
    separation</a></li>
    <li><a href="#separation-versus-sufficiency">Separation versus
    sufficiency</a></li>
    </ul></li>
    <li><a href="#case-study-credit-scoring">Case study: Credit
    scoring</a>
    <ul>
    <li><a href="#score-distribution">Score distribution</a></li>
    <li><a href="#performance-variables-and-roc-curves">Performance
    variables and ROC curves</a></li>
    <li><a href="#comparison-of-different-criteria">Comparison of
    different criteria</a></li>
    <li><a href="#calibration-values">Calibration values</a></li>
    </ul></li>
    <li><a
    href="#inherent-limitations-of-observational-criteria">Inherent
    limitations of observational criteria</a></li>
    <li><a href="#chapter-notes">Chapter notes</a>
    <ul>
    <li><a href="#a-dictionary-of-criteria">A dictionary of
    criteria</a></li>
    </ul></li>
    <li><a href="#bibliography">References</a></li>
    </ul>
  </div>
</div>


<p>The goal of classification is to leverage patterns in natural and
social processes to conjecture about uncertain outcomes. An outcome may
be uncertain because it lies in the future. This is the case when we try
to predict whether a loan applicant will pay back a loan by looking at
various characteristics such as credit history and income.
Classification also applies to situations where the outcome has already
occurred, but we are unsure about it. For example, we might try to
classify whether financial fraud has occurred by looking at financial
transaction.</p>
<p>What makes classification possible is the existence of patterns that
connect the outcome of interest in a population to pieces of information
that we can observe. Classification is specific to a population and the
patterns prevalent in the population. Risky loan applicants might have a
track record of high credit utilization. Financial fraud often coincides
with irregularities in the distribution of digits in financial
statements. These patterns might exist in some contexts but not
others.</p>
<p>We formalize classification in two steps. The first is to represent a
population as a probability distribution. While often taken for granted
in quantitative work today, the act of representing a dynamic population
of individuals as a probability distribution is significant. The second
step is to apply statistics, specifically statistical decision theory,
to the probability distribution that represents the population.
Statistical decision theory formalizes the classification objective,
allowing us to talk about the quality of different classifiers.</p>
<p>The statistical decision-theoretic treatment of classification forms
the foundation of supervised machine learning. Supervised learning makes
classification algorithmic in how it provides heuristics to turn samples
from a population into good classification rules.</p>
<section id="modeling-populations-as-probability-distributions"
class="level1">
<h1>Modeling populations as probability distributions</h1>
<p>One of the earliest applications of probability to the study of human
populations is Halley’s <em>life table</em> from 1693. Halley tabulated
births and deaths in a small town in order to estimate life expectancy
in the population. Estimates of life expectancy, then as novel as
probability theory itself, found use in accurately pricing investments
that paid an amount of money annualy for the remainder of a person’s
life.</p>
<figure>
<img src="assets/halley_life_table_1693.png" style="width:75.0%"
alt="Halley’s life table (1693)" />
<figcaption aria-hidden="true">Halley’s life table (1693)</figcaption>
</figure>
<p>For centuries that followed, the use of probability to model human
populations, however, remained contentious both scientifically and
politically.<span class="citation"
data-cites="desrosieres1998politics porter2020rise bouk2015how"><span><label for="sn-0" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-0" class="margin-toggle" /><span class="sidenote">Desrosières,
<em>The Politics of Large Numbers: A History of Statistical
Reasoning</em> (Harvard University Press, 1998); Porter, <em>The Rise of
Statistical Thinking, 1820–1900</em> (Princeton University Press, 2020);
Bouk, <em>How Our Days Became Numbered: Risk and the Rise of the
Statistical Individual</em> (University of Chicago Press,
2015).</span></span></span> Among the first to apply statistics to the
social sciences was the 19th astronomer and sociologist Adolphe
Quetelet. In a scientific program he called “social physics”, Quetelet
sought to demonstrate the existence of statistical <em>laws</em> in
human populations. He introduced the concept of the “average man”
characterized by the mean values of measured variables that followed a
normal distribution. As much a descriptive as a normative proposal,
Quetelet regarded averages as an ideal to be pursued. Among others, his
work influenced Francis Galton in the development of eugenics.</p>
<p>The success of statistics throughout the 20th century cemented in the
use of probability to model human populations. Few raise an eyebrow
today if we talk about a survey as sampling responses from a
distribution. It seems obvious now that we’d like to estimate parameters
such as mean and standard deviation from this distribution. Statistics
is so deeply embedded in the social sciences that we rarely revisit the
premise that we can represent a human population as a probability
distribution.</p>
<p>The differences between a human population and a distribution are
stark. Human populations change over time, sometimes rapidly, due to
different mechanisms and interactions among individuals. A distribution,
in contrast, can be thought of as a static array where rows correspond
to individuals and columns correspond to measured covariates of an
individual. The mathematical abstraction for such an array is a set of
nonnegative numbers, called <em>probabilities</em>, that sum
up&nbsp;<span class="math inline">1</span> and give us for each row the
relative weight of this setting of covariates in the population. To
sample from a such a distribution corresponds to picking one of the rows
in the table at random in proportion to its weight. We can repeat this
process without change or deteriation.</p>
<p>Much of statistics deals with samples and the question how we can
relate quantities computed on a sample, such as the sample avearge, to
corresponding parameters of a distribution, such as the population mean.
The focus in our chapter is different. We’ll use statistics to talk
about properties of populations as distributions and by extension
classification rules applied to a population. While sampling introduces
many additional issues, the questions we raise in this chapter come out
most clearly at the population level.</p>
</section>
<section id="formalizing-classification" class="level1">
<h1>Formalizing classification</h1>
<p>The goal of classification is to determine a plausible value for an
unknown <em>target</em> <span class="math inline">Y</span> given
observed <em>covariates</em> <span class="math inline">X</span>.
Formally, the covariates <span class="math inline">X</span> and
target&nbsp;<span class="math inline">Y</span> are jointly distributed
random variables. This means that there is one probability distribution
over pairs of values&nbsp;<span class="math inline">(x,y)</span> that
the random variables&nbsp;<span class="math inline">(X,Y)</span> might
take on. This probability distribution models a population of instances
of the classification problem. In most of our examples, we think of each
instance as the covariates and target of one individual.</p>
<p>At the time of classification, the value of the target variable is
not known to us, but we observe the covariates&nbsp;<span
class="math inline">X</span> and make a guess&nbsp;<span
class="math inline">\hat Y = f(X)</span> based on what we observed. The
funtion&nbsp;<span class="math inline">f</span> that maps our covariates
into our guess&nbsp;<span class="math inline">\hat Y</span> is called a
<em>classifier</em>. The output of the classifier is sometimes called a
<em>prediction</em> or <em>label</em>. Throughout this chapter we are
primarily interested with the random variable&nbsp;<span
class="math inline">\hat Y</span> and how it relates to other random
variables. The function that defines this random variables is secondary.
For this reason, we stretch the terminology slightly and refer
to&nbsp;<span class="math inline">\hat Y</span> itself as the
classifier.</p>
<p>What makes a classifier <em>good</em> for an application and how do
we choose one out of many possible classifiers? This question often does
not have a fully satisfying answer, but statistical decision theory
provides criteria can help highlight different qualities of a classifier
that can inform our choice.</p>
<p>Perhaps the most well known property of a classifier&nbsp;<span
class="math inline">\hat Y</span> is its <em>classification
accuracy</em>, or <em>accuracy</em> for short, defined as <span
class="math inline">\mathbb{P}\{Y=\hat Y\},</span> the probability of
correctly predicting the target variable. We define <em>classification
error</em> as&nbsp;<span class="math inline">\mathop\mathbb{P}\{Y\ne\hat
Y\}</span>. Accuracy is easy to define, but misses some important
aspects when evaluating a classifier. A classifier that always predicts
<em>no traffic fatality in the next year</em> might have high accuracy
on any given individual, simply because fatal accidents are unlikely.
However, it’s a constant function that has no value in assessing the
risk of a traffic fatality.</p>
<p>Other decision-theoretic criteria highlight different aspects of a
classifier. We can define the most common ones by considering the
conditional probability <span
class="math inline">\mathbb{P}\{\mathrm{event}\mid
\mathrm{condition}\}</span> for various different settings.</p>
<table>
<caption>Common classification criteria <span
label="table:classification-1"></span></caption>
<thead>
<tr class="header">
<th style="text-align: center;">Event</th>
<th style="text-align: center;">Condition</th>
<th>Resulting notion (<span
class="math inline">\mathbb{P}\{\mathrm{event}\mid
\mathrm{condition}\}</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\hat
Y=1</span></td>
<td style="text-align: center;"><span
class="math inline">Y=1</span></td>
<td>True positive rate, recall</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\hat
Y=0</span></td>
<td style="text-align: center;"><span
class="math inline">Y=1</span></td>
<td>False negative rate</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\hat
Y=1</span></td>
<td style="text-align: center;"><span
class="math inline">Y=0</span></td>
<td>False positive rate</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\hat
Y=0</span></td>
<td style="text-align: center;"><span
class="math inline">Y=0</span></td>
<td>True negative rate</td>
</tr>
</tbody>
</table>
<p>The true positive rate corresponds to the frequency with which the
classifier correctly assigns a positive label when the outcome is
positive. We call this a <em>true positive</em>. The other terms
<em>false positive</em>, <em>false negative</em>, and <em>true
negative</em> derive analogously from the respective definitions. It is
not important to memorize all these terms. They do, however, come up
regularly in the classification settings.</p>
<p>Another family of classification criteria arises from swapping event
and condition. We’ll only highlight two of the four possible
notions.</p>
<table>
<caption>Additional classification criteria<span
label="table:classification-2"></span></caption>
<thead>
<tr class="header">
<th style="text-align: center;">Event</th>
<th style="text-align: center;">Condition</th>
<th>Resulting notion (<span
class="math inline">\mathbb{P}\{\mathrm{event}\mid\mathrm{condition}\}</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span
class="math inline">Y=1</span></td>
<td style="text-align: center;"><span class="math inline">\hat
Y=1</span></td>
<td>Positive predictive value, precision</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span
class="math inline">Y=0</span></td>
<td style="text-align: center;"><span class="math inline">\hat
Y=0</span></td>
<td>Negative predictive value</td>
</tr>
</tbody>
</table>
<section id="optimal-classification" class="level2">
<h2>Optimal classification</h2>
<p>Suppose we assign a cost (or reward) to each of the four possible
classification outcomes, true positive, false positive, true negative,
false negative. The problem of optimal classification is to find a
classifier that minimizes cost in expectation over a population. We can
write the cost as a real number&nbsp;<span class="math inline">\ell(\hat
y, y)</span> that we experience when we classify a target
value&nbsp;<span class="math inline">y</span> with a label&nbsp;<span
class="math inline">\hat y</span>. An <em>optimal classifier</em> is any
classifier that minimizes the objective: <span class="math display">
\mathop\mathbb{E}[\ell(\hat Y, Y)]
</span> This objective is called classification <em>risk</em> and
<em>risk minimization</em> refers to the optimization problem of finding
a classifier that minimizes risk.</p>
<p>As an example, choose&nbsp;<span
class="math inline">\ell(0,1)=\ell(1,0)=1</span> and&nbsp;<span
class="math inline">\ell(1,1)=\ell(0,0)=0.</span> For this loss
function, the optimal classifier is the one that minimizes
classification error. The resulting optimal classifier has an intuitive
solution.</p>
<div class="numenv Fact">
<span class="numenv Fact title">Fact.</span>
<p>The optimal predictor minimizing classification error satisfies <span
class="math display">
\hat Y = f(X)\,,\quad\text{where}\quad
f(x) = \begin{cases}
1 &amp; \text{ if } \mathop\mathbb{P}\{ Y = 1\mid X=x\} &gt; 1/2\\
0 &amp; \text{ otherwise.}
\end{cases}
</span></p>
</div>
<p>The optimal classifier checks if the propensity of positive outcomes
given the observed covariates&nbsp;<span class="math inline">X</span> is
greater than&nbsp;<span class="math inline">1/2</span>. If so, it makes
the guess that the outcome is&nbsp;<span class="math inline">1</span>.
Otherwise, it guesses that the outcome is&nbsp;<span
class="math inline">0</span>. Although the optimal predictor is specific
to classicication error, it extends to the general case by varying the
threshold. This makes intuitive sense. If our cost for false positives
was much higher than our cost for false negatives, we’d better err on
the side of not declaring a positive.</p>
</section>
<section id="risk-scores" class="level2">
<h2>Risk scores</h2>
<p>The optimal classifier we just saw has an important property. We were
able to write it as a threshold applied to the function <span
class="math display">
r(x) = \mathop\mathbb{P}\{Y = 1\mid X=x\} = \mathop\mathbb{E}[Y\mid
X=x]\,.
</span> This function is an example of a <em>risk score</em>.
Statistical decision theory tells us that optimal classifiers can
generally be written as a threshold applied to this risk score. The risk
score we see here is a particularly important and natural one. We can
think of it as taking the available evidence <span
class="math inline">X=x</span> and calculating the expected outcome
given the observed information. This is called the <em>posterior
probability</em> of&nbsp;<span class="math inline">Y</span>
given&nbsp;<span class="math inline">X</span>. In an intuitive sense,
the conditional expectation is a statistical <em>lookup table</em> that
gives us for each setting of features the frequency of positive outcomes
given these features.</p>
<p>The risk score is in itself optimal in that it minimizes the
<em>squared loss</em> <span class="math display">
\mathop\mathbb{E}(Y-r(X))^2
</span> among all possible real-valued risk scores&nbsp;<span
class="math inline">r(X)</span>. Minimization problems where we try to
approximate the target variable&nbsp;<span class="math inline">Y</span>
with a real-valued risk score are called <em>regression</em> problems.
In this context, risk scores are often called <em>regressors</em>.</p>
<p>Although our loss function was specific, there is a general lesson.
Classification is often attacked by first solving a regression problem
to summarize the data in a single real-valued risk score. We then turn
the risk score into a classifier by thresholding.</p>
<p>Risk scores need not be optimal or learned from data. For an
illustrative example consider the well-known body mass index, due to
Quetelet by the way, which summarizes <em>weight</em> and
<em>height</em> of a person into a single real number. In our formal
notation, the features are&nbsp;<span class="math inline">X=(H,
W)</span> where&nbsp;<span class="math inline">H</span> denotes height
in meters and&nbsp;<span class="math inline">W</span> denotes weight in
kilograms. The body mass index corresponds to the score
function&nbsp;<span class="math inline">R=W/H^2.</span></p>
<figure>
<img src="assets/bmi1.svg" style="width:50.0%"
alt="Plot of the body mass index." />
<figcaption aria-hidden="true">Plot of the body mass index.</figcaption>
</figure>
<p>We could interpret the body mass index as measuring risk of, say,
diabetes. Thresholding it at the value <em>30</em>, we might decide that
individuals with a body mass index above this value are at risk of
developing diabetes while others are not. It does not take a medical
degree to worry that the resulting classifier may not be very accurate.
The body mass index has a number of known issues leading to errors when
used for classification. We won’t go into detail, but it’s worth noting
that these classification errors can systematically align with certain
groups in the population. For instance, the body mass index tends to be
inflated as a risk measure for taller people due to scaling issues.</p>
<p>A more refined approach to finding a risk score for diabetes would be
to solve a regression problem involing the available covariates and the
outcome variable. Solved optimally, the resulting risk score would tell
us for every setting of weight (say, rounded to the nearest kg unit) and
every physical height (rounded to the nearest cm unit), the incidence
rate of diabetes among individuals with these values of weight and
height. The target variable in this case is a binary indicator of
diabetes. So,&nbsp;<span class="math inline">r((176, 68))</span> would
be the incidence rate of diabetes among individuals who are 1.76m tall
and weigh 68kg. The conditional expectation is likely more useful as a
risk measure of diabetes than the body mass index we saw earlier. After
all, the conditional expectation directly reflects the incidence rate of
diabetes given the observed characteristics, while the body mass index
didn’t solve this specific classification problem.</p>
</section>
<section id="varying-thresholds-and-roc-curves" class="level2">
<h2>Varying thresholds and ROC curves</h2>
<p>In the optimal predictor for classification error we chose a
threshold of&nbsp;<span class="math inline">1/2</span>. This exact
number was a consequence of the equal cost for false positives and false
negatives. If a false positive was significantly more costly, we might
wish to choose a higher threshold for declaring a positive. Each choice
of a threshold results in a specific trade-off between true positive
rate and false positive rate. By varying the treshold from&nbsp;<span
class="math inline">0</span> to&nbsp;<span class="math inline">1,</span>
we can trace out a curve in a two-dimensional space where the axes
correspond to true positive rate and false positive rate. This curve is
called an <em>ROC curve</em>. ROC stands for receiver operator
characteristic, a name pointing at the roots of the concept in signal
processing.</p>
<figure>
<img src="assets/roc_curve_1.svg" style="width:50.0%"
alt="Example of an ROC curve" />
<figcaption aria-hidden="true">Example of an ROC curve</figcaption>
</figure>
<p>We can achieve any trade-off below the ROC curve via randomization.
To understand this point, think about how we can realize trade-offs on
the the dashed line in the plot. Take one classifier that accepts
everyone. This corresponds to true and false positive rate 1, hence
achieving the upper right corner of the plot. Take another classifier
that accepts no one, resulting in true and false positive rate 0, the
lower left corner of the plot. Now, construct a third classifier that
given an instance randomly picks and applies the first classifier with
probability&nbsp;<span class="math inline">1-p</span>, and the second
with probability <span class="math inline">p</span>. This classifier
achieves true and false positive rate&nbsp;<span
class="math inline">p</span> thus giving us one point on the dashed line
in the plot. In the same manner, we could have picked any other pair of
classifiers and randomized between them. This way we can realize the
entire area under the ROC curve. In particular, this means the area
under the ROC curve must be <em>convex</em>, meaning that every point in
it lies on a line segment between two classifiers on the boundary.</p>
<p>Strictly speaking, in statistical decision theory, the ROC curve is a
property of a distribution&nbsp;<span class="math inline">(X, Y)</span>.
It gives us for each setting of false positive rate, the optimal true
positive rate that can be achieved for the given false positive rate on
the distribution&nbsp;<span class="math inline">(X, Y)</span>. This
leads to several nice theoretical properties of the ROC curve. In the
machine learning context, ROC curves are computed more liberally for any
given risk score, even if it isn’t optimal. The ROC curve is often used
to eyeball how predictive our score is of the target variable. A common
measure of predictiveness is the area under the curve (AUC), which
equals the probability that a random positive instance gets a score
higher than a random negative instance. An area of&nbsp;<span
class="math inline">1/2</span> corresponds to random guessing, and an
area of 1 corresponds to perfect classification.</p>
</section>
</section>
<section id="supervised-learning" class="level1">
<h1>Supervised learning</h1>
<p>Supervised learning is what makes classification algorithmic. It
instructs us how to construct good classifiers from samples drawn from a
population. The details of supervised learning won’t matter for this
chapter, but it is still worthwhile to have a working understanding of
the basic idea.</p>
<p>Suppose we have labeled data, also called <em>training examples</em>,
of the form <span class="math inline">(x_1,y_1), ..., (x_n, y_n),</span>
where each <em>example</em> is a pair&nbsp;<span
class="math inline">(x_i,y_i)</span> of an <em>instance</em> <span
class="math inline">x_i</span> and a <em>label</em> <span
class="math inline">y_i.</span> We typically assume that these examples
were drawn independently and repeatedly from the same
distribution&nbsp;<span class="math inline">(X, Y)</span>. A supervised
learning algorithm takes in training examples and returns a classifier,
typically a threshold of a score:&nbsp;<span
class="math inline">f(x)=\mathbb{1}\{r(x) &gt; t\}</span>. A simple
example of a learning algorithm is the familiar least squares method
that attempts to minimize the objective function <span
class="math display">
\sum_{i=1}^n \left(r(x_i)-y_i\right)^2\,.
</span> We saw earlier that at the population level, the optimal score
is the conditional expectation&nbsp;<span
class="math inline">r(x)=\mathbb{E}\left[Y\mid X=x\right].</span> The
problem is that we don’t necessarily have enough data to estimate each
of the conditional probabilities required to construct this score. After
all, the number of possible values that&nbsp;<span
class="math inline">x</span> can assume is exponential in the number of
covariates.</p>
<p>The whole trick in supervised learning is to approximate this optimal
solution with algorithmically feasible solutions. In doing so,
supervised learning must negotiate a balance along three axes:</p>
<ul>
<li><strong>Representation</strong>: Choose a family of functions that
the score <span class="math inline">r</span> comes from. A common choice
are linear functions <span class="math inline">r(x) = \langle w,
x\rangle</span> for some vector of coefficients <span
class="math inline">w</span>. More complex representations involve
non-linear functions, such as <em>artificial neural networks</em>. This
function family is often called the <em>model class</em> and the
coefficients <span class="math inline">w</span> are called <em>model
parameters</em>.</li>
<li><strong>Optimization</strong>: Solve the resulting optimization
problem by finding model parameters that minimize the loss function on
the training examples.</li>
<li><strong>Generalization</strong>: Argue why small loss on the
training examples implies small loss on the population that we drew the
training examples from.</li>
</ul>
<p>The three goals of supervised learning are entangled. A powerful
representation might make it easier to express complicated patterns, but
it might also burden optimization and generalization. Likewise, there
are tricks to make optimization feasible at the expense of
representation or generalization.</p>
<p>For the remainder of this chapter, we can think of supervised
learning as a black box that provides us with classifiers when given
training data. What matters are which properties these classifiers have
at the population level. At the population level, we interpret a
classifier as a random variable by considering&nbsp;<span
class="math inline">\hat Y=f(X)</span>. We ignore how&nbsp;<span
class="math inline">\hat Y</span> was learned from a finite sample, what
the functional form of the classifier is, and how we estimate various
statistical quantities from finite samples. While finite sample
considerations are fundamental to machine learning, they are not central
to the conceptual and technical questions around fairness that we will
discuss in this chapter.</p>
</section>
<section id="protected-categories" class="level1">
<h1>Protected categories</h1>
<p>Chapter 2 introduced some of the reasons why individuals might want
to object to the use of statistical classification rules in
consequential decisions. We now turn to one specific concern, namely,
<em>discrimination on the basis of membership in specific groups of the
population</em>. Discrimination is not a general concept. It is
concerned with socially salient categories that have served as the basis
for unjustified and systematically adverse treatment in the past. United
States law recognizes certain <em>protected categories</em> including
race, sex (which extends to sexual orientation), religion, disability
status, and place of birth.</p>
<p>In many classification tasks, the features <span
class="math inline">X</span> implicitly or explicitly encode and
individual’s status in a protected category. We will set aside the
letter <span class="math inline">A</span> to designate a discrete random
variable that captures one or multiple sensitive characteristics.
Different settings of the random variable&nbsp;<span
class="math inline">A</span> correspond to different mutually disjoint
groups of the population. The random variable&nbsp;<span
class="math inline">A</span> is often called a <em>sensitive
attribute</em> in the technical literature.</p>
<p>Note that formally we can always represent any number of discrete
protected categories as a single discrete attribute whose support
corresponds to each of the possible settings of the original attributes.
Consequently, our formal treatment in this chapter does apply to the
case of multiple protected categories.</p>
<p>The fact that we allocate a special random variable for group
membership does not mean that we can cleanly partition the set of
features into two independent categories such as “neutral” and
“sensitive”. In fact, we will see shortly that sufficiently many
seemingly neutral features can often give high accuracy predictions of
group membership. This should not be surprising. After all, if we think
of <span class="math inline">A</span> as the target variable in a
classification problem, there is reason to believe that the remaining
features would give a non-trivial classifier for <span
class="math inline">A.</span></p>
<p>The choice of sensitive attributes will generally have profound
consequences as it decides which groups of the population we highlight,
and what conclusions we draw from our investigation. The taxonomy
induced by discretization can on its own be a source of harm if it is
too coarse, too granular, misleading, or inaccurate. The act of
classifying status in protected categories, and collecting associated
data, can on its own can be problematic. We will revisit this important
discussion in the next chapter.</p>
<section id="no-fairness-through-unawareness" class="level2">
<h2>No fairness through unawareness</h2>
<p>Some have hoped that removing or ignoring sensitive attributes would
somehow ensure the impartiality of the resulting classifier.
Unfortunately, this practice can be ineffective and even harmful.</p>
<p>In a typical dataset, we have many features that are slightly
correlated with the sensitive attribute. Visiting the website
<code>pinterest.com</code>, for example, has a small statistical
correlation with being
female.<span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle" /><span class="sidenote">As
of August 2017, 58.9% of Pinterest’s users in the United States were
female. See <a
href="https://www.statista.com/statistics/277759/pinterest-gender-usa/">here</a>
(Retrieved 3-27-2018)</span></span> The correlation on its own is too
small to predict someone’s gender with high accuracy. However, if
numerous such features are available, as is the case in a typical
browsing history, the task of predicting gender becomes feasible at high
accuracy levels.</p>
<figure>
<img src="assets/redundant.svg"
alt="On the left, we see the distribution of a single feature that differs only very slightly between the two groups. In both groups the feature follows a normal distribution. Only the means are slightly different in each group. Multiple features like this can be used to build a high accuracy group membership classifier. On the right, we see how the accuracy grows as more and more features become available." />
<figcaption aria-hidden="true">On the left, we see the distribution of a
single feature that differs only very slightly between the two groups.
In both groups the feature follows a normal distribution. Only the means
are slightly different in each group. Multiple features like this can be
used to build a high accuracy group membership classifier. On the right,
we see how the accuracy grows as more and more features become
available.</figcaption>
</figure>
<p>Several features that are slightly predictive of the sensitive
attribute can be used to build high accuracy classifiers for that
attribute. In large feature spaces sensitive attributes are generally
<em>redundant</em> given the other features. If a classifier trained on
the original data uses the sensitive attribute and we remove the
attribute, the classifier will then find a redundant encoding in terms
of the other features. This results in an essentially equivalent
classifier, in the sense of implementing the same function.</p>
<p>To further illustrate the issue, consider a fictitious start-up that
sets out to predict your income from your genome. At first, this task
might seem impossible. How could someone’s DNA reveal their income?
However, we know that DNA encodes information about ancestry, which in
turn correlates with income in some countries such as the United States.
Hence, DNA can likely be used to predict income better than random
guessing. The resulting classifier uses ancestry in an entirely implicit
manner. Removing redundant encodings of ancestry from the genome is a
difficult task that cannot be accomplished by removing a few individual
genetic markers. What we learn from this is that machine learning can
wind up building classifiers for sensitive attributes without explicitly
being asked to, simply because it is an available route to improving
accuracy.</p>
<p>Redundant encodings typically abound in large feature spaces. What
about small hand-curated feature spaces? In some studies, features are
chosen carefully so as to be roughly statistically independent of each
other. In such cases, the sensitive attribute may not have good
redundant encodings. That does not mean that removing it is a good idea.
Medication, for example, sometimes depends on race in legitimate ways if
these correlate with underlying causal factors.<span class="citation"
data-cites="bonham2016will"><span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle" /><span class="sidenote">Bonham,
Callier, and Royal, <span>“Will Precision Medicine Move Us Beyond
Race?”</span> <em>The New England Journal of Medicine</em> 374, no. 21
(2016): 2003.</span></span></span> Forcing medications to be
uncorrelated with race in such cases can harm the individual.</p>
</section>
</section>
<section id="statistical-non-discrimination-criteria" class="level1">
<h1>Statistical non-discrimination criteria</h1>
<p>Statistical non-discrimination criteria aim to define the absence of
discrimination in terms of statistical expressions involving random
variables describing a classification or decision making scenario.<br />
Formally, statistical non-discrimination criteria are properties of the
joint distribution of the sensitive attribute <span
class="math inline">A</span>, the target variable <span
class="math inline">Y</span>, the classifier&nbsp;<span
class="math inline">\hat Y</span> or score <span
class="math inline">R,</span> and in some cases also features&nbsp;<span
class="math inline">X</span>. This means that we can unambiguously
decide whether or not a criterion is satisfied by looking at the joint
distribution of these random variables.</p>
<p>Broadly speaking, different statistical fairness criteria all
equalize some group-dependent statistical quantity across groups defined
by the different settings of&nbsp;<span class="math inline">A</span>.
For example, we could ask to equalize acceptance rates across all
groups. This corresponds to imposing the constraint for all
groups&nbsp;<span class="math inline">a</span> and <span
class="math inline">b</span>: <span class="math display">
\mathbb{P}\{\hat Y = 1 \mid A=a\} = \mathop\mathbb{P}\{\hat Y=1 \mid
A=b\}\,.
</span> In the case where&nbsp;<span class="math inline">\hat Y\in\{0,
1\}</span> is a binary classifier and we have two groups <span
class="math inline">a</span> and&nbsp;<span
class="math inline">b</span>, we can determine if acceptance rates are
equal in both groups by knowing the three probabilities&nbsp;<span
class="math inline">\mathbb{P}\{\hat Y=1, A=a\},</span> <span
class="math inline">\mathbb{P}\{\hat Y=1,A=b\},</span> and <span
class="math inline">\mathbb{P}\{A=a\}</span> that fully specify the
joint distribution of&nbsp;<span class="math inline">\hat Y</span>
and&nbsp;<span class="math inline">A</span>. We can also estimate the
relevant probabilities given random samples from the joint distribution
using standard statistical arguments that are not the focus of this
chapter.</p>
<p>Researchers have proposed dozens of different criteria, each trying
to capture different intuitions about what is <em>fair</em>. Simplifying
the landscape of fairness criteria, we can say that there are
essentially three fundamentally different ones. Each of these equalizes
one of the following three statistics across all groups:</p>
<ul>
<li>Acceptance rate <span class="math inline">\mathop\mathbb{P}\{\hat Y
= 1\}</span> of a classifier <span class="math inline">\hat
Y</span></li>
<li>Error rates <span class="math inline">\mathop\mathbb{P}\{\hat Y = 0
\mid Y = 1\}</span> and <span
class="math inline">\mathop\mathbb{P}\{\hat Y = 1 \mid Y =0\}</span> of
a classifier <span class="math inline">\hat Y</span></li>
<li>Outcome frequency given score value <span
class="math inline">\mathop\mathbb{P}\{Y = 1 \mid R = r\}</span> of a
score <span class="math inline">R</span></li>
</ul>
<p>The three criteria can be generalized and simplified using three
different (conditional) independence statements. We use the
notation&nbsp;<span class="math inline">U\bot V\mid W</span> to denote
that random variables&nbsp;<span class="math inline">U</span>
and&nbsp;<span class="math inline">V</span> are conditionally
independent given <span
class="math inline">W</span>.<span><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle" /><span class="sidenote">Wasserman’s
text<span class="citation" data-cites="wasserman2010all"> (Wasserman,
<em>All of Statistics: A Concise Course in Statistical Inference</em>
(Springer, 2010))</span> gives an excellent introduction to conditional
independence and its properties.</span></span> This means that
conditional on any setting&nbsp;<span class="math inline">W=w</span>,
the random variables&nbsp;<span class="math inline">U</span>
and&nbsp;<span class="math inline">V</span> are independent.</p>
<table>
<caption>Non-discrimination criteria</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Independence</th>
<th style="text-align: center;">Separation</th>
<th style="text-align: center;">Sufficiency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">R\bot
A</span></td>
<td style="text-align: center;"><span class="math inline">R\bot A \mid
Y</span></td>
<td style="text-align: center;"><span class="math inline">Y\bot A\mid
R</span></td>
</tr>
</tbody>
</table>
<p>Below we will introduce and discuss each of these conditions in
detail. Variants of these criteria arise from different ways of relaxing
them. Earlier on we used the letter&nbsp;<span class="math inline">\hat
Y</span> for a classifier. To avoid notational clutter, we think of a
binary classifier now as a special case of a score function&nbsp;<span
class="math inline">R\in\{0,1\}</span> that has just two values.</p>
<p>This chapter focuses on the mathematical properties of and
relationships between these different criteria. Once we have acquired
familiarity with the technical matter, we’ll have a broader debate
around the moral and normative content of these definitions in Chapter
4.</p>
</section>
<section id="independence" class="level1">
<h1>Independence</h1>
<p>Our first formal criterion requires the sensitive characteristic to
be statistically independent of the score.</p>
<div class="numenv Definition">
<span class="numenv Definition title">Definition 1.</span>
<p>Random variables&nbsp;<span class="math inline">(A, R)</span> satisfy
<em>independence</em> if&nbsp;<span class="math inline">A\bot
R.</span></p>
</div>
<p>Independence has been explored through many equivalent terms or
variants, referred to as <em>demographic parity</em>, <em>statistical
parity</em>, <em>group fairness</em>, <em>disparate impact</em> and
others. In the case of binary classification, independence simplifies to
the condition <span class="math display">\mathbb{P}\{R=1\mid
A=a\}=\mathbb{P}\{R=1\mid A=b\}\,,</span> for all groups&nbsp;<span
class="math inline">a, b.</span> Thinking of the event&nbsp;<span
class="math inline">R=1</span> as “acceptance”, the condition requires
the acceptance rate to be the same in all groups. A relaxation of the
constraint introduces a positive amount of slack&nbsp;<span
class="math inline">\epsilon&gt;0</span> and requires that <span
class="math display">\mathbb{P}\{R=1\mid A=a\}\ge \mathbb{P}\{R=1\mid
A=b\}-\epsilon\,.</span></p>
<p>Note that we can swap&nbsp;<span class="math inline">a</span>
and&nbsp;<span class="math inline">b</span> to get an inequality in the
other direction. An alternative relaxation is to consider a ratio
condition, such as, <span class="math display">\frac{\mathbb{P}\{R=1\mid
A=a\}}
{\mathbb{P}\{R=1\mid A=b\}}\ge1-\epsilon\,.</span> Some have argued
that, for&nbsp;<span class="math inline">\epsilon=0.2,</span> this
condition relates to the <em>80 percent rule</em> that appears in
discussions around disparate impact law.<span class="citation"
data-cites="feldman2015certifying"><span><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle" /><span class="sidenote">Feldman
et al., <span>“Certifying and Removing Disparate Impact,”</span> in
<em>Proc. <span class="math inline">21</span>St <span>SIGKDD</span></em>
(ACM, 2015).</span></span></span></p>
<p>Yet another way to state the independence condition in full
generality is to require that&nbsp;<span class="math inline">A</span>
and&nbsp;<span class="math inline">R</span> must have zero mutual
information&nbsp;<span class="math inline">I(A;R)=0.</span> Mutual
information quantifies the amount of information that one random
variable reveals about the other. We can define it in terms of the more
standard entropy function as&nbsp;<span
class="math inline">I(A;R)=H(A)+H(R)-H(A,R).</span> The characterization
in terms of mutual information leads to useful relaxations of the
constraint. For example, we could require&nbsp;<span
class="math inline">I(A;R)\le\epsilon.</span></p>
<section id="limitations-of-independence" class="level2">
<h2>Limitations of independence</h2>
<p>Independence is pursued as a criterion in many papers, for multiple
reasons. Some argue that the condition reflects an assumption of
equality: All groups have an equal claim to acceptance and resources
should therefore be allocated proportionally. What we encounter here is
a question about the <em>normative</em> significance of independence,
which we extend on in Chapter 4. But there is a more mundane reason for
the prevalence of this criterion, too. Independence has convenient
technical properties, which makes the criterion appealing to machine
learning researchers. It is often the easiest one to work with
mathematically and algorithmically.</p>
<p>However, decisions based on a classifier that satisfies independence
can have undesirable properties (and similar arguments apply to other
statistical critiera). Here is one way in which this can happen, which
is easiest to illustrate if we imagine a callous or ill-intentioned
decision maker. Imagine a company that in group <span
class="math inline">a</span> hires diligently selected applicants at
some rate <span class="math inline">p&gt;0.</span> In group&nbsp;<span
class="math inline">b</span>, the company hires carelessly selected
applicants at the same rate <span class="math inline">p.</span> Even
though the acceptance rates in both groups are identical, it is far more
likely that unqualified applicants are selected in one group than in the
other. As a result, it will appear in hindsight that members of
group <span class="math inline">b</span> performed worse than members of
group <span class="math inline">a,</span> thus establishing a negative
track record for group <span class="math inline">b.</span></p>
<p>This situation might arise without positing malice: the company might
have historically hired employees primarily from group <span
class="math inline">a,</span> giving them a better understanding of this
group. As a technical matter, the company might have substantially more
training data in group <span class="math inline">a,</span> thus
potentially leading to lower error rates of a learned classifier within
that group. The last point is a bit subtle. After all, if both groups
were entirely homogenous in all ways relevant to the classification
task, more training data in one group would equally benefit both. Then
again, the mere fact that we chose to distinguish these two groups
indicates that we believe they might be heterogeneous in relevant
aspects.</p>
</section>
</section>
<section id="separation" class="level1">
<h1>Separation</h1>
<p>Our next criterion engages with the limitation of independence that
we described. In a typical classification problem, there is a difference
between accepting a positive instance or accepting a negative instance.
The target variable&nbsp;<span class="math inline">Y</span> suggests one
way of partitioning the population into strata of equal claim to
acceptance. Viewed this way, the target variable gives us a sense of
<em>merit</em>. A particular demographic group&nbsp;<span
class="math inline">(A=a)</span> may be more or less well represented in
these different strata defined by the target variable. A decision maker
might argue that in such cases it is justified to accept more or fewer
individuals from group&nbsp;<span class="math inline">a</span>.</p>
<p>These considerations motivate a criterion that demands independence
within each stratum of the population defined by target variable. We can
formalize this requirement using a conditional independence
statement.</p>
<div class="numenv Definition">
<span class="numenv Definition title">Definition 2.</span>
<p>Random variables&nbsp;<span class="math inline">(R, A, Y)</span>
satisfy <em>separation</em> if <span class="math inline">R\bot A \mid
Y.</span></p>
</div>
<p>The conditional independence statement applies even if the variables
take on more than two values each. For example, the target variable
might partition the population into many different types of
individuals.</p>
<p>We can display separation as a graphical model in which&nbsp;<span
class="math inline">R</span> is separated from&nbsp;<span
class="math inline">A</span> by the target variable&nbsp;<span
class="math inline">Y</span>:</p>
<figure>
<img src="assets/R-Y-A.svg" style="width:35.0%"
alt="Graphical model representation of separation" />
<figcaption aria-hidden="true">Graphical model representation of
separation</figcaption>
</figure>
<p>If you haven’t seen graphical models before, don’t worry. All this
says is that <span class="math inline">R</span> is conditionally
independent of&nbsp;<span class="math inline">A</span> given&nbsp;<span
class="math inline">Y</span>.</p>
<p>In the case where&nbsp;<span class="math inline">R</span> is a binary
classifier, separation is equivalent to requiring for all
groups&nbsp;<span class="math inline">a,b</span> the two constraints
<span class="math display">
\begin{aligned}
\mathbb{P}\{ R=1 \mid Y=1, A=a\} &amp;= \mathbb{P}\{ R=1 \mid Y=1,
A=b\}\\
\mathbb{P}\{ R=1 \mid Y=0, A=a\} &amp;= \mathbb{P}\{ R=1 \mid Y=0,
A=b\}\,.
\end{aligned}
</span></p>
<p>Recall that&nbsp;<span class="math inline">\mathbb{P}\{R=1 \mid
Y=1\}</span> is called the <em>true positive rate</em> of the
classifier. It is the rate at which the classifier correctly recognizes
positive instances. The <em>false positive rate</em> <span
class="math inline">\mathbb{P}\{R=1 \mid Y=0\}</span> highlights the
rate at which the classifier mistakenly assigns positive outcomes to
negative instances. What separation therefore requires is that all
groups experience the same false negative rate and the same false
positive rate. Consequently, also true positive rates are equalized
between groups, as well as true negative rates.</p>
<p>This interpretation in terms of equality of error rates leads to
natural relaxations. For example, we could only require equality of
false negative rates. A false negative, intuitively speaking,
corresponds to denied opportunity in scenarios where acceptance is
desirable, such as in hiring. In contrast, when the task is to identify
high-risk individuals, as in the case of loan default prediction, it is
common to denote the undesirable outcome as the “positive” class. This
inverts the meaning of false positives and false negatives, and is a
frequent source of terminological confusion.</p>
<section id="why-equalize-error-rates" class="level2">
<h2>Why equalize error rates?</h2>
<p>The idea of equalizing error rates across has been subject to
critique. Much of the debate has to do with the fact that an optimal
predictor need not have equal error rates in all groups. Specifically,
when the propensity of positive outcomes (<span
class="math inline">\mathbb{P}\{Y=1\}</span>) differs between groups, an
optimal predictor will generally have different error rates. In such
cases, enforcing equality of error rates leads to a predictor that
performs worse in some groups than it could be. How is that
<em>fair</em>?</p>
<p>One response is that sepearation puts emphasis on the question: Who
bears the cost of misclassification? A violation of separation
highlights the fact that different groups experience different costs of
misclassification. There is concern that higher error rates coincide
with historically marginalized and disadvantaged groups, thus inflicting
additional harm on these groups.</p>
<p>The act of measuring and reporting group specific error rates can
create an incentive for decision makers to work toward improving error
rates through collecting better datasets and building better models. If
there is no way to improve error rates in some group relative to others,
this raises questions about the legitimate use of machine learning in
such cases. We will return to this normative question in later
chapters.</p>
<p>A second line of concern with the separation criterion relates to the
use of the target variable as a stand-in for merit. Researchers have
rightfully pointed out that in many cases machine learning practitioners
use target variables that reflect existing inequality and injustice. In
such cases, satisfying separation with respect to an inadequate target
variable does no good. This valid concern, however, applies equally to
the use of supervised learning at large in such cases. If we cannot
agree on an adequate target variable, the right action may be to suspend
the use of supervised learning.</p>
<p>These observations hint at the subtle role that non-discrimination
criteria play. Rather than presenting constraints that we can optimize
for without further thought, they can help surface issues with the use
of machine learning in specific scenarios.</p>
</section>
<section id="visualizing-separation" class="level2">
<h2>Visualizing separation</h2>
<p>A binary classifier that satisfies separation must achieve the same
true positive rates and the same false positive rates in all groups. We
can visualize this condition by plotting group-specific ROC curves.</p>
<figure>
<img src="assets/roc_curve_2.svg" title="fig:" style="width:50.0%"
alt="ROC curve by group." />
<figcaption aria-hidden="true">ROC curve by group.</figcaption>
</figure>
<p>We see the ROC curves of a score displayed for each group separately.
The two groups have different curves indicating that not all trade-offs
between true and false positive rate are achievable in both groups. The
trade-offs that are achievable in both groups are precisely those that
lie under both curves, corresponding to the intersection of the regions
enclosed by the curves.</p>
<figure>
<img src="assets/roc_curve_3.svg" title="fig:" style="width:50.0%"
alt="Intersection of area under the curves." />
<figcaption aria-hidden="true">Intersection of area under the
curves.</figcaption>
</figure>
<p>The highlighted region is the <em>feasible region</em> of trade-offs
that we can achieve in all groups. However, the thresholds that achieve
these trade-offs are in general also group-specific. In other words, the
bar for acceptance varies by group.</p>
<p>Trade-offs that are not exactly on the curves, but rather in the
interior of the region, require randomization as we discussed when we
introduced the ROC curve earlier.</p>
</section>
<section id="conditional-acceptance-rates" class="level2">
<h2>Conditional acceptance rates</h2>
<p>A relative of the independence and separation criteria is common in
debates around discrimination. Here, we designate a random
variable&nbsp;<span class="math inline">W</span> and ask for conditional
independence of the decision&nbsp;<span class="math inline">\hat
Y</span> and group status&nbsp;<span class="math inline">A</span>
conditional on the variable&nbsp;<span class="math inline">W</span>.
That is, for all values&nbsp;<span class="math inline">w</span>
that&nbsp;<span class="math inline">W</span> could take on, and all
groups <span class="math inline">a</span> and&nbsp;<span
class="math inline">b</span> we demand: <span class="math display">
\mathop\mathbb{P}\{\hat Y = 1 \mid W=w, A=a\}
= \mathop\mathbb{P}\{\hat Y = 1 \mid W=w, A=b\}
</span> Formally, this is equivalent to replacing&nbsp;<span
class="math inline">Y</span> with&nbsp;<span
class="math inline">W</span> in our definition of separation.
Often&nbsp;<span class="math inline">W</span> corresponds to a subset of
the covariates of&nbsp;<span class="math inline">X</span>. For example,
we might demand that independence holds among all individuals of equal
<em>educational attainment</em>. In this case, we would
choose&nbsp;<span class="math inline">W</span> to reflect educational
attainment. In doing so, we license the decision maker to distinguish
between individuals with different eductional backgrounds. When we apply
this criterion, the burden falls on the proper choice of what to
condition on, which determines whether we detect discrimination or not.
In particular, we must be careful not to condition on the mechanism by
which the decision maker discriminates. For example, an ill-intentioned
decision maker might discriminate by imposing excessive educational
requirements for a specific job, exploiting that this level of education
is distributed unevenly among different groups. We will be able to
return to the question of what to condition on with significantly more
substance once we reach familiarity with causality in Chapter 5.</p>
</section>
</section>
<section id="sufficiency" class="level1">
<h1>Sufficiency</h1>
<p>Our third criterion formalizes that the score already subsumes the
sensitive characteristic for the purpose of predicting the target. This
idea again boils down to a conditional independence statement.</p>
<div class="numenv Definition">
<span class="numenv Definition title">Definition 3.</span>
<p>We say the random variables&nbsp;<span class="math inline">(R, A,
Y)</span> satisfy <em>sufficiency</em> if <span
class="math inline">Y\bot A \mid R.</span></p>
</div>
<p>We can display sufficiency as a graphical model as we did with
separation before.</p>
<figure>
<img src="assets/Y-R-A.svg" style="width:35.0%"
alt="Graphical model representation of sufficiency" />
<figcaption aria-hidden="true">Graphical model representation of
sufficiency</figcaption>
</figure>
<p>Let us write out the definition more explicitly in the binary case
where <span class="math inline">Y\in\{0,1\}.</span> In this case, a
random variable <span class="math inline">R</span> is sufficient
for&nbsp;<span class="math inline">A</span> if and only if for all
groups <span class="math inline">a,b</span> and all values <span
class="math inline">r</span> in the support of <span
class="math inline">R,</span> we have <span
class="math display">\mathbb{P}\{Y=1 \mid R=r, A=a\}=\mathbb{P}\{ Y=1
\mid R=r, A=b\}\,.</span> When&nbsp;<span class="math inline">R</span>
has only two values we recognize this condition as requiring a parity of
positive/negative predictive values across all groups.</p>
<p>While it is often useful to think of sufficiency in terms of positive
and negative predictive values, there’s a useful alternative. Indeed,
sufficiency turns out to be closely related to an important notion
called <em>calibration</em>, as we will discuss next.</p>
<section id="calibration-and-sufficiency" class="level2">
<h2>Calibration and sufficiency</h2>
<p>In some applications it is desirable to be able to interpret the
values of the score functions as if they were probabilities. The notion
of calibration allows us to move in this direction. Restricting our
attention to binary outcome variables, we say that a score <span
class="math inline">R</span> is <em>calibrated</em> with respect to an
outcome variable&nbsp;<span class="math inline">Y</span> if for all
score values <span class="math inline">r\in[0,1],</span> we have <span
class="math display">
\mathbb{P}\{Y=1 \mid R=r\} = r\,.
</span> This condition means that the set of all instances assigned a
score value <span class="math inline">r</span> has an&nbsp;<span
class="math inline">r</span> fraction of positive instances among them.
The condition refers to the group of all individuals receiving a
particular score value. Calibration need not hold in subgroups of the
population. In particular, it’s important not to interpret the score as
an <em>individual probability</em>. Calibration does not tell us
anything about the outcome of a specific individual that receives a
particular value.</p>
<p>From the definition, we can see that sufficiency is closely related
to the idea of calibration. To formalize the connection we say that the
score <span class="math inline">R</span> satisfies <em>calibration by
group</em> if it satisfies <span class="math display">\mathbb{P}\{Y=1
\mid R=r, A=a\} = r\,,</span> for all score values <span
class="math inline">r</span> and groups <span
class="math inline">a.</span> Observe that calibration is the same
requirement at the population level without the conditioning on <span
class="math inline">A.</span></p>
<div class="numenv Fact">
<span class="numenv Fact title">Fact 1.</span>
<p>Calibration by group implies sufficiency.</p>
</div>
<p>Conversely, sufficiency is only slightly weaker than calibration by
group in the sense that a simple renaming of score values goes from one
property to the other.</p>
<div class="numenv Proposition">
<span class="numenv Proposition title">Proposition 1.</span>
<p>If a score <span class="math inline">R</span> satisfies sufficiency,
then there exists a function <span
class="math inline">\ell\colon[0,1]\to[0,1]</span> so that&nbsp;<span
class="math inline">\ell(R)</span> satisfies calibration by group.</p>
</div>
<div class="numenv Proof">
<span class="numenv Proof title">Proof.</span>
<p>Fix any group <span class="math inline">a</span> and put <span
class="math inline">\ell(r) = \mathbb{P}\{Y=1\mid R=r, A=a\}.</span>
Since&nbsp;<span class="math inline">R</span> satisfies sufficiency,
this probability is the same for all groups <span
class="math inline">a</span> and hence this map <span
class="math inline">\ell</span> is the same regardless of what
value <span class="math inline">a</span> we chose.</p>
<p>Now, consider any two groups <span class="math inline">a,b.</span> We
have, <span class="math display">\begin{aligned}
r
&amp;= \mathbb{P}\{Y=1\mid \ell(R)=r, A=a\} \\
&amp;= \mathbb{P}\{Y=1\mid R\in \ell^{-1}(r), A=a\} \\
&amp;= \mathbb{P}\{Y=1\mid R\in \ell^{-1}(r), A=b\} \\
&amp;= \mathbb{P}\{Y=1\mid \ell(R)=r, A=b\}\,,\end{aligned}</span> thus
showing that&nbsp;<span class="math inline">\ell(R)</span> is calibrated
by group.</p>
</div>
<p>We conclude that sufficiency and calibration by group are essentially
equivalent notions.</p>
<p>In practice, there are various heuristics to achieve calibration. For
example, Platt scaling takes a possibly uncalibrated score, treats it as
a single feature, and fits a one variable regression model against the
target variable based on this feature.<span class="citation"
data-cites="platt1999probabilistic"><span><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle" /><span class="sidenote">Platt
et al., <span>“Probabilistic Outputs for Support Vector Machines and
Comparisons to Regularized Likelihood Methods,”</span> <em>Advances in
Large Margin Classifiers</em> 10, no. 3 (1999):
61–74.</span></span></span> We also apply Platt scaling for each of the
groups defined by the sensitive attribute.</p>
</section>
<section
id="calibration-by-group-as-a-consequence-of-unconstrained-learning"
class="level2">
<h2>Calibration by group as a consequence of unconstrained learning</h2>
<p>Sufficiency is often satisfied by the outcome of unconstrained
supervised learning without the need for any explicit intervention. This
should not come as a surprise. After all, the goal of supervised
learning is to approximate an optimal score function. The optimal score
function we saw earlier, however, is calibrated for any group as the
next fact states formally.</p>
<div class="numenv Fact">
<span class="numenv Fact title">Fact.</span>
<p>The optimal score&nbsp;<span class="math inline">r(x) = \mathbb{E}[ Y
\mid X=x]</span> satisfies group calibration for any group.
Specifically, for any set&nbsp;<span class="math inline">S</span> we
have <span class="math display">\mathbb{P}\{Y=1\mid R=r,
X\in S\}=r.</span></p>
</div>
<p>We generally expect a learned score to satisfy sufficiency in cases
where the group membership is either explicitly encoded in the data or
can be predicted from the other attributes. To illustrate this point we
look at the calibration values of a standard machine learning model, a
<em>random forest ensemble</em>, on an income classification task
derived from the American Community Survey of the US Census Bureau.<span
class="citation"
data-cites="ding2021retiring"><span><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle" /><span class="sidenote">Ding
et al., <span>“Retiring Adult: New Datasets for Fair Machine
Learning,”</span> <em>Advances in Neural Information Processing
Systems</em> 34 (2021).</span></span></span> We restrict the dataset to
the three most populous states, California, Texas, and Florida.</p>
<p>After splitting the data into training and testing data, we fit a
random forest ensemble using the standard Python library
<em>sklearn</em> on the training data. We then examine how
well-calibrated the model is out of the box on test data.</p>
<figure>
<img src="assets/acsincome_calibration.svg"
alt="Group calibration curves on Census ACS data" />
<figcaption aria-hidden="true">Group calibration curves on Census ACS
data</figcaption>
</figure>
<p>We see that the calibration curves for the three largest racial
groups in the dataset, which the Census Bureau codes as “White alone”,
“Black or African American alone”, and “Asian alone”, are very close to
the main diagonal. This means that the scores derived from our random
forest model satisfy calibration by group up to small error. The same is
true when looking at the two groups “Male” and “Female” in the
dataset.</p>
<p>These observations are no coincidence. Theory shows that under
certain technical conditions, unconstrained supervised learning does, in
fact, imply group calibration.<span class="citation"
data-cites="liu2019implicit"><span><label for="sn-7" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-7" class="margin-toggle" /><span class="sidenote">Liu,
Simchowitz, and Hardt, <span>“The Implicit Fairness Criterion of
Unconstrained Learning,”</span> in <em>International Conference on
Machine Learning</em> (PMLR, 2019), 4051–60.</span></span></span> Note,
however, that for this to be true the classifier must be able to detect
group membership. If detecting group membership is impossible, then
group calibration generally fails.</p>
<p>The lesson is that sufficiency often comes for free (at least
approximately) as a consequence of standard machine learning practices.
The flip side is that imposing sufficiency as a constraint on a
classification system may not be much of an intervention. In particular,
it would not effect a substantial change in current practices.</p>
</section>
</section>
<section id="how-to-achieve-a-non-discrimination-criterion"
class="level1">
<h1>How to achieve a non-discrimination criterion</h1>
<p>Now that we have formally introduced three non-discrimination
criteria, it is worth asking how we can achieve them algorithmically. We
distinguish between three different techniques. While they generally
apply to all the criteria and their relaxations that we review in this
chapter, our discussion here focuses on independence.</p>
<ul>
<li><strong>Pre-processing</strong>: Adjust the feature space to be
uncorrelated with the sensitive attribute.</li>
<li><strong>In-training</strong>: Work the constraint into the
optimization process that constructs a classifier from training
data.</li>
<li><strong>Post-processing</strong>: Adjust a learned classifier so as
to be uncorrelated with the sensitive attribute.</li>
</ul>
<p>The three approaches have different strengths and weaknesses.</p>
<p>Pre-processing is a family of techniques to transform a feature space
into a representation that as a whole is independent of the sensitive
attribute. This approach is generally agnostic to what we do with the
new feature space in downstream applications. After the pre-processing
transformation ensures independence, any deterministic training process
on the new space will also satisfy independence. This is a formal
consequence of the well-known data processing inequality from
information theory.<span class="citation"
data-cites="cover1999elements"><span><label for="sn-8" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-8" class="margin-toggle" /><span class="sidenote">Cover,
<em>Elements of Information Theory</em> (John Wiley &amp; Sons,
1999).</span></span></span></p>
<p>Achieving independence at training time can lead to the highest
utility since we get to optimize the classifier with this criterion in
mind. The disadvantage is that we need access to the raw data and
training pipeline. We also give up a fair bit of generality as this
approach typically applies to specific model classes or optimization
problems.</p>
<p>Post-processing refers to the process of taking a trained classifier
and adjusting it possibly depending on the sensitive attribute and
additional randomness in such a way that independence is achieved.
Formally, we say a <em>derived classifier</em> <span
class="math inline">\hat Y = F(R, A)</span> is a possibly randomized
function of a given score <span class="math inline">R</span> and the
sensitive attribute. Given a cost for false negatives and false
positives, we can find the derived classifier that minimizes the
expected cost of false positive and false negatives subject to the
fairness constraint at hand. Post-processing has the advantage that it
works for any <em>black-box</em> classifier regardless of its inner
workings. There’s no need for re-training, which is useful in cases
where the training pipeline is complex. It’s often also the only
available option when we have access only to a trained model with no
control over the training process. These advantages of post-processing
are simultaneously also a weakness as it often leads to a significant
loss in utility.</p>
</section>
<section id="relationships-between-criteria" class="level1">
<h1>Relationships between criteria</h1>
<p>The criteria we reviewed constrain the joint distribution in
non-trivial ways. We should therefore suspect that imposing any two of
them simultaneously over-constrains the space to the point where only
degenerate solutions remain. We will now see that this intuition is
largely correct.</p>
<p>What this shows is that we cannot impose multiple criteria as hard
constraints. This leaves open the possibility that meaningful trade-offs
between these different criteria exist.</p>
<section id="independence-versus-sufficiency" class="level2">
<h2>Independence versus sufficiency</h2>
<p>We begin with a simple proposition that shows how in general
independence and sufficiency are mutually exclusive. The only assumption
needed here is that the sensitive attribute&nbsp;<span
class="math inline">A</span> and the target variable <span
class="math inline">Y</span> are <em>not</em> independent. This is a
different way of saying that group membership has an effect on the
statistics of the target variable. In the binary case, this means one
group has a higher rate of positive outcomes than another. Think of this
as the typical case.</p>
<div class="numenv Proposition">
<span class="numenv Proposition title">Proposition 2.</span>
<p>Assume that&nbsp;<span class="math inline">A</span> and&nbsp;<span
class="math inline">Y</span> are not independent. Then sufficiency and
independence cannot both hold.</p>
</div>
<div class="numenv Proof">
<span class="numenv Proof title">Proof.</span>
<p>By the contraction rule for conditional independence, <span
class="math display">
A\bot R \quad\mathrm{and}\quad A\bot Y \mid R
\quad\Longrightarrow\quad
A\bot (Y, R)
\quad\Longrightarrow\quad
A\bot Y\,.
</span> To be clear,&nbsp;<span class="math inline">A\bot (Y, R)</span>
means that&nbsp;<span class="math inline">A</span> is independent of the
pair of random variables&nbsp;<span class="math inline">(Y,R).</span>
Dropping&nbsp;<span class="math inline">R</span> cannot introduce a
dependence between&nbsp;<span class="math inline">A</span>
and&nbsp;<span class="math inline">Y.</span></p>
<p>In the contrapositive, <span class="math display">A\not\bot Y
\quad\Longrightarrow\quad
A\not\bot R
\quad\mathrm{or}\quad
A\not\bot R \mid Y\,.</span></p>
</div>
</section>
<section id="independence-versus-separation" class="level2">
<h2>Independence versus separation</h2>
<p>An analogous result of mutual exclusion holds for independence and
separation. The statement in this case is a bit more contrived and
requires the additional assumption that the target variable <span
class="math inline">Y</span> is binary. We also additionally need that
the score is not independent of the target. This is a rather mild
assumption, since any useful score function should have correlation with
the target variable.</p>
<div class="numenv Proposition">
<span class="numenv Proposition title">Proposition 3.</span>
<p>Assume&nbsp;<span class="math inline">Y</span> is binary,&nbsp;<span
class="math inline">A</span> is not independent of&nbsp;<span
class="math inline">Y</span>, and&nbsp;<span
class="math inline">R</span> is not independent of&nbsp;<span
class="math inline">Y.</span> Then, independence and separation cannot
both hold.</p>
</div>
<div class="numenv Proof">
<span class="numenv Proof title">Proof.</span>
<p>Assume&nbsp;<span class="math inline">Y\in\{0,1\}.</span> In its
contrapositive form, the statement we need to show is</p>
<p><span class="math display">
A\bot R
\quad\mathrm{and}\quad
A\bot R \mid Y
\quad\Longrightarrow\quad
A\bot Y
\quad\mathrm{or}\quad
R\bot Y
</span></p>
<p>By the law of total probability,</p>
<p><span class="math display">
\mathbb{P}\{R=r\mid A=a\}
=\sum_y \mathbb{P}\{R=r\mid A=a, Y=y\}\mathbb{P}\{Y=y\mid A=a\}
</span></p>
<p>Applying the assumption&nbsp;<span class="math inline">A\bot R</span>
and&nbsp;<span class="math inline">A\bot R\mid Y,</span> this equation
simplifies to</p>
<p><span class="math display">
\mathbb{P}\{R=r\}
=\sum_y \mathbb{P}\{R=r\mid Y=y\}\mathbb{P}\{Y=y\mid A=a\}
</span></p>
<p>Applied differently, the law of total probability also gives <span
class="math display">
\mathbb{P}\{R=r\}
=\sum_y \mathbb{P}\{R=r\mid Y=y\}\mathbb{P}\{Y=y\}
</span></p>
<p>Combining this with the previous equation, we have <span
class="math display">
\sum_y \mathbb{P}\{R=r\mid Y=y\}\mathbb{P}\{Y=y\}
=\sum_y \mathbb{P}\{R=r\mid Y=y\}\mathbb{P}\{Y=y\mid A=a\}
</span></p>
<p>Careful inspection reveals that when&nbsp;<span
class="math inline">y</span> ranges over only two values, this equation
can only be satisfied if&nbsp;<span class="math inline">A\bot Y</span>
or&nbsp;<span class="math inline">R\bot Y.</span></p>
<p>Indeed, we can rewrite the equation more compactly using the symbols
<span class="math inline">p=\mathbb{P}\{Y=0\},</span> <span
class="math inline">p_a=\mathbb{P}\{Y=0\mid A=a\},</span> <span
class="math inline">r_y=\mathbb{P}\{R=r\mid Y=y\},</span> as:</p>
<p><span class="math display">
pr_0 + (1-p)r_1 = p_ar_0 + (1-p_a)r_1.
</span></p>
<p>Equivalently,&nbsp;<span class="math inline">p(r_0 -r_1) =
p_a(r_0-r_1).</span></p>
<p>This equation can only be satisfied if&nbsp;<span
class="math inline">r_0=r_1,</span> in which case&nbsp;<span
class="math inline">R\bot Y</span>, or if <span class="math inline">p =
p_a</span> for all&nbsp;<span class="math inline">a,</span> in which
case&nbsp;<span class="math inline">Y\bot A.</span></p>
</div>
<p>The claim is not true when the target variable can assume more than
two values, which is a natural case to consider.</p>
</section>
<section id="separation-versus-sufficiency" class="level2">
<h2>Separation versus sufficiency</h2>
<p>Finally, we turn to the relationship between separation and
sufficiency. Both ask for a non-trivial conditional independence
relationship between the three variables&nbsp;<span
class="math inline">A, R, Y.</span> Imposing both simultaneously leads
to a degenerate solution space, as our next proposition confirms.</p>
<div class="numenv Proposition">
<span class="numenv Proposition title">Proposition 4.</span>
<p>Assume that all events in the joint distribution of <span
class="math inline">(A, R, Y)</span> have positive probability, and
assume&nbsp;<span class="math inline">A\not\bot Y.</span> Then,
separation and sufficiency cannot both hold.</p>
</div>
<div class="numenv Proof">
<span class="numenv Proof title">Proof.</span>
<p>A standard
fact<span><label for="sn-9" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-9" class="margin-toggle" /><span class="sidenote">See
Theorem 17.2 in <span class="citation" data-cites="wasserman2010all">
(Wasserman, <em>All of Statistics</em>)</span></span></span> about
conditional independence shows</p>
<p><span class="math display">A\bot R \mid Y\quad\text{and}\quad A\bot
Y\mid R
\quad\implies\quad A\bot (R, Y)\,.</span> Moreover, <span
class="math display">A\bot (R,Y)\quad\implies\quad A\bot
R\quad\text{and}\quad A\bot Y\,.</span> Taking the contrapositive
completes the proof.</p>
</div>
<p>For a binary target, the non-degeneracy assumption in the previous
proposition states that in all groups, at all score values, we have both
positive and negative instances. In other words, the score value never
fully resolves uncertainty regarding the outcome.</p>
<p>The proposition also applies to binary classifiers. Here, the
assumption says that within each group the classifier must have nonzero
true positive, false positive, true negative, and false negative rates.
We can weaken this assumption a bit and require only that the classifier
is imperfect in the sense of making at least one false positive
prediction. What’s appealing about the resulting claim is that its proof
essentially only uses a well-known relationship between true positive
rate (recall) and positive predictive value (precision). This trade-off
is often called <em>precision-recall trade-off</em>.</p>
<div class="numenv Proposition">
<span class="numenv Proposition title">Proposition 5.</span>
<p>Assume&nbsp;<span class="math inline">Y</span> is not independent
of <span class="math inline">A</span> and assume&nbsp;<span
class="math inline">\hat Y</span> is a binary classifier with nonzero
false positive rate. Then, separation and sufficiency cannot both
hold.</p>
</div>
<div class="numenv Proof">
<span class="numenv Proof title">Proof.</span>
<p>Since&nbsp;<span class="math inline">Y</span> is not independent
of <span class="math inline">A</span> there must be two groups, call
them&nbsp;<span class="math inline">0</span> and&nbsp;<span
class="math inline">1</span>, such that <span
class="math display">p_0=\mathbb{P}\{Y=1\mid A=0\}\ne
\mathbb{P}\{Y=1\mid A=1\}=p_1\,.</span> Now suppose that separation
holds. Since the classifier is imperfect this means that all groups have
the same non-zero false positive rate <span
class="math inline">\mathrm{FPR}&gt;0,</span> and the same true positive
rate <span class="math inline">\mathrm{TPR}\ge0.</span> We will show
that sufficiency does not hold.</p>
<p>Recall that in the binary case, sufficiency implies that all groups
have the same positive predictive value. The positive predictive value
in group&nbsp;<span class="math inline">a,</span> denoted&nbsp;<span
class="math inline">\mathrm{PPV}_a</span> satisfies <span
class="math display">\mathrm{PPV_a}
= \frac{\mathrm{TPR}p_a}{\mathrm{TPR}p_a+\mathrm{FPR}(1-p_a)}\,.</span>
From the expression we can see that&nbsp;<span
class="math inline">\mathrm{PPV}_a=\mathrm{PPV}_b</span> only if <span
class="math inline">\mathrm{TPR}=0</span> or&nbsp;<span
class="math inline">\mathrm{FPR}=0.</span> The latter is ruled out by
assumption. So it must be that&nbsp;<span
class="math inline">\mathrm{TPR}=0.</span> However, in this case, we can
verify that the negative predictive value&nbsp;<span
class="math inline">\mathrm{NPV}_0</span> in group <span
class="math inline">0</span> must be different from the negative
predictive value <span class="math inline">\mathrm{NPV}_1</span> in
group <span class="math inline">1.</span> This follows from the
expression <span class="math display">\mathrm{NPV_a}
=
\frac{(1-\mathrm{FPR})(1-p_a)}{(1-\mathrm{TPR})p_a+(1-\mathrm{FPR})(1-p_a)}\,.</span>
Hence, sufficiency does not hold.</p>
</div>
</section>
</section>
<section id="case-study-credit-scoring" class="level1">
<h1>Case study: Credit scoring</h1>
<p>We now apply some of the notions we saw to credit scoring. Credit
scores support lending decisions by giving an estimate of the risk that
a loan applicant will default on a loan. Credit scores are widely used
in the United States and other countries when allocating credit, ranging
from micro loans to jumbo mortgages. In the United States, there are
three major credit-reporting agencies that collect data on various
lendees. These agencies are for-profit organizations that each offer
risk scores based on the data they collected. FICO scores are a
well-known family of proprietary scores developed by FICO and sold by
the three credit reporting agencies.</p>
<p>Regulation of credit agencies in the United States started with the
Fair Credit Reporting Act, first passed in 1970, that aims to promote
the accuracy, fairness, and privacy of consumer of information collected
by the reporting agencies. The Equal Credit Opportunity Act, a United
States law enacted in 1974, makes it unlawful for any creditor to
discriminate against any applicant the basis of race, color, religion,
national origin, sex, marital status, or age.</p>
<section id="score-distribution" class="level2">
<h2>Score distribution</h2>
<p>Our analysis relies on data published by the Federal Reserve<span
class="citation"
data-cites="federalreserve"><span><label for="sn-10" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-10" class="margin-toggle" /><span class="sidenote">The
Federal Reserve Board, <span>“Report to the Congress on Credit Scoring
and Its Effects on the Availability and Affordability of Credit”</span>
(<a
href="https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/"
class="uri"
role="doc-biblioref">https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/</a>,
2007).</span></span></span>. The dataset provides aggregate statistics
from 2003 about a credit score, demographic information (race or
ethnicity, gender, marital status), and outcomes (to be defined
shortly). We’ll focus on the joint statistics of score, race, and
outcome, where the race attributes assume four values detailed
below.<span><label for="sn-11" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-11" class="margin-toggle" /><span class="sidenote">These
numbers come from the “Estimation sample” column of Table 9 on this <a
href="https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/datamodel_tables.htm">web
page</a>.</span></span></p>
<table>
<caption>Credit score distribution by ethnicity</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Race or ethnicity</th>
<th style="text-align: center;">Samples with both score and outcome</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">White</td>
<td style="text-align: center;">133,165</td>
</tr>
<tr class="even">
<td style="text-align: center;">Black</td>
<td style="text-align: center;">18,274</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Hispanic</td>
<td style="text-align: center;">14,702</td>
</tr>
<tr class="even">
<td style="text-align: center;">Asian</td>
<td style="text-align: center;">7,906</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Total</td>
<td style="text-align: center;">174,047</td>
</tr>
</tbody>
</table>
<p>The score used in the study is based on the TransUnion TransRisk
score. TransUnion is a US credit-reporting agency. The TransRisk score
is in turn based on a proprietary model created by FICO, hence often
referred to as FICO scores. The Federal Reserve renormalized the scores
for the study to vary from 0 to 100, with 0 being <em>least
creditworthy</em>.</p>
<p>The information on race was provided by the Social Security
Administration, thus relying on self-reported values. The cumulative
distribution of these credit scores strongly depends on the racial group
as the next figure reveals.</p>
<figure>
<img src="assets/credit_cdf.svg" style="width:75.0%"
alt="Cumulative density of scores by group." />
<figcaption aria-hidden="true">Cumulative density of scores by
group.</figcaption>
</figure>
</section>
<section id="performance-variables-and-roc-curves" class="level2">
<h2>Performance variables and ROC curves</h2>
<p>As is often the case, the outcome variable is a subtle aspect of this
data set. Its definition is worth emphasizing. Since the score model is
proprietary, it is not clear what target variable was used during the
training process. What is it then that the score is trying to predict?
In a first reaction, we might say that the goal of a credit score is to
predict a <em>default</em> outcome. However, that’s not a clearly
defined notion. Defaults vary in the amount of debt recovered, and the
amount of time given for recovery. Any single binary performance
indicator is typically an oversimplification.</p>
<p>What is available in the Federal Reserve data is a so-called
<em>performance</em> variable that measures a <em>serious delinquency in
at least one credit line of a certain time period</em>. More
specifically, the Federal Reserve states</p>
<blockquote>
<p>(the) measure is based on the performance of new or existing accounts
and measures whether individuals have been late 90 days or more on one
or more of their accounts or had a public record item or a new
collection agency account during the performance period.</p>
</blockquote>
<p>With this performance variable at hand, we can look at the ROC curve
to get a sense of how predictive the score is in different
demographics.</p>
<figure>
<img src="assets/credit_roc_curve_both.svg"
alt="ROC curve of credit score by group." />
<figcaption aria-hidden="true">ROC curve of credit score by
group.</figcaption>
</figure>
<p>The meaning of true positive rate is <em>the rate of predicted
positive performance given positive performance.</em> Similarly, false
positive rate is <em>the rate of predicted negative performance given a
positive performance</em>.</p>
<p>We see that the shapes appear roughly visually similar in the groups,
although the ‘White’ group encloses a noticeably larger area under the
curve than the ‘Black’ group. Also note that even two ROC curves with
the same shape can correspond to very different score functions. A
particular trade-off between true positive rate and false positive rate
achieved at a threshold&nbsp;<span class="math inline">t</span> in one
group could require a different threshold&nbsp;<span
class="math inline">t&#39;</span> in the other group.</p>
</section>
<section id="comparison-of-different-criteria" class="level2">
<h2>Comparison of different criteria</h2>
<p>With the score data at hand, we compare four different classification
strategies:</p>
<ul>
<li><strong>Maximum profit</strong>: Pick possibly group-dependent score
thresholds in a way that maximizes profit.</li>
<li><strong>Single threshold</strong>: Pick a single uniform score
threshold for all groups in a way that maximizes profit.</li>
<li><strong>Independence</strong>: Achieve an equal acceptance rate in
all groups. Subject to this constraint, maximize profit.</li>
<li><strong>Separation</strong>: Achieve an equal true/false positive
rate in all groups. Subject to this constraint, maximize profit.</li>
</ul>
<p>To make sense of maximizing profit, we need to assume a reward for a
true positive (correctly predicted positive performance), and a cost for
false positives (negative performance predicted as positive). In
lending, the cost of a false positive is typically many times greater
than the reward for a true positive. In other words, the interest
payments resulting from a loan are relatively small compared with the
loan amount that could be lost. For illustrative purposes, we imagine
that the cost of a false positive is 6 times greater than the return on
a true positive. The absolute numbers don’t matter. Only the ratio
matters. This simple cost structure glosses over a number of details
that are likely relevant for the lender such as the terms of the
loan.</p>
<p>There is another major caveat to the kind of analysis we’re about to
do. Since we’re only given aggregate statistics, we cannot retrain the
score with a particular classification strategy in mind. The only thing
we can do is to define a setting of thresholds that achieves a
particular criterion. This approach may be overly pessimistic with
regards to the profit achieved subject to each constraint. For this
reason and the fact that our choice of cost function was rather
arbitrary, we do not state the profit numbers. The numbers can be found
in the original analysis<span class="citation"
data-cites="hardt2016equality"><span><label for="sn-12" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-12" class="margin-toggle" /><span class="sidenote">Hardt,
Price, and Srebro, <span>“Equality of Opportunity in Supervised
Learning,”</span> in <em>Proc. <span class="math inline">29</span>Th
<span>NIPS</span></em>, 2016, 3315–23.</span></span></span>, which
reports that ‘single threshold’ achieves higher profit than
‘separation’, which in turn achieves higher profit than
‘independence’.</p>
<p>What we do instead is to look at the different trade-offs between
true and false positive rate that each criterion achieves in each
group.</p>
<figure>
<img src="assets/credit_roc_curve_with_thresholds.svg"
style="width:75.0%"
alt="ROC curves with optimal thresholds for different criteria." />
<figcaption aria-hidden="true">ROC curves with optimal thresholds for
different criteria.</figcaption>
</figure>
<p>We can see that even though the ROC curves are somewhat similar, the
resulting trade-offs can differ widely by group for some of the
criteria. The true positive rate achieved by <em>max profit</em> for the
Asian group is twice of what it is for the Black group. The separation
criterion, of course, results in the same trade-off in all groups.
Independence equalizes acceptance rate, but leads to widely different
trade-offs. For instance, the Black group has a false positive rate more
than three times higher than the false positive rate of the Asian
group.</p>
</section>
<section id="calibration-values" class="level2">
<h2>Calibration values</h2>
<p>Finally, we consider the non-default rate by group. This corresponds
to the calibration plot by group.</p>
<figure>
<img src="assets/credit_performance.svg" style="width:100.0%"
alt="Calibration values of credit score by group." />
<figcaption aria-hidden="true">Calibration values of credit score by
group.</figcaption>
</figure>
<p>We see that the performance curves by group are reasonably well
aligned. This means that a monotonic transformation of the score values
would result in a score that is roughly calibrated by group according to
our earlier definition. Due to the differences in score distribution by
group, it could nonetheless be the case that thresholding the score
leads to a classifier with different positive predictive values in each
group. Calibration is typically lost when taking a multi-valued score
and making it binary.</p>
</section>
</section>
<section id="inherent-limitations-of-observational-criteria"
class="level1">
<h1>Inherent limitations of observational criteria</h1>
<p>All criteria we’ve seen so far have one important aspect in common.
They are properties of the joint distribution of the score, sensitive
attribute, and the target variable. In other words, if we know the joint
distribution of the random variables&nbsp;<span class="math inline">(R,
A, Y),</span> we can without ambiguity determine whether this joint
distribution satisfies one of these criteria or not. For example, if all
variables are binary, there are eight numbers specifying the joint
distributions. We can verify each of the criteria we discussed in this
chapter by looking only at these eight numbers and nothing else.</p>
<p>We can broaden this notion a bit and also include all other features,
not just the group attribute. So, let’s call a criterion
<em>observational</em> if it is a property of the joint distribution of
the features&nbsp;<span class="math inline">X,</span> the sensitive
attribute <span class="math inline">A,</span> a score function <span
class="math inline">R</span> and an outcome variable <span
class="math inline">Y.</span> Intuitively speaking, a criterion is
observational if we can write it down unambiguously using probability
statements involving the random variables at hand.</p>
<p>Observational definitions have many appealing aspects. They’re often
easy to state and require only a lightweight formalism. They make no
reference to the inner workings of the classifier, the decision maker’s
intent, the impact of the decisions on the population, or any notion of
whether and how a feature actually influences the outcome. We can reason
about them fairly conveniently as we saw earlier. In principle,
observational definitions can always be verified given samples from the
joint distribution—subject to statistical sampling error.</p>
<p>This simplicity of observational definitions also leads to inherent
limitations. What observational definitions hide are the mechanisms that
created an observed disparity. In one case, a difference in acceptance
rate could be due to spiteful consideration of group membership by a
decision maker. In another case, the difference in acceptance rates
could reflect an underlying inequality in society that gives one group
an advantage in getting accepted. While both are cause for concern, in
the first case discrimination is a direct action of the decision maker.
In the the other case, the locus of discrimination may be outside the
agency of the decision maker.</p>
<p>Observational criteria cannot, in general, give satisfactory answers
as to what the causes and mechanisms of discrimination are. Subsequent
chapters, in particular our chapter on causality, develop tools to go
beyond the scope of observational criteria.</p>
</section>
<section id="chapter-notes" class="level1">
<h1>Chapter notes</h1>
<p>For the early history of probability and the rise of statistical
thinking, turn to books by Hacking<span class="citation"
data-cites="hacking1990taming hacking2006emergence"><span><label for="sn-13" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-13" class="margin-toggle" /><span class="sidenote">Hacking,
Hacking, et al., <em>The Taming of Chance</em>, 17 (Cambridge University
Press, 1990); Hacking, <em>The Emergence of Probability: A Philosophical
Study of Early Ideas about Probability, Induction and Statistical
Inference</em> (Cambridge University Press, 2006).</span></span></span>,
Porter<span class="citation"
data-cites="porter2020rise"><span><label for="sn-14" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-14" class="margin-toggle" /><span class="sidenote">Porter,
<em>The Rise of Statistical Thinking,
1820–1900</em>.</span></span></span>, and Desrosières<span
class="citation"
data-cites="desrosieres1998politics"><span><label for="sn-15" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-15" class="margin-toggle" /><span class="sidenote">Desrosières,
<em>The Politics of Large Numbers</em>.</span></span></span>.</p>
<p>The satistical decision theory we covered in this chapter is also
called (signal) detection theory and is the subject of various
textbooks. What we call classification is also called prediction in
other contexts. Likewise, classifiers are often called predictors. For a
graduate introduction, see the text by Hardt and Recht.<span
class="citation"
data-cites="hardtrecht2022patterns"><span><label for="sn-16" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-16" class="margin-toggle" /><span class="sidenote">Hardt
and Recht, <em>Patterns, Predictions, and Actions: Foundations of
Machine Learning</em> (Princeton University Press,
2022).</span></span></span></p>
<p>Similar fairness criteria to the ones reviewed in this chapter were
already known in the 1960s and 70s, primarily in the education testing
and psychometrics literature.<span class="citation"
data-cites="hutchinson201950"><span><label for="sn-17" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-17" class="margin-toggle" /><span class="sidenote">Hutchinson
and Mitchell, <span>“50 Years of Test (Un) Fairness: Lessons for Machine
Learning,”</span> in <em>Proc. <span class="math inline">2</span>Nd
Conference on <span class="nocase">Fairness, Accountability, and
Transparency (FAccT)</span></em>, 2019, 49–58.</span></span></span> The
first and most influential fairness criterion in this context is due to
Cleary.<span class="citation"
data-cites="cleary1966test cleary1968test"><span><label for="sn-18" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-18" class="margin-toggle" /><span class="sidenote">Cleary,
<span>“Test Bias: Validity of the Scholastic Aptitude Test for Negro and
White Students in Integrated Colleges,”</span> <em>ETS Research Bulletin
Series</em> 1966, no. 2 (1966): i–23; Cleary, <span>“Test Bias:
Prediction of Grades of Negro and White Students in Integrated
Colleges,”</span> <em>Journal of Educational Measurement</em> 5, no. 2
(1968): 115–24.</span></span></span> A score passes Cleary’s criterion
if knowledge of group membership does not help in predicting the outcome
from the score with a linear model. This condition follows from
sufficiency and can be expressed by replacing the conditional
independence statement with an analogous statement about partial
correlations.<span class="citation"
data-cites="darlington1971another"><span><label for="sn-19" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-19" class="margin-toggle" /><span class="sidenote">Darlington,
<span>“Another Look at <span>‘Cultural Fairness’</span>,”</span>
<em>Journal of Educational Measurement</em> 8, no. 2 (1971):
71–82.</span></span></span></p>
<p>Einhorn and Bass<span class="citation"
data-cites="einhorn1971methodological"><span><label for="sn-20" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-20" class="margin-toggle" /><span class="sidenote">Einhorn
and Bass, <span>“Methodological Considerations Relevant to
Discrimination in Employment Testing.”</span> <em>Psychological
Bulletin</em> 75, no. 4 (1971): 261.</span></span></span> considered
equality of precision values, which is a relaxation of sufficiency as we
saw earlier. Thorndike<span class="citation"
data-cites="thorndike1971concepts"><span><label for="sn-21" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-21" class="margin-toggle" /><span class="sidenote">Thorndike,
<span>“Concepts of Culture-Fairness,”</span> <em>Journal of Educational
Measurement</em> 8, no. 2 (1971): 63–70.</span></span></span> considered
a weak variant of calibration by which the frequency of positive
predictions must equal the frequency of positive outcomes in each group,
and proposed achieving it via a post-processing step that sets different
thresholds in different groups. Thorndike’s criterion is incomparable to
sufficiency in general.</p>
<p>Darlington<span class="citation"
data-cites="darlington1971another"><span><label for="sn-22" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-22" class="margin-toggle" /><span class="sidenote">Darlington,
<span>“Another Look at <span>‘Cultural
Fairness’</span>.”</span></span></span></span> stated four different
criteria in terms of succinct expressions involving the correlation
coefficients between various pairs of random variables. These criteria
include independence, a relaxation of sufficiency, a relaxation of
separation, and Thorndike’s criterion. Darlington included an intuitive
visual argument showing that the four criteria are incompatible except
in degenerate cases. Lewis<span class="citation"
data-cites="lewis1978comparison"><span><label for="sn-23" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-23" class="margin-toggle" /><span class="sidenote">Lewis,
<span>“A Comparison of Three Models for Determining Test
Fairness”</span> (Federal Aviation Administration Washington DC Office
of Aviation Medicine, 1978).</span></span></span> reviewed three
fairness criteria including equal precision and equal true/false
positive rates.</p>
<p>These important early works were re-discovered later in the machine
learning and data mining community.<span class="citation"
data-cites="hutchinson201950"><span><label for="sn-24" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-24" class="margin-toggle" /><span class="sidenote">Hutchinson
and Mitchell, <span>“50 Years of Test (Un)
Fairness.”</span></span></span></span> Numerous works considered
variants of independence as a fairness constraint.<span class="citation"
data-cites="calders2009building kamiran2009classifying"><span><label for="sn-25" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-25" class="margin-toggle" /><span class="sidenote">Calders,
Kamiran, and Pechenizkiy, <span>“Building Classifiers with Independency
Constraints,”</span> in <em>In Proc. <span>IEEE</span>
<span>ICDMW</span></em>, 2009, 13–18; Kamiran and Calders,
<span>“Classifying Without Discriminating,”</span> in <em>Proc. <span
class="math inline">2</span>Nd International Conference on Computer,
Control and Communication</em>, 2009.</span></span></span> Feldman et
al.<span class="citation"
data-cites="feldman2015certifying"><span><label for="sn-26" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-26" class="margin-toggle" /><span class="sidenote">Feldman
et al., <span>“Certifying and Removing Disparate
Impact.”</span></span></span></span> studied a relaxation of demographic
parity in the context of disparate impact law. Zemel et al.<span
class="citation"
data-cites="zemel2013learning"><span><label for="sn-27" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-27" class="margin-toggle" /><span class="sidenote">Zemel
et al., <span>“Learning Fair Representations,”</span> in <em>Proc. <span
class="math inline">30</span>Th <span>ICML</span></em>,
2013.</span></span></span> adopted the mutual information viewpoint and
proposed a heuristic pre-processing approach for minimizing mutual
information. As early as 2012, Dwork et al.<span class="citation"
data-cites="dwork2012fairness"><span><label for="sn-28" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-28" class="margin-toggle" /><span class="sidenote">Dwork
et al., <span>“Fairness Through Awareness,”</span> in <em>Proc. <span
class="math inline">3</span>Rd <span>ITCS</span></em>, 2012,
214–26.</span></span></span> argued that the independence criterion was
inadequate as a fairness constraint. In particular, this work identified
the problem with independence we discussed in this chapter.</p>
<p>The separation criterion appeared under the name <em>equalized
odds</em><span class="citation"
data-cites="hardt2016equality"><span><label for="sn-29" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-29" class="margin-toggle" /><span class="sidenote">Hardt,
Price, and Srebro, <span>“Equality of Opportunity in Supervised
Learning.”</span></span></span></span>, alongside the relaxation to
equal false negative rates, called <em>equality of opportunity.</em>
These criteria also appeared in an independent work<span
class="citation"
data-cites="zafar2017fairnessbeyond"><span><label for="sn-30" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-30" class="margin-toggle" /><span class="sidenote">Zafar
et al., <span>“Fairness Beyond Disparate Treatment &amp; Disparate
Impact: Learning Classification Without Disparate Mistreatment,”</span>
in <em>Proc. <span class="math inline">26</span>Th
<span>WWW</span></em>, 2017.</span></span></span> under different names.
Woodworth et al.<span class="citation"
data-cites="woodworth2017learning"><span><label for="sn-31" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-31" class="margin-toggle" /><span class="sidenote">Woodworth
et al., <span>“Learning Non-Discriminatory Predictors,”</span> in
<em>Proc. <span class="math inline">30</span>Th <span>COLT</span></em>,
2017, 1920–53.</span></span></span> studied a relaxation of separation
stated in terms of correlation coefficients. This relaxation corresponds
to the third criterion studied by Darlington.<span class="citation"
data-cites="darlington1971another"><span><label for="sn-32" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-32" class="margin-toggle" /><span class="sidenote">Darlington,
<span>“Another Look at <span>‘Cultural
Fairness’</span>.”</span></span></span></span></p>
<p>ProPublica<span class="citation"
data-cites="angwin2016machine"><span><label for="sn-33" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-33" class="margin-toggle" /><span class="sidenote">Angwin
et al., <span>“Machine Bias,”</span> <em>Pro Publica</em>,
2016.</span></span></span> implicitly adopted equality of false positive
rates as a fairness criterion in their article on COMPAS scores.
Northpointe, the maker of the COMPAS software, emphasized the importance
of calibration by group in their rebuttal<span class="citation"
data-cites="dieterich16compas"><span><label for="sn-34" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-34" class="margin-toggle" /><span class="sidenote">Dieterich,
Mendoza, and Brennan, <span>“COMPAS Risk Scales: Demonstrating Accuracy
Equity and Predictive Parity,”</span> 2016, <a
href="https://www.documentcloud.org/documents/2998391-ProPublica-Commentary-Final-070616.html"
role="doc-biblioref">https://www.documentcloud.org/documents/2998391-ProPublica-Commentary-Final-070616.html</a>.</span></span></span>
to ProPublica’s article. Similar arguments were made quickly after the
publication of ProPublica’s article by bloggers including Abe Gong.
There has been extensive scholarship on actuarial risk assessment in
criminal justice that long predates the ProPublica debate; Berk et
al.<span class="citation"
data-cites="berk2017fairness"><span><label for="sn-35" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-35" class="margin-toggle" /><span class="sidenote">Berk
et al., <span>“<span class="nocase">Fairness in Criminal Justice Risk
Assessments: The State of the Art</span>,”</span> <em>ArXiv
e-Prints</em> 1703.09207 (2017).</span></span></span> provide a survey
with commentary.</p>
<p>Variants of the trade-off between separation and sufficiency were
shown by Chouldechova<span class="citation"
data-cites="chouldechova2016fair"><span><label for="sn-36" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-36" class="margin-toggle" /><span class="sidenote">Chouldechova,
<span>“Fair Prediction with Disparate Impact: A Study of Bias in
Recidivism Prediction Instruments,”</span> in <em>Proc. <span
class="math inline">3</span>Rd <span>FATML</span></em>,
2016.</span></span></span> and Kleinberg et al.<span class="citation"
data-cites="kleinberg2016inherent"><span><label for="sn-37" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-37" class="margin-toggle" /><span class="sidenote">Kleinberg,
Mullainathan, and Raghavan, <span>“Inherent Trade-Offs in the Fair
Determination of Risk Scores,”</span> <em>arXiv Preprint
arXiv:1609.05807</em>, 2016.</span></span></span> Each of them
considered somewhat different criteria to trade-off. Chouldechova’s
argument is very similar to the proof we presented that invokes the
relationship between positive predictive value and true positive rate.
Subsequent work<span class="citation"
data-cites="weinberger2017calibration"><span><label for="sn-38" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-38" class="margin-toggle" /><span class="sidenote">Pleiss
et al., <span>“On Fairness and Calibration,”</span> in <em>Proc. <span
class="math inline">30</span>Th <span>NIPS</span></em>,
2017.</span></span></span> considers trade-offs between relaxed and
approximate criteria. The other trade-off results presented in this
chapter are new to this book. The proof of the proposition relating
separation and independence for binary classifiers, as well as the
counterexample for ternary classifiers, is due to Shira Mitchell and
Jackie Shadlen, pointed out to us in personal communication.</p>
<p>The credit score case study is from Hardt, Price, and Srebro<span
class="citation"
data-cites="hardt2016equality"><span><label for="sn-39" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-39" class="margin-toggle" /><span class="sidenote">Hardt,
Price, and Srebro, <span>“Equality of Opportunity in Supervised
Learning.”</span></span></span></span> However, we highlight the
independence criterion in our plots, whereas the authors of the paper
highlight the euality of opportunity criterion instead.</p>
<section id="a-dictionary-of-criteria" class="level2">
<h2>A dictionary of criteria</h2>
<p>For convenience we collect some demographic fairness criteria below
that have been proposed in the past (not necessarily including the
original reference). We’ll match them to their closest relative among
the three criteria independence, separation, and sufficiency. This table
is meant as a reference only and is not exhaustive. There is no need to
memorize these different names.</p>
<table>
<caption>List of statistical non-discrimination criteria</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Criterion</th>
<th style="text-align: center;">Note</th>
<th style="text-align: center;">Reference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Independence</td>
<td style="text-align: center;">Indep.</td>
<td style="text-align: center;">Equiv.</td>
<td style="text-align: center;">Calders et al. (2009)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Group fairness</td>
<td style="text-align: center;">Indep.</td>
<td style="text-align: center;">Equiv.</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Demographic parity</td>
<td style="text-align: center;">Indep.</td>
<td style="text-align: center;">Equiv.</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Conditional statistical parity</td>
<td style="text-align: center;">Indep.</td>
<td style="text-align: center;">Relax.</td>
<td style="text-align: center;">Corbett-Davies et al. (2017)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Darlington criterion (4)</td>
<td style="text-align: center;">Indep.</td>
<td style="text-align: center;">Relax.</td>
<td style="text-align: center;">Darlington (1971)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Equal opportunity</td>
<td style="text-align: center;">Separ.</td>
<td style="text-align: center;">Relax.</td>
<td style="text-align: center;">Hardt, Price, Srebro (2016)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Equalized odds</td>
<td style="text-align: center;">Separ.</td>
<td style="text-align: center;">Equiv.</td>
<td style="text-align: center;">Hardt, Price, Srebro (2016)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conditional procedure accuracy</td>
<td style="text-align: center;">Separ.</td>
<td style="text-align: center;">Equiv.</td>
<td style="text-align: center;">Berk et al. (2017)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Avoiding disparate mistreatment</td>
<td style="text-align: center;">Separ.</td>
<td style="text-align: center;">Equiv.</td>
<td style="text-align: center;">Zafar et al. (2017)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Balance for the negative class</td>
<td style="text-align: center;">Separ.</td>
<td style="text-align: center;">Relax.</td>
<td style="text-align: center;">Kleinberg et al. (2016)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Balance for the positive class</td>
<td style="text-align: center;">Separ.</td>
<td style="text-align: center;">Relax.</td>
<td style="text-align: center;">Kleinberg et al. (2016)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Predictive equality</td>
<td style="text-align: center;">Separ.</td>
<td style="text-align: center;">Relax.</td>
<td style="text-align: center;">Corbett-Davies et al. (2017)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Equalized correlations</td>
<td style="text-align: center;">Separ.</td>
<td style="text-align: center;">Relax.</td>
<td style="text-align: center;">Woodworth (2017)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Darlington criterion (3)</td>
<td style="text-align: center;">Separ.</td>
<td style="text-align: center;">Relax.</td>
<td style="text-align: center;">Darlington (1971)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Cleary model</td>
<td style="text-align: center;">Suff.</td>
<td style="text-align: center;">Relax.</td>
<td style="text-align: center;">Cleary (1966)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conditional use accuracy</td>
<td style="text-align: center;">Suff.</td>
<td style="text-align: center;">Equiv.</td>
<td style="text-align: center;">Berk et al. (2017)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Predictive parity</td>
<td style="text-align: center;">Suff.</td>
<td style="text-align: center;">Relax.</td>
<td style="text-align: center;">Chouldechova (2016)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Calibration within groups</td>
<td style="text-align: center;">Suff.</td>
<td style="text-align: center;">Equiv.</td>
<td style="text-align: center;">Chouldechova (2016)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Darlington criterion (1), (2)</td>
<td style="text-align: center;">Suff.</td>
<td style="text-align: center;">Relax.</td>
<td style="text-align: center;">Darlington (1971)</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography">
<div id="ref-angwin2016machine" class="csl-entry"
role="doc-biblioentry">
Angwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner.
<span>“Machine Bias.”</span> <em>Pro Publica</em>, 2016.
</div>
<div id="ref-berk2017fairness" class="csl-entry" role="doc-biblioentry">
Berk, Richard, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron
Roth. <span>“<span class="nocase">Fairness in Criminal Justice Risk
Assessments: The State of the Art</span>.”</span> <em>ArXiv
e-Prints</em> 1703.09207 (2017).
</div>
<div id="ref-bonham2016will" class="csl-entry" role="doc-biblioentry">
Bonham, Vence L, Shawneequa L Callier, and Charmaine D Royal.
<span>“Will Precision Medicine Move Us Beyond Race?”</span> <em>The New
England Journal of Medicine</em> 374, no. 21 (2016): 2003.
</div>
<div id="ref-bouk2015how" class="csl-entry" role="doc-biblioentry">
Bouk, Dan. <em>How Our Days Became Numbered: Risk and the Rise of the
Statistical Individual</em>. University of Chicago Press, 2015.
</div>
<div id="ref-calders2009building" class="csl-entry"
role="doc-biblioentry">
Calders, Toon, Faisal Kamiran, and Mykola Pechenizkiy. <span>“Building
Classifiers with Independency Constraints.”</span> In <em>In
Proc. <span>IEEE</span> <span>ICDMW</span></em>, 13–18, 2009.
</div>
<div id="ref-chouldechova2016fair" class="csl-entry"
role="doc-biblioentry">
Chouldechova, Alexandra. <span>“Fair Prediction with Disparate Impact: A
Study of Bias in Recidivism Prediction Instruments.”</span> In
<em>Proc. <span class="math inline">3</span>Rd <span>FATML</span></em>,
2016.
</div>
<div id="ref-cleary1968test" class="csl-entry" role="doc-biblioentry">
Cleary, T Anne. <span>“Test Bias: Prediction of Grades of Negro and
White Students in Integrated Colleges.”</span> <em>Journal of
Educational Measurement</em> 5, no. 2 (1968): 115–24.
</div>
<div id="ref-cleary1966test" class="csl-entry" role="doc-biblioentry">
———. <span>“Test Bias: Validity of the Scholastic Aptitude Test for
Negro and White Students in Integrated Colleges.”</span> <em>ETS
Research Bulletin Series</em> 1966, no. 2 (1966): i–23.
</div>
<div id="ref-cover1999elements" class="csl-entry"
role="doc-biblioentry">
Cover, Thomas M. <em>Elements of Information Theory</em>. John Wiley
&amp; Sons, 1999.
</div>
<div id="ref-darlington1971another" class="csl-entry"
role="doc-biblioentry">
Darlington, Richard B. <span>“Another Look at <span>‘Cultural
Fairness’</span>.”</span> <em>Journal of Educational Measurement</em> 8,
no. 2 (1971): 71–82.
</div>
<div id="ref-desrosieres1998politics" class="csl-entry"
role="doc-biblioentry">
Desrosières, Alain. <em>The Politics of Large Numbers: A History of
Statistical Reasoning</em>. Harvard University Press, 1998.
</div>
<div id="ref-dieterich16compas" class="csl-entry"
role="doc-biblioentry">
Dieterich, William, Christina Mendoza, and Tim Brennan. <span>“COMPAS
Risk Scales: Demonstrating Accuracy Equity and Predictive
Parity,”</span> 2016. <a
href="https://www.documentcloud.org/documents/2998391-ProPublica-Commentary-Final-070616.html">https://www.documentcloud.org/documents/2998391-ProPublica-Commentary-Final-070616.html</a>.
</div>
<div id="ref-ding2021retiring" class="csl-entry" role="doc-biblioentry">
Ding, Frances, Moritz Hardt, John Miller, and Ludwig Schmidt.
<span>“Retiring Adult: New Datasets for Fair Machine Learning.”</span>
<em>Advances in Neural Information Processing Systems</em> 34 (2021).
</div>
<div id="ref-dwork2012fairness" class="csl-entry"
role="doc-biblioentry">
Dwork, Cynthia, Moritz Hardt, Toniann Pitassi, Omer Reingold, and
Richard Zemel. <span>“Fairness Through Awareness.”</span> In
<em>Proc. <span class="math inline">3</span>Rd <span>ITCS</span></em>,
214–26, 2012.
</div>
<div id="ref-einhorn1971methodological" class="csl-entry"
role="doc-biblioentry">
Einhorn, Hillel J, and Alan R Bass. <span>“Methodological Considerations
Relevant to Discrimination in Employment Testing.”</span>
<em>Psychological Bulletin</em> 75, no. 4 (1971): 261.
</div>
<div id="ref-feldman2015certifying" class="csl-entry"
role="doc-biblioentry">
Feldman, Michael, Sorelle A Friedler, John Moeller, Carlos Scheidegger,
and Suresh Venkatasubramanian. <span>“Certifying and Removing Disparate
Impact.”</span> In <em>Proc. <span class="math inline">21</span>St
<span>SIGKDD</span></em>. ACM, 2015.
</div>
<div id="ref-hacking2006emergence" class="csl-entry"
role="doc-biblioentry">
Hacking, Ian. <em>The Emergence of Probability: A Philosophical Study of
Early Ideas about Probability, Induction and Statistical Inference</em>.
Cambridge University Press, 2006.
</div>
<div id="ref-hacking1990taming" class="csl-entry"
role="doc-biblioentry">
Hacking, Ian, Tim Hacking, et al. <em>The Taming of Chance</em>. 17.
Cambridge University Press, 1990.
</div>
<div id="ref-hardt2016equality" class="csl-entry"
role="doc-biblioentry">
Hardt, Moritz, Eric Price, and Nati Srebro. <span>“Equality of
Opportunity in Supervised Learning.”</span> In <em>Proc. <span
class="math inline">29</span>Th <span>NIPS</span></em>, 3315–23, 2016.
</div>
<div id="ref-hardtrecht2022patterns" class="csl-entry"
role="doc-biblioentry">
Hardt, Moritz, and Benjamin Recht. <em>Patterns, Predictions, and
Actions: Foundations of Machine Learning</em>. Princeton University
Press, 2022.
</div>
<div id="ref-hutchinson201950" class="csl-entry" role="doc-biblioentry">
Hutchinson, Ben, and Margaret Mitchell. <span>“50 Years of Test (Un)
Fairness: Lessons for Machine Learning.”</span> In <em>Proc. <span
class="math inline">2</span>Nd Conference on <span
class="nocase">Fairness, Accountability, and Transparency
(FAccT)</span></em>, 49–58, 2019.
</div>
<div id="ref-kamiran2009classifying" class="csl-entry"
role="doc-biblioentry">
Kamiran, Faisal, and Toon Calders. <span>“Classifying Without
Discriminating.”</span> In <em>Proc. <span
class="math inline">2</span>Nd International Conference on Computer,
Control and Communication</em>, 2009.
</div>
<div id="ref-kleinberg2016inherent" class="csl-entry"
role="doc-biblioentry">
Kleinberg, Jon, Sendhil Mullainathan, and Manish Raghavan.
<span>“Inherent Trade-Offs in the Fair Determination of Risk
Scores.”</span> <em>arXiv Preprint arXiv:1609.05807</em>, 2016.
</div>
<div id="ref-lewis1978comparison" class="csl-entry"
role="doc-biblioentry">
Lewis, Mary A. <span>“A Comparison of Three Models for Determining Test
Fairness.”</span> Federal Aviation Administration Washington DC Office
of Aviation Medicine, 1978.
</div>
<div id="ref-liu2019implicit" class="csl-entry" role="doc-biblioentry">
Liu, Lydia T, Max Simchowitz, and Moritz Hardt. <span>“The Implicit
Fairness Criterion of Unconstrained Learning.”</span> In
<em>International Conference on Machine Learning</em>, 4051–60. PMLR,
2019.
</div>
<div id="ref-platt1999probabilistic" class="csl-entry"
role="doc-biblioentry">
Platt, John et al. <span>“Probabilistic Outputs for Support Vector
Machines and Comparisons to Regularized Likelihood Methods.”</span>
<em>Advances in Large Margin Classifiers</em> 10, no. 3 (1999): 61–74.
</div>
<div id="ref-weinberger2017calibration" class="csl-entry"
role="doc-biblioentry">
Pleiss, Geoff, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q
Weinberger. <span>“On Fairness and Calibration.”</span> In
<em>Proc. <span class="math inline">30</span>Th <span>NIPS</span></em>,
2017.
</div>
<div id="ref-porter2020rise" class="csl-entry" role="doc-biblioentry">
Porter, Theodore M. <em>The Rise of Statistical Thinking,
1820–1900</em>. Princeton University Press, 2020.
</div>
<div id="ref-federalreserve" class="csl-entry" role="doc-biblioentry">
The Federal Reserve Board. <span>“Report to the Congress on Credit
Scoring and Its Effects on the Availability and Affordability of
Credit.”</span> <a
href="https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/"
class="uri">https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/</a>,
2007.
</div>
<div id="ref-thorndike1971concepts" class="csl-entry"
role="doc-biblioentry">
Thorndike, Robert L. <span>“Concepts of Culture-Fairness.”</span>
<em>Journal of Educational Measurement</em> 8, no. 2 (1971): 63–70.
</div>
<div id="ref-wasserman2010all" class="csl-entry" role="doc-biblioentry">
Wasserman, Larry. <em>All of Statistics: A Concise Course in Statistical
Inference</em>. Springer, 2010.
</div>
<div id="ref-woodworth2017learning" class="csl-entry"
role="doc-biblioentry">
Woodworth, Blake E., Suriya Gunasekar, Mesrob I. Ohannessian, and Nathan
Srebro. <span>“Learning Non-Discriminatory Predictors.”</span> In
<em>Proc. <span class="math inline">30</span>Th <span>COLT</span></em>,
1920–53, 2017.
</div>
<div id="ref-zafar2017fairnessbeyond" class="csl-entry"
role="doc-biblioentry">
Zafar, Muhammad Bilal, Isabel Valera, Manuel Gómez Rodriguez, and
Krishna P. Gummadi. <span>“Fairness Beyond Disparate Treatment &amp;
Disparate Impact: Learning Classification Without Disparate
Mistreatment.”</span> In <em>Proc. <span class="math inline">26</span>Th
<span>WWW</span></em>, 2017.
</div>
<div id="ref-zemel2013learning" class="csl-entry"
role="doc-biblioentry">
Zemel, Richard S., Yu Wu, Kevin Swersky, Toniann Pitassi, and Cynthia
Dwork. <span>“Learning Fair Representations.”</span> In <em>Proc. <span
class="math inline">30</span>Th <span>ICML</span></em>, 2013.
</div>
</div>
</section>

<div id="lastupdate">
Last updated: Tue Jun 28 10:06:01 CEST 2022
</div>
</article>


<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>

</body>
</html>
