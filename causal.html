<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Causality</title>
  <style>q { quotes: "“" "”" "‘" "’"; }</style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
   var mathElements = document.getElementsByClassName("math");
   var macros = [];
   for (var i = 0; i < mathElements.length; i++) {
    var texText = mathElements[i].firstChild;
    if (mathElements[i].tagName == "SPAN") {
     katex.render(texText.data, mathElements[i], {
      displayMode: mathElements[i].classList.contains('display'),
      throwOnError: false,
      macros: macros,
      fleqn: false
     });
  }}});
  </script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" />
  <link rel="stylesheet" href="style.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article>
<header>
<div id="chapter">5</div>
<h1 class="title">Causality</h1>
</header>



<div id="collapsiblemenu">
  <button class="collapsible">
    <div class="shortthickbar"></div>
    <div class="shortthickbar"></div>
    <div class="shortthickbar"></div>
  </button>
  <div class="content">
    <ul>
    <li><a href="index.html">Home</a></li>
    </ul>
    <ul>
    <li><a href="#the-limitations-of-observation">The limitations of observation</a></li>
    <li><a href="#causal-models">Causal models</a>
    <ul>
    <li><a href="#a-first-example">A first example</a></li>
    <li><a href="#structural-causal-models-more-formally">Structural causal models, more formally</a></li>
    </ul></li>
    <li><a href="#causal-graphs">Causal graphs</a>
    <ul>
    <li><a href="#forks">Forks</a></li>
    <li><a href="#mediators">Mediators</a></li>
    <li><a href="#colliders">Colliders</a></li>
    </ul></li>
    <li><a href="#interventions-and-causal-effects">Interventions and causal effects</a>
    <ul>
    <li><a href="#substitutions-and-the-do-operator">Substitutions and the do-operator</a></li>
    <li><a href="#causal-effects">Causal effects</a></li>
    </ul></li>
    <li><a href="#confounding">Confounding</a>
    <ul>
    <li><a href="#the-backdoor-criterion">The backdoor criterion</a></li>
    <li><a href="#unobserved-confounding">Unobserved confounding</a></li>
    <li><a href="#randomization">Randomization</a></li>
    </ul></li>
    <li><a href="#graphical-discrimination-analysis">Graphical discrimination analysis</a>
    <ul>
    <li><a href="#direct-effects">Direct effects</a></li>
    <li><a href="#indirect-paths">Indirect paths</a></li>
    <li><a href="#path-inspection">Path inspection</a></li>
    <li><a href="#structural-discrimination">Structural discrimination</a></li>
    </ul></li>
    <li><a href="#counterfactuals">Counterfactuals</a>
    <ul>
    <li><a href="#a-simple-counterfactual">A simple counterfactual</a></li>
    <li><a href="#the-general-recipe">The general recipe</a></li>
    <li><a href="#potential-outcomes">Potential outcomes</a></li>
    </ul></li>
    <li><a href="#counterfactual-discrimination-analysis">Counterfactual discrimination analysis</a>
    <ul>
    <li><a href="#quantitative-path-analysis">Quantitative path analysis</a></li>
    <li><a href="#counterfactual-discrimination-criteria">Counterfactual discrimination criteria</a></li>
    <li><a href="#counterfactuals-in-the-law">Counterfactuals in the law</a></li>
    <li><a href="#harvard-college-admissions">Harvard college admissions</a></li>
    </ul></li>
    <li><a href="#validity-of-causal-modeling">Validity of causal modeling</a>
    <ul>
    <li><a href="#social-construction-of-categories">Social construction of categories</a></li>
    <li><a href="#ontological-instability">Ontological instability</a></li>
    <li><a href="#certificates-of-ontological-stability">Certificates of ontological stability</a></li>
    </ul></li>
    <li><a href="#chapter-notes">Chapter notes</a></li>
    <li><a href="#bibliography">References</a></li>
    </ul>
  </div>
</div>


<p>Our starting point is the difference between an observation and an action. What we see in passive observation is how individuals follow their routine behavior, habits, and natural inclination. Passive observation reflects the state of the world projected to a set of features we chose to highlight. Data that we collect from passive observation show a snapshot of our world as it is.</p>
<p>There are many questions we can answer from passive observation alone: Do 16 year-old drivers have a higher incidence rate of traffic accidents than 18 year-old drivers? Formally, the answer corresponds to a difference of conditional probabilities assuming we model the population as a distribution as we did in the last chapter. We can calculate the conditional probability of a traffic accident given that the driver’s age is 16 years and subtract from it the conditional probability of a traffic accident given the age is 18 years. Both conditional probabilities can be estimated from a large enough sample drawn from the distribution, assuming that there are both 16 year old and 18 year old drivers. The answer to the question we asked is solidly in the realm of observational statistics.</p>
<p>But important questions often are not observational in nature. Would traffic fatalities decrease if we raised the legal driving age by two years? Although the question seems similar on the surface, we quickly realize that it asks for a fundamentally different insight. Rather than asking for the frequency of an event in our manifested world, this question asks for the effect of a hypothetical action.</p>
<p>As a result, the answer is not so simple. Even if older drivers have a lower incidence rate of traffic accidents, this might simply be a consequence of additional driving experience. There is no obvious reason why an 18 year old with two months on the road would be any less likely to be involved in an accident than, say, a 16 year-old with the same experience. We can try to address this problem by holding the number of months of driving experience fixed, while comparing individuals of different ages. But we quickly run into subtleties. What if 18 year-olds with two months of driving experience correspond to individuals who are exceptionally cautious and hence—by their natural inclination—not only drive less, but also more cautiously? What if such individuals predominantly live in regions where traffic conditions differ significantly from those in areas where people feel a greater need to drive at a younger age?</p>
<p>We can think of numerous other strategies to answer the original question of whether raising the legal driving age reduces traffic accidents. We could compare countries with different legal driving ages, say, the United States and Germany. But again, these countries differ in many other possibly relevant ways, such as, the legal drinking age.</p>
<p>At the outset, causal reasoning is a conceptual and technical framework for addressing questions about the effect of hypothetical actions or <em>interventions</em>. Once we understand what the effect of an action is, we can turn the question around and ask what action plausibly <em>caused</em> an event. This gives us a formal language to talk about cause and effect.</p>
<p>Not every question about cause is equally easy to address. Some questions are overly broad, such as, “What is the cause of success?” Other questions are too specific: “What caused your interest in 19th century German philosophy?” Neither question might have a clear answer. Causal inference gives us a formal language to ask these questions, in principle, but it does not make it easy to choose the right questions. Nor does it trivialize the task of finding and interpreting the answer to a question. Especially in the context of fairness, the difficulty is often in deciding what the question is that causal inference is the answer to.</p>
<p>In this chapter, we will develop sufficient technical understanding of causality to support at least three different purposes. The first is to conceptualize and address some limitations of the observational techniques we saw in Chapter 3. The second is to provide tools that help in the design of interventions that reliably achieve a desired effect. The third is to engage with the important normative debate about when and to which extent reasoning about discrimination and fairness requires causal understanding.</p>
<section id="the-limitations-of-observation" class="level1">
<h1>The limitations of observation</h1>
<p>Before we develop any new formalism, it is important to understand why we need it in the first place. To see why we turn to the venerable example of graduate admissions at the University of California, Berkeley in 1973.<span class="citation" data-cites="bickel1975sex"><span><label for="sn-0" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-0" class="margin-toggle" /><span class="sidenote">Bickel et al. et al., <span>“Sex Bias in Graduate Admissions: Data from Berkeley,”</span> <em>Science</em> 187, no. 4175 (1975): 398–404.</span></span></span> Historical data show that 12763 applicants were considered for admission to one of 101 departments and inter-departmental majors. Of the 4321 women who applied roughly 35 percent were admitted, while 44 percent of the 8442 men who applied were admitted. Standard statistical significance tests suggest that the observed difference would be highly unlikely to be the outcome of sample fluctuation if there were no difference in underlying acceptance rates.</p>
<p>A similar pattern exists if we look at the aggregate admission decisions of the six largest departments. The acceptance rate across all six departments for men is about 44%, while it is only roughly 30% for women, again, a significant difference. Recognizing that departments have autonomy over who to admit, we can look at the gender bias of each department.</p>
<table>
<caption>UC Berkeley admissions data from 1973.</caption>
<thead>
<tr class="header">
<th></th>
<th>Men</th>
<th></th>
<th>Women</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Department</td>
<td>Applied</td>
<td>Admitted (%)</td>
<td>Applied</td>
<td>Admitted (%)</td>
</tr>
<tr class="even">
<td>A</td>
<td>825</td>
<td>62</td>
<td>108</td>
<td><strong>82</strong></td>
</tr>
<tr class="odd">
<td>B</td>
<td>520</td>
<td>60</td>
<td>25</td>
<td><strong>68</strong></td>
</tr>
<tr class="even">
<td>C</td>
<td>325</td>
<td><strong>37</strong></td>
<td>593</td>
<td>34</td>
</tr>
<tr class="odd">
<td>D</td>
<td>417</td>
<td>33</td>
<td>375</td>
<td><strong>35</strong></td>
</tr>
<tr class="even">
<td>E</td>
<td>191</td>
<td><strong>28</strong></td>
<td>393</td>
<td>24</td>
</tr>
<tr class="odd">
<td>F</td>
<td>373</td>
<td>6</td>
<td>341</td>
<td><strong>7</strong></td>
</tr>
</tbody>
</table>
<p>What we can see from the table is that four of the six largest departments show a higher acceptance ratio among women, while two show a higher acceptance rate for men. However, these two departments cannot account for the large difference in acceptance rates that we observed in aggregate. So, it appears that the higher acceptance rate for men that we observed in aggregate seems to have reversed at the department level.</p>
<p>Such reversals are sometimes called <em>Simpson’s paradox</em>, even though mathematically they are no surprise. It’s a fact of conditional probability that there can be an event&nbsp;<span class="math inline">Y</span> (here, acceptance), an attribute&nbsp;<span class="math inline">A</span> (here, female gender taken to be a binary variable) and a random variable&nbsp;<span class="math inline">Z</span> (here, department choice) such that:</p>
<ol type="1">
<li><span class="math inline">\mathbb{P}\{ Y \mid A \} &lt; \mathbb{P}\{ Y \mid \neg A \}</span></li>
<li><span class="math inline">\mathbb{P}\{ Y \mid A, Z = z \} &gt; \mathbb{P}\{ Y \mid \neg A, Z = z\}</span> for all values <span class="math inline">z</span> that the random variable <span class="math inline">Z</span> assumes.</li>
</ol>
<p>Simpson’s paradox nonetheless causes discomfort to some, because intuition suggests that a trend which holds for all subpopulations should also hold at the population level.</p>
<p>The reason why Simpson’s paradox is relevant to our discussion is that it’s a consequence of how we tend to misinterpret what information conditional probabilities encode. Recall that a statement of conditional probability corresponds to passive observation. What we see here is a snapshot of the normal behavior of women and men applying to graduate school at UC Berkeley in 1973.</p>
<p>What is evident from the data is that gender influences department choice. Women and men appear to have different preferences for different fields of study. Moreover, different departments have different admission criteria. Some have lower acceptance rates, some higher. Therefore, one explanation for the data we see is that women <em>chose</em> to apply to more competitive departments, hence getting rejected at a higher rate than men.</p>
<p>Indeed, this is the conclusion the original study drew:</p>
<blockquote>
<p><em>The bias in the aggregated data stems not from any pattern of discrimination on the part of admissions committees, which seems quite fair on the whole, but apparently from prior screening at earlier levels of the educational system. Women are shunted by their socialization and education toward fields of graduate study that are generally more crowded, less productive of completed degrees, and less well funded, and that frequently offer poorer professional employment prospects.<span class="citation" data-cites="bickel1975sex"><span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle" /><span class="sidenote">Bickel et al. et al.</span></span></span></em></p>
</blockquote>
<p>In other words, the article concluded that the source of gender bias in admissions was a <em>pipeline problem</em>: Without wrongdoing by the admissions committee, women were “shunted by their socialization” that happened at an earlier stage in their lives.</p>
<p>It is difficult to debate this conclusion on the basis of the available data alone. The question of discrimination, however, is far from resolved. We can ask why women applied to more competitive departments in the first place. There are several possible reasons. Perhaps less competitive departments, such as engineering schools, were unwelcoming of women at the time. This may have been a general pattern at the time or specific to the university. Perhaps some departments had a track record of poor treatment of women that was known to the applicants. Perhaps the department advertised the program in a manner that discouraged women from applying.</p>
<p>The data we have also shows no measurement of <em>qualification</em> of an applicant. It’s possible that due to self-selection women applying to engineering schools in 1973 were over-qualified relative to their peers. In this case, an equal acceptance rate between men and women might actually be a sign of discrimination.</p>
<p>There is no way of knowing what was the case from the data we have. There are multiple possible scenarios with different interpretations and consequences that we cannot distinguish from the data at hand. At this point, we have two choices. One is to design a new study and collect more data in a manner that might lead to a more conclusive outcome. The other is to argue over which scenario is more likely based on our beliefs and plausible assumptions about the world. Causal inference is helpful in either case. On the one hand, it can be used as a guide in the design of new studies. It can help us choose which variables to include, which to exclude, and which to hold constant. On the other hand, causal models can serve as a mechanism to incorporate scientific domain knowledge and exchange plausible assumptions for plausible conclusions.</p>
</section>
<section id="causal-models" class="level1">
<h1>Causal models</h1>
<p>We will develop just enough formal concepts to engage with the technical and normative debate around causality and discrimination. The topic is much deeper than what we can explore in this chapter.</p>
<p>We choose <em>structural causal models</em> as the basis of our formal discussion as they have the advantage of giving a sound foundation for various causal notions we will encounter. The easiest way to conceptualize a structural causal model is as a program for generating a distribution from independent noise variables through a sequence of formal instructions. Let’s unpack this statement. Imagine instead of samples from a distribution, somebody gave you a step-by-step computer program to generate samples on your own starting from a random seed. The process is not unlike how you would write code. You start from a simple random seed and build up increasingly more complex constructs. That is basically what a structural causal model is, except that each assignment uses the language of mathematics rather than any concrete programming syntax.</p>
<section id="a-first-example" class="level2">
<h2>A first example</h2>
<p>Let’s start with a toy example not intended to capture the real world. Imagine a hypothetical population in which an individual exercises regularly with probability&nbsp;<span class="math inline">1/2</span>. With probability&nbsp;<span class="math inline">1/3</span>, the individual has a latent disposition to develop overweight that manifests in the absence of regular exercise. Similarly, in the absence of exercise, heart disease occurs with probability&nbsp;<span class="math inline">1/3</span>. Denote by&nbsp;<span class="math inline">X</span> the indicator variable of regular exercise, by&nbsp;<span class="math inline">W</span> that of excessive weight, and by&nbsp;<span class="math inline">H</span> the indicator of heart disease. Below is a structural causal model to generate samples from this hypothetical population. To ease the description, we let&nbsp;<span class="math inline">\mathrm{B}(p)</span> denote a Bernoulli random variable with bias&nbsp;<span class="math inline">p</span>, i.e., a biased coin toss that assumes value&nbsp;<span class="math inline">1</span> with probability&nbsp;<span class="math inline">p</span> and value&nbsp;<span class="math inline">0</span> with probability&nbsp;<span class="math inline">1-p.</span></p>
<ol type="1">
<li>Sample independent Bernoulli random variables <span class="math inline">U_1\sim \mathrm{B}(1/2), U_2\sim \mathrm{B}(1/3), U_3\sim\mathrm{B}(1/3).</span></li>
<li><span class="math inline">X := U_1</span></li>
<li><span class="math inline">W := \,</span> if <span class="math inline">X=1</span> then <span class="math inline">0</span> else <span class="math inline">U_2</span></li>
<li><span class="math inline">H := \,</span> if <span class="math inline">X=1</span> then <span class="math inline">0</span> else <span class="math inline">U_3</span></li>
</ol>
<p>Contrast this generative description of the population with a random sample drawn from the population. From the program description, we can immediately see that in our hypothetical population <em>exercise</em> averts both <em>overweight</em> and <em>heart disease</em>, but in the absence of exercise the two are independent. At the outset, our program generates a joint distribution over the random variables&nbsp;<span class="math inline">(X, W, H).</span> We can calculate probabilities under this distribution. For example, the probability of heart disease under the distribution specified by our model is&nbsp;<span class="math inline">1/2 \cdot 1/3 = 1/6.</span> We can also calculate the conditional probability of heart diseases given overweight. From the event&nbsp;<span class="math inline">W=1</span> we can infer that the individual does not exercise so that the probability of heart disease given overweight increases to&nbsp;<span class="math inline">1/3</span> compared with the baseline of&nbsp;<span class="math inline">1/6</span>.</p>
<p>Does this mean that overweight causes heart disease in our model? The answer is <em>no</em> as is intuitive given the program to generate the distribution. But let’s see how we would go about arguing this point formally. Having a program to generate a distribution is substantially more powerful than just having sampling access. One reason is that we can manipulate the program in whichever way we want, assuming we still end up with a valid program. We could, for example, set&nbsp;<span class="math inline">W := 1,</span> resulting in a new distribution. The resulting program looks like this:</p>
<ol start="2" type="1">
<li><span class="math inline">X := U_1</span></li>
<li><span class="math inline">W := 1</span></li>
<li><span class="math inline">H := \,</span> if <span class="math inline">X=1</span> then <span class="math inline">0</span> else <span class="math inline">U_3</span></li>
</ol>
<p>This new program specifies a new distribution. We can again calculate the probability of heart disease under this new distribution. We still get&nbsp;<span class="math inline">1/6.</span> This simple calculation reveals a significant insight. The substitution&nbsp;<span class="math inline">W:=1</span> does not correspond to a conditioning on&nbsp;<span class="math inline">W=1.</span> One is an action, albeit inconsequential in this case. The other is an observation from which we can draw inferences. If we observe that an individual is overweight, we can infer that they have a higher risk of heart disease (in our toy example). However, this does not mean that lowering body weight would avoid heart disease. It wouldn’t in our example. The active substitution&nbsp;<span class="math inline">W:=1</span> in contrast creates a new hypothetical population in which all individuals are overweight with all that it entails in our model.</p>
<p>Let us belabor this point a bit more by considering another hypothetical population, specified by the equations:</p>
<ol start="2" type="1">
<li><span class="math inline">W := U_2</span></li>
<li><span class="math inline">X := \,</span> if <span class="math inline">W=0</span> then <span class="math inline">0</span> else <span class="math inline">U_1</span></li>
<li><span class="math inline">H := \,</span> if <span class="math inline">X=1</span> then <span class="math inline">0</span> else <span class="math inline">U_3</span></li>
</ol>
<p>In this population exercise habits are driven by body weight. Overweight individuals choose to exercise with some probability, but that’s the only reason anyone would exercise. Heart disease develops in the absence of exercise. The substitution&nbsp;<span class="math inline">W:=1</span> in this model leads to an increased probability of exercise, hence lowering the probability of heart disease. In this case, the conditioning on&nbsp;<span class="math inline">W=1</span> has the same affect. Both lead to a probability of&nbsp;<span class="math inline">1/6.</span></p>
<p>What we see is that fixing a variable by substitution may or may not correspond to a conditional probability. This is a formal rendering of our earlier point that observation isn’t action. A substitution corresponds to an action we perform. By substituting a value we break the natural course of action our model captures. This is the reason why the substitution operation is sometimes called the <em>do-operator</em>, written as&nbsp;<span class="math inline">\mathrm{do}(W:=1)</span>.</p>
<p>Structural causal models give us a formal calculus to reason about the effect of hypothetical actions. We will see how this creates a formal basis for all the different causal notions that we will encounter in this chapter.</p>
</section>
<section id="structural-causal-models-more-formally" class="level2">
<h2>Structural causal models, more formally</h2>
<p>Formally, a structural causal model is a sequence of assignments for generating a joint distribution starting from independent noise variables. By executing the sequence of assignments we incrementally build a set of jointly distributed random variables. A structural causal model therefore not only provides a joint distribution, but also a description of how the joint distribution can be generated from elementary noise variables. The formal definition is a bit cumbersome compared with the intuitive notion.</p>
<div class="numenv Definition">
<span class="numenv Definition title">Definition 1.</span>
<p>A <em>structural causal model</em> <span class="math inline">M</span> is given by a set of variables&nbsp;<span class="math inline">X_1,..., X_d</span> and corresponding assignments of the form <span class="math display">
X_i := f_i(P_i, U_i),\quad\quad i=1,..., d\,.
</span> Here,&nbsp;<span class="math inline">P_i\subseteq\{X_1,...,X_d\}</span> is a subset of the variables that we call the <em>parents</em> of&nbsp;<span class="math inline">X_i</span>. The random variables&nbsp;<span class="math inline">U_1,..., U_d</span> are called <em>noise variables</em>, which we require to be jointly independent. The <em>causal graph</em> corresponding to the structural causal model is the directed graph that has one node for each variable&nbsp;<span class="math inline">X_i</span> with incoming edges from all the parents&nbsp;<span class="math inline">P_i.</span></p>
</div>
<p>Let’s walk through the formal concepts introduced in this definition in a bit more detail. The noise variables that appear in the definition model <em>exogenous factors</em> that influence the system. Consider, for example, how the weather influences the delay on a traffic route you choose. Due to the difficulty of modeling the influence of weather more precisely, we could take the weather induced delay to be an exogenous factor that enters the model as a noise variable. The choice of exogenous variables and their distribution can have important consequences for what conclusions we draw from a model.</p>
<p>The parent nodes&nbsp;<span class="math inline">P_i</span> of node&nbsp;<span class="math inline">i</span> in a structural causal model are often called the <em>direct causes</em> of&nbsp;<span class="math inline">X_i.</span> Similarly, we call&nbsp;<span class="math inline">X_i</span> the direct effect of its direct causes&nbsp;<span class="math inline">P_i.</span> Recall our hypothetical population in which weight gain was determined by lack of exercise via the assignment&nbsp;<span class="math inline">W:=\min\{U_1,1-X\}.</span> Here we would say that exercise (or lack thereof) is a direct cause of weight gain.</p>
<p>Structural causal model are a collection of formal <em>assumptions</em> about how certain variables interact. Each assignment specifies a <em>response function</em>. We can think of nodes as receiving messages from their parents and acting according to these messages as well as the influence of an exogenous noise variable.</p>
<p>To which extent a structural causal model conforms to reality is a separate and difficult question that we will return to in more detail later. For now, think of a structural causal model as formalizing and exposing a set of assumptions about a data generating process. As such different models can expose different hypothetical scenarios and serve as a basis for discussion. When we make statements about cause and effect in reference to a model, we don’t mean to suggest that these relationship necessarily hold in the real world. Whether they do depends on the scope, purpose, and validity of our model, which may be difficult to substantiate.</p>
<p>It’s not hard to show that a structural causal model defines a unique joint distribution over the variables&nbsp;<span class="math inline">(X_1,..., X_d)</span> such that&nbsp;<span class="math inline">X_i=f_i(P_i,U_i).</span> It’s convenient to introduce a notion for probabilities under this distribution. When&nbsp;<span class="math inline">M</span> denotes a structural causal model, we will write the probability of an event&nbsp;<span class="math inline">E</span> under the entailed joint distribution as&nbsp;<span class="math inline">\mathbb{P}_M\{E\}.</span> To gain familiarity with the notation, let&nbsp;<span class="math inline">M</span> denote the structural causal model for the hypothetical population in which both weight gain and heart disease are directly caused by an absence of exercise. We calculated earlier that the probability of heart disease in this model is&nbsp;<span class="math inline">\mathbb{P}_M\{H\}=1/6.</span></p>
<p>In what follows we will derive from this single definition of a structural causal model all the different notions and terminology that we’ll need in this chapter. Throughout, we restrict our attention to acyclic assignments. Many real-world systems are naturally described as stateful dynamical system with closed feedback loops. There are some ways of dealing with such closed loop systems. For example, often cycles can be broken up by introducing time dependent variables, such as, investments at time&nbsp;<span class="math inline">0</span> grow the economy at time&nbsp;<span class="math inline">1</span> which in turn grows investments at time&nbsp;<span class="math inline">2</span>, continuing so forth until some chosen time horizon&nbsp;<span class="math inline">t</span>. This processing is called <em>unrolling</em> a dynamical system.</p>
</section>
</section>
<section id="causal-graphs" class="level1">
<h1>Causal graphs</h1>
<p>We saw how structural causal models naturally give rise to <em>causal graphs</em> that represent the assignment structure of the model graphically. We can go the other way as well by simply looking at directed graphs as placeholders for an unspecified structural causal model which has the assignment structure given by the graph. Causal graphs are often called <em>causal diagrams</em>. We’ll use these terms interchangeably.</p>
<p>The causal graphs for the two hypothetical populations from our heart disease example each have two edges and the same three nodes. They agree on the link between exercise and heart disease, but they differ in the direction of the link between exercise and weight gain.</p>
<figure>
<img src="assets/causal-ex.svg" style="width:70.0%" alt="Causal diagrams for the heart disease examples." /><figcaption aria-hidden="true">Causal diagrams for the heart disease examples.</figcaption>
</figure>
<p>Causal graphs are convenient when the exact assignments in a structural causal models are of secondary importance, but what matters are the paths present and absent in the graph. Graphs also let us import the established language of graph theory to discuss causal notions. We can say, for example, that an <em>indirect cause</em> of a node is any ancestor of the node in a given causal graph. In particular, causal graphs allow us to distinguish cause and effect based on whether a node is an ancestor or descendant of another node.</p>
<p>Let’s take a first glimpse at a few important graph structures.</p>
<section id="forks" class="level2">
<h2>Forks</h2>
<p>A <em>fork</em> is a node&nbsp;<span class="math inline">Z</span> in a graph that has outgoing edges to two other variables&nbsp;<span class="math inline">X</span> and&nbsp;<span class="math inline">Y</span>. Put differently, the node&nbsp;<span class="math inline">Z</span> is a common cause of&nbsp;<span class="math inline">X</span> and&nbsp;<span class="math inline">Y</span>. We already saw an example of a fork in our weight and exercise example:&nbsp;<span class="math inline">W\leftarrow X \rightarrow H</span>. Here, exercise&nbsp;<span class="math inline">X</span> influences both weight and heart disease. We also learned from the example that&nbsp;<span class="math inline">Z</span> has a <em>confounding</em> effect: Ignoring exercise&nbsp;<span class="math inline">X</span>, we saw that&nbsp;<span class="math inline">W</span> and&nbsp;<span class="math inline">H</span> appear to be positively correlated. However, the correlation is a mere result of confounding. Once we hold exercise levels constant (via the do-operation), weight has no effect on heart disease in our example.</p>
<figure>
<img src="assets/causal-conf.svg" style="width:30.0%" alt="Example of a fork (confounder)." /><figcaption aria-hidden="true">Example of a fork (confounder).</figcaption>
</figure>
<p>Confounding leads to a disagreement between the calculus of conditional probabilities (observation) and do-interventions (actions). Real-world examples of confounding are a common threat to the validity of conclusions drawn from data. For example, in a well known medical study a suspected beneficial effect of <em>hormone replacement therapy</em> in reducing cardiovascular disease disappeared after identifying <em>socioeconomic status</em> as a confounding variable.<span class="citation" data-cites="humphrey2002postmenopausal"><span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle" /><span class="sidenote">Humphrey, Chan, and Sox, <span>“<span class="nocase">Postmenopausal Hormone Replacement Therapy and the Primary Prevention of Cardiovascular Disease</span>,”</span> <em>Annals of Internal Medicine</em> 137, no. 4 (August 2002): 273–84.</span></span></span></p>
</section>
<section id="mediators" class="level2">
<h2>Mediators</h2>
<p>The case of a fork is quite different from the situation where&nbsp;<span class="math inline">Z</span> lies on a directed path from&nbsp;<span class="math inline">X</span> to&nbsp;<span class="math inline">Y</span>. In this case, the path&nbsp;<span class="math inline">X\to Z\to Y</span> contributes to the total effect of&nbsp;<span class="math inline">X</span> on&nbsp;<span class="math inline">Y</span>. It’s a causal path and thus one of the ways in which&nbsp;<span class="math inline">X</span> causally influences&nbsp;<span class="math inline">Y</span>. That’s why&nbsp;<span class="math inline">Z</span> is not a confounder. We call&nbsp;<span class="math inline">Z</span> a <em>mediator</em> instead.</p>
<figure>
<img src="assets/causal-chain.svg" style="width:30.0%" alt="Example of a chain (mediator)." /><figcaption aria-hidden="true">Example of a chain (mediator).</figcaption>
</figure>
<p>We saw a plausible example of a mediator in our UC Berkeley admissions example. In one plausible causal graph, department choice mediates the influences of gender on the admissions decision. The notion of a mediator is particularly relevant to the topic of discrimination analysis, since mediators can be interpreted as the mechanism behind a causal link.</p>
</section>
<section id="colliders" class="level2">
<h2>Colliders</h2>
<p>Finally, let’s consider another common situation: the case of a <em>collider</em>. Colliders aren’t confounders. In fact, in the above graph,&nbsp;<span class="math inline">X</span> and&nbsp;<span class="math inline">Y</span> are unconfounded, meaning that we can replace do-statements by conditional probabilities. However, something interesting happens when we condition on a collider. The conditioning step can create correlation between&nbsp;<span class="math inline">X</span> and&nbsp;<span class="math inline">Y</span>, a phenomenon called <em>explaining away</em>. A good example of the explaining away effect, or <em>collider bias</em>, is due to Berkson. Two independent diseases can become negatively correlated when analyzing hospitalized patients. The reason is that when either disease (<span class="math inline">X</span> or&nbsp;<span class="math inline">Y</span>) is sufficient for admission to the hospital (indicated by variable&nbsp;<span class="math inline">Z</span>), observing that a patient has one disease makes the other statistically less likely.<span class="citation" data-cites="berkson2014limitations"><span><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle" /><span class="sidenote">Berkson, <span>“Limitations of the Application of Fourfold Table Analysis to Hospital Data,”</span> <em>International Journal of Epidemiology</em> 43, no. 2 (2014): 511–15.</span></span></span></p>
<figure>
<img src="assets/causal-collider.svg" style="width:30.0%" alt="Example of a collider." /><figcaption aria-hidden="true">Example of a collider.</figcaption>
</figure>
<p>Berkson’s law is a cautionary tale for statistical analysis when we’re studying a cohort that has been subjected to a selection rule. For example, there’s an ongoing debate about the effectiveness of GRE scores in higher education. Some studies<span class="citation" data-cites="moneta-koehler2017the hall2017predictors"><span><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle" /><span class="sidenote">Moneta-Koehler, <span>“The Limitations of the GRE in Predicting Success in Biomedical Graduate School,”</span> <em>PLOS ONE</em> 12, no. 1 (January 2017): 1–17; Hall, <span>“Predictors of Student Productivity in Biomedical Graduate School Applications,”</span> <em>PLOS ONE</em> 12, no. 1 (January 2017): 1–14.</span></span></span> argue that GRE scores are not predictive of various success outcomes in a graduate student population. However, care must be taken when studying the effectiveness of educational tests, such as the GRE, by examining a sample of admitted students. After all, students were in part admitted on the basis of the test score. It’s the selection rule that introduces the potential for collider bias.</p>
</section>
</section>
<section id="interventions-and-causal-effects" class="level1">
<h1>Interventions and causal effects</h1>
<p>Structural causal models give us a way to formalize the effect of hypothetical actions or interventions on the population within the assumptions of our model. As we saw earlier all we needed was the ability to do substitutions.</p>
<section id="substitutions-and-the-do-operator" class="level2">
<h2>Substitutions and the do-operator</h2>
<p>Given a structural causal model&nbsp;<span class="math inline">M</span> we can take any assignment of the form</p>
<p><span class="math display">
X := f(P, U)
</span></p>
<p>and replace it by another assignment. The most common substitution is to assign&nbsp;<span class="math inline">X</span> a constant value&nbsp;<span class="math inline">x</span>:</p>
<p><span class="math display">
X := x
</span></p>
<p>We will denote the resulting model by&nbsp;<span class="math inline">M&#39;=M[X:=x]</span> to indicate the surgery we performed on the original model&nbsp;<span class="math inline">M</span>. Under this assignment we hold&nbsp;<span class="math inline">X</span> constant by removing the influence of its parent nodes and thereby any other variables in the model.</p>
<figure>
<img src="assets/causal-sub.svg" style="width:60.0%" alt="Graph before and after substitution." /><figcaption aria-hidden="true">Graph before and after substitution.</figcaption>
</figure>
<p>Graphically, the operation corresponds to eliminating all incoming edges to the node&nbsp;<span class="math inline">X</span>. The children of&nbsp;<span class="math inline">X</span> in the graph now receive a fixed message&nbsp;<span class="math inline">x</span> from&nbsp;<span class="math inline">X</span> when they query the node’s value. The assignment operator is also called the <em>do-operator</em> to emphasize that it corresponds to performing an action or intervention. We already have notation to compute probabilities after applying the do-operator, namely,&nbsp;<span class="math inline">\mathbb{P}_{M[X:=x]}(E).</span> Another notation is popular and common: <span class="math display">
\mathbb{P}\{E\mid \mathrm{do}(X:=x)\} = \mathbb{P}_{M[X:=x]}(E)
</span></p>
<p>This notation analogizes the do-operation with the usual notation for conditional probabilities, and is often convenient when doing calculations involving the do-operator. Keep in mind, however, that the do-operator (action) is fundamentally different from the conditioning operator (observation).</p>
</section>
<section id="causal-effects" class="level2">
<h2>Causal effects</h2>
<p>The <em>causal effect</em> of an action&nbsp;<span class="math inline">X:=x</span> on a variable&nbsp;<span class="math inline">Y</span> refers to the distribution of the variable&nbsp;<span class="math inline">Y</span> in the model&nbsp;<span class="math inline">M[X:=x].</span> When we speak of the causal effect of a variable&nbsp;<span class="math inline">X</span> on another variable&nbsp;<span class="math inline">Y</span> we refer to all the ways in which setting&nbsp;<span class="math inline">X</span> to any possible value&nbsp;<span class="math inline">x</span> affects the distribution of&nbsp;<span class="math inline">Y</span>.</p>
<p>Often we think of&nbsp;<span class="math inline">X</span> as a binary treatment variable and are interested in a quantity such as</p>
<p><span class="math display">
\mathbb{E}_{M[X:=1]}[Y] - \mathbb{E}_{M[X:=0]}[Y]\,.
</span></p>
<p>This quantity is called the <em>average treatment effect</em>. It tells us how much treatment (action&nbsp;<span class="math inline">X:=1</span>) increases the expectation of&nbsp;<span class="math inline">Y</span> relative to no treatment (action&nbsp;<span class="math inline">X:=0</span>). Causal effects are population quantities. They refer to effects averaged over the whole population. Often the effect of treatment varies greatly from one individual or group of individuals to another. Such treatment effects are called <em>heterogeneous</em>.</p>
</section>
</section>
<section id="confounding" class="level1">
<h1>Confounding</h1>
<p>Important questions in causality relate to when we can rewrite a do-operation in terms of conditional probabilities. When this is possible, we can estimate the effect of the do-operation from conventional conditional probabilities that we can estimate from data.</p>
<p>The simplest question of this kind asks when a causal effect&nbsp;<span class="math inline">\mathbb{P}\{Y=y\mid \mathrm{do}(X:=x)\}</span> coincides with the condition probability&nbsp;<span class="math inline">\mathbb{P}\{Y=y\mid X=x\}.</span> In general, this is not true. After all, the difference between observation (conditional probability) and action (interventional calculus) is what motivated the development of causality.</p>
<p>The disagreement between interventional statements and conditional statements is so important that it has a well-known name: <em>confounding</em>. We say that&nbsp;<span class="math inline">X</span> and&nbsp;<span class="math inline">Y</span> are confounded when the causal effect of action&nbsp;<span class="math inline">X:=x</span> on&nbsp;<span class="math inline">Y</span> does not coincide with the corresponding conditional probability.</p>
<p>When&nbsp;<span class="math inline">X</span> and&nbsp;<span class="math inline">Y</span> are confounded, we can ask if there is some combination of conditional probability statements that give us the desired effect of a do-intervention. This is generally possible given a causal graph by conditioning on the parent nodes&nbsp;<span class="math inline">\mathit{PA}</span> of the node&nbsp;<span class="math inline">X</span>: <span class="math display">
\mathbb{P}\{Y=y\mid \mathrm{do}(X:=x)\}
= \sum_z \mathbb{P}\{Y=y\mid X=x, \mathit{PA} = z\}
\mathbb{P}\{\mathit{PA} = z\}
</span> This formula is called the <em>adjustment formula</em>. It gives us one way of estimating the effect of a do-intervention in terms of conditional probabilities.</p>
<p>The adjustment formula is one example of what is often called <em>controlling for</em> a set of variables: We estimate the effect of&nbsp;<span class="math inline">X</span> on&nbsp;<span class="math inline">Y</span> separately in every slice of the population defined by a condition&nbsp;<span class="math inline">Z=z</span> for every possible value of&nbsp;<span class="math inline">z</span>. We then average these estimated sub-population effects weighted by the probability of&nbsp;<span class="math inline">Z=z</span> in the population. To give an example, when we control for age, we mean that we estimate an effect separately in each possible age group and then average out the results so that each age group is weighted by the fraction of the population that falls into the age group.</p>
<p>Controlling for more variables in a study isn’t always the right choice. It depends on the graph structure. Let’s consider what happens when we control for the variable&nbsp;<span class="math inline">Z</span> in the three causal graphs we discussed above.</p>
<ul>
<li>Controlling for a confounding variable <span class="math inline">Z</span> in a fork <span class="math inline">X \leftarrow Z \rightarrow Y</span> will deconfound the effect of <span class="math inline">X</span> on <span class="math inline">Y</span>.</li>
<li>Controlling for a mediator <span class="math inline">Z</span> on a chain <span class="math inline">X\rightarrow Z\rightarrow Y</span> will eliminate some of the causal influence of <span class="math inline">X</span> on <span class="math inline">Y</span>.</li>
<li>Controlling for a collider will create correlation between <span class="math inline">X</span> and <span class="math inline">Y</span>. That is the opposite of what controlling for <span class="math inline">Z</span> accomplishes in the case of a fork. The same is true if we control for a descendant of a collider.</li>
</ul>
<section id="the-backdoor-criterion" class="level2">
<h2>The backdoor criterion</h2>
<p>At this point, we might worry that things get increasingly complicated. As we introduce more nodes in our graph, we might fear a combinatorial explosion of possible scenarios to discuss. Fortunately, there are simple sufficient criteria for choosing a set of deconfounding variables that is safe to control for.</p>
<p>A well known graph-theoretic notion is the <em>backdoor</em> criterion<span class="citation" data-cites="pearl2009causality">.<span><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle" /><span class="sidenote">Pearl, <em>Causality</em> (Cambridge University Press, 2009).</span></span></span> Two variables are confounded if there is a so-called <em>backdoor</em> path between them. A <em>backdoor path</em> from&nbsp;<span class="math inline">X</span> to&nbsp;<span class="math inline">Y</span> is any path starting at&nbsp;<span class="math inline">X</span> with a backward edge “<span class="math inline">\leftarrow</span>” into&nbsp;<span class="math inline">X</span> such as:</p>
<p><span class="math display"> X \leftarrow A \rightarrow B \leftarrow C \rightarrow Y </span></p>
<p>Intuitively, backdoor paths allow information flow from&nbsp;<span class="math inline">X</span> to&nbsp;<span class="math inline">Y</span> in a way that is not causal. To deconfound a pair of variables we need to select a <em>backdoor set</em> of variables that “blocks” all backdoor paths between the two nodes. A backdoor path involving a chain&nbsp;<span class="math inline">A\rightarrow B\rightarrow C</span> can be blocked by controlling for&nbsp;<span class="math inline">B</span>. Information by default cannot flow through a collider&nbsp;<span class="math inline">A\rightarrow B\leftarrow C</span>. So we only have to be careful not to open information flow through a collider by conditioning on the collider, or descendant of a collider.</p>
</section>
<section id="unobserved-confounding" class="level2">
<h2>Unobserved confounding</h2>
<p>The adjustment formula might suggest that we can always eliminate confounding bias by conditioning on the parent nodes. However, this is only true in the absence of <em>unobserved confounding</em>. In practice often there are variables that are hard to measure, or were simply left unrecorded. We can still include such unobserved nodes in a graph, typically denoting their influence with dashed lines, instead of solid lines.</p>
<figure>
<img src="assets/causal-unob-conf.svg" style="width:75.0%" alt="Two cases of unobserved confounding." /><figcaption aria-hidden="true">Two cases of unobserved confounding.</figcaption>
</figure>
<p>The above figure shows two cases of unobserved confounding. In the first example, the causal effect of&nbsp;<span class="math inline">X</span> on&nbsp;<span class="math inline">Y</span> is unidentifiable. In the second case, we can block the confounding backdoor path&nbsp;<span class="math inline">X\leftarrow Z\rightarrow W\rightarrow Y</span> by controlling for&nbsp;<span class="math inline">W</span> even though&nbsp;<span class="math inline">Z</span> is not observed. The backdoor criterion lets us work around unobserved confounders in some cases where the adjustment formula alone wouldn’t suffice.</p>
<p>Unobserved confounding nonetheless remains a major obstacle in practice. The issue is not just lack of measurement, but often lack of anticipation or awareness of a counfounding variable. We can try to combat unobserved confounding by increasing the number of variables under consideration. But as we introduce more variables into our study, we also increase the burden of coming up with a valid causal model for all variables under consideration. In practice, it is not uncommon to control for as many variables as possible in a hope to disable confounding bias. However, as we saw, controlling for mediators or colliders can be harmful.</p>
</section>
<section id="randomization" class="level2">
<h2>Randomization</h2>
<p>The backdoor criterion gives a non-experimental way of eliminating confounding bias given a causal model and a sufficient amount of observational data from the joint distribution of the variables. An alternative experimental method of eliminating confounding bias is the well-known <em>randomized controlled trial</em>.</p>
<p>In a <em>randomized controlled trial</em> a group of subjects is randomly partitioned into a <em>control group</em> and a <em>treatment group</em>. Participants do not know which group they were assigned to and neither do the staff administering the trial. The treatment group receives an actual treatment, such as a drug that is being tested for efficacy, while the control group receives a placebo identical in appearance. An outcome variable is measured for all subjects.</p>
<p>The goal of a randomized controlled trial is to break natural inclination. Rather than observing who chose to be treated on their own, we assign treatment randomly. Thinking in terms of causal models, what this means is that we eliminate all incoming edges into the treatment variable. In particular, this closes all backdoor paths and hence avoids confounding bias.</p>
<p>There are many reasons why often randomized controlled trials are difficult or impossible to administer. Treatment might be physically or legally impossible, too costly, or too dangerous. As we saw, randomized controlled trials are not always necessary for avoiding confounding bias and for reasoning about cause and effect. Nor are they free of issues and pitfalls<span class="citation" data-cites="deaton2018understanding">.<span><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle" /><span class="sidenote">Deaton and Cartwright, <span>“Understanding and Misunderstanding Randomized Controlled Trials,”</span> <em>Social Science &amp; Medicine</em> 210 (2018): 2–21.</span></span></span></p>
</section>
</section>
<section id="graphical-discrimination-analysis" class="level1">
<h1>Graphical discrimination analysis</h1>
<p>We now explore how we can bring causal graphs to bear on discussions of discrimination. We return to the example of graduate admissions at Berkeley and develop a causal perspective on the earlier analysis.</p>
<p>The first step is to come up with a plausible causal graph consistent with the data that we saw earlier. The data contained only three variables, sex&nbsp;<span class="math inline">A</span>, department choice&nbsp;<span class="math inline">Z</span>, and admission decision&nbsp;<span class="math inline">Y</span>. It makes sense to draw two arrows&nbsp;<span class="math inline">A\rightarrow Y</span> and&nbsp;<span class="math inline">Z\rightarrow Y</span>, because both features&nbsp;<span class="math inline">A</span> and&nbsp;<span class="math inline">Z</span> are available to the institution when making the admissions decision. We’ll draw one more arrow, for now, simply because we have to. If we only included the two arrows&nbsp;<span class="math inline">A\rightarrow Y</span> and&nbsp;<span class="math inline">Z\rightarrow Y</span>, our graph would claim that&nbsp;<span class="math inline">A</span> and&nbsp;<span class="math inline">Z</span> are statistically independent. However, this claim is inconsistent with the data. We can see from the table that several departments have a statistically significant gender bias among applicants. This means we need to include either the arrow&nbsp;<span class="math inline">A\rightarrow Z</span> or&nbsp;<span class="math inline">Z\rightarrow A</span>. Deciding between the two isn’t as straightforward as it might first appear.</p>
<p>If we interpreted&nbsp;<span class="math inline">A</span> in the narrowest possible sense as the applicant’s <em>reported sex</em>, i.e., literally which box they checked on the application form, we could imagine a scenario where some applicants choose to (mis-)report their sex in a certain way that depends in part on their department choice. Even if we assume no misreporting occurs, it’s hard to substantiate <em>reported sex</em> as a plausible cause of department choice. The fact that an applicant checked a box labeled <em>male</em> certainly isn’t the cause for their interest in engineering.</p>
<p>The proposed causal story in the study is a different one. It alludes to a socialization and preference formation process that took place in the applicant’s life before they applied which. It is this process that, at least in part, depended on the applicant’s sex. To align this story with our causal graph, we need the variable&nbsp;<span class="math inline">A</span> to reference whatever ontological entity it is that through this “socialization process” influences intellectual and professional preferences, and hence, department choice. It is difficult to maintain that this ontological entity coincides with sex as a biological trait. There is no scientific basis to support that the biological trait <em>sex</em> is what determines our intellectual preferences. Few scholars (if any) would currently attempt to maintain a claim such as <em>two X chromosomes cause an interest in English literature</em>.</p>
<p>The truth is that we don’t know the exact mechanism by which the thing referenced by&nbsp;<span class="math inline">A</span> influences department choice. In drawing the arrow&nbsp;<span class="math inline">A</span> to&nbsp;<span class="math inline">Z</span> we assert—perhaps with some naivety or ignorance—that there exists such a mechanism. We will discuss the important difficulty we encountered here in depth later on. For now, we commit to this modeling choice and thus arrive at the following graph.</p>
<figure>
<img src="assets/causal-berkeley-basic.svg" style="width:30.0%" alt="Possible causal graph for the UC Berkeley graduate admissions scenario." /><figcaption aria-hidden="true">Possible causal graph for the UC Berkeley graduate admissions scenario.</figcaption>
</figure>
<p>In this graph, department choice mediates the influence of gender on admissions. There’s a direct path from&nbsp;<span class="math inline">A</span> to&nbsp;<span class="math inline">Y</span> and an indirect path that goes through&nbsp;<span class="math inline">Z</span>. We will use this model to put pressure on the claim that <em>there is no evidence of sex discrimination</em>. In causal language, the argument had two components:</p>
<ol type="1">
<li>There appears to be no direct effect of sex <span class="math inline">A</span> on the admissions decision <span class="math inline">Y</span> that favors men.</li>
<li>The indirect effect of <span class="math inline">A</span> on <span class="math inline">Y</span> that is mediated by department choice should not be counted as evidence of discrimination.</li>
</ol>
<p>We will discuss both arguments in turn.</p>
<section id="direct-effects" class="level2">
<h2>Direct effects</h2>
<p>To obtain the direct effect of&nbsp;<span class="math inline">A</span> on&nbsp;<span class="math inline">Y</span> we need to disable all paths between&nbsp;<span class="math inline">A</span> and&nbsp;<span class="math inline">Y</span> except for the direct link. In our model, we can accomplish this by holding department choice&nbsp;<span class="math inline">Z</span> constant and evaluating the conditional distribution of&nbsp;<span class="math inline">Y</span> given&nbsp;<span class="math inline">A</span>. Recall that holding a variable constant is generally not the same as conditioning on the variable. Specifically, a problem would arise if department choice and admissions outcome were confounded by another variable, such as, state of residence&nbsp;<span class="math inline">R</span></p>
<figure>
<img src="assets/causal-berkeley-residence.svg" style="width:30.0%" alt="Alternative causal graph for the UC Berkeley graduate admissions scenario showing influence of residence." /><figcaption aria-hidden="true">Alternative causal graph for the UC Berkeley graduate admissions scenario showing influence of residence.</figcaption>
</figure>
<p>Department choice is now a collider between&nbsp;<span class="math inline">A</span> and&nbsp;<span class="math inline">R</span>. Conditioning on a collider opens the backdoor path&nbsp;<span class="math inline">A\rightarrow Z\leftarrow R\rightarrow Y</span>. In this graph, conditioning on department choice does <em>not</em> give us the desired direct effect. The real possibility that state of residence confounds department choice and decision was the subject of an exchange between Bickel and Kruskal.<span class="citation" data-cites="pearl2018book">.<span><label for="sn-7" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-7" class="margin-toggle" /><span class="sidenote">Pearl and Mackenzie, <em>The Book of Why: The New Science of Cause and Effect</em> (Basic Books, 2018).</span></span></span></p>
<p>If we assume, however, that department choice and admissions decisions are unconfounded, then the approach Bickel, Hammel, and O’Connell took indeed supports the first claim. Unfortunately, the direct effect of a protected variable on a decision is a poor measure of discrimination on its own. At a technical level, it is rather brittle as it cannot detect any form of <em>proxy discrimination</em>. The department could, for example, use the applicant’s personal statement to make inferences about their gender, which are then used to discriminate.</p>
<p>We can think of the direct effect as corresponding to the explicit <em>use</em> of the attribute in the decision rule. The absence of a direct effect loosely corresponds to the somewhat troubled notion of a <em>blind</em> decision rule that doesn’t have explicit access to the sensitive attribute. As we argued in preceding chapters, blind decision rules can still be the basis of discriminatory practices.</p>
</section>
<section id="indirect-paths" class="level2">
<h2>Indirect paths</h2>
<p>Let’s turn to the indirect effect of sex on admission that goes through department choice. It’s tempting to think of the the node&nbsp;<span class="math inline">Z</span> as referencing the applicant’s inherent department preferences. In this view, the department is not responsible for the applicant’s preferences. Therefore the mediating influence of department preferences is not interpreted as a sign of discrimination. This, however, is a substantive judgment that may not be a fact. There are other plausible scenarios consistent with both the data and our causal model, in which the indirect path encodes a pattern of discrimination.</p>
<p>For example, the admissions committee may have advertised the program in a manner that strongly discouraged women from applying. In this case, department preference in part measures exposure to this hostile advertising campaign. Alternatively, the department could have a track record of hostile behavior against women and it is awareness of such that shapes preferences in an applicant. Finally, even blatant discriminatory practices, such as compensating women at a lower rate than equally qualified male graduate students, correspond to an indirect effect mediated by department choice.</p>
<figure>
<img src="assets/causal-berkeley-fear.svg" style="width:40.0%" alt="Alternative causal graph for the UC Berkeley graduate admissions scenario where department preferences are shaped by fear of discrimination." /><figcaption aria-hidden="true">Alternative causal graph for the UC Berkeley graduate admissions scenario where department preferences are shaped by fear of discrimination.</figcaption>
</figure>
<p>Accepting the indirect path as <em>non-discriminatory</em> is to assert that all these scenarios we described are deemed implausible. Fundamentally, we are confronted with a substantive question. The path&nbsp;<span class="math inline">A\rightarrow Z\rightarrow Y</span> could either be where discrimination occurs or what explains the absence thereof. Which case we’re in isn’t a purely technical matter and cannot be resolved without subject matter knowledge. Causal modeling gives us a framework for exposing these questions, but not necessarily one to resolve them.</p>
</section>
<section id="path-inspection" class="level2">
<h2>Path inspection</h2>
<p>To summarize, discrimination may not only occur on the direct pathway from the sensitive category to the outcome. Seemingly innocuous mediating paths can hide discriminatory practices. We have to carefully discuss what pathways we consider evidence for or against discrimination.</p>
<p>To appreciate this point, contrast our Berkeley scenario with the important legal case <em>Griggs v. Duke Power Co.</em> that was argued before the U.S. Supreme Court in 1970. Duke Power Company had introduced the requirement of a high school diploma for certain higher paying jobs. We could draw a causal graph for this scenario not unlike the one for the Berkeley case. There’s a mediating variable (here, level of education), a sensitive category (here, race) and an employment outcome (here, employment in a higher paying job). The company didn’t directly make employment decisions based on race, but rather used the mediating variable. The court ruled that the requirement of a high school diploma was not justified by business necessity, but rather had adverse impact on ethnic minority groups where the prevalence of high school diplomas is lower. Put differently, the court decided that the use of this mediating variable was not an argument against, but rather for discrimination.</p>
<p>Glymour<span class="citation" data-cites="glymour2006using"><span><label for="sn-8" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-8" class="margin-toggle" /><span class="sidenote">Glymour, <span>“Using Causal Diagrams to Understand Common Problems in Social Epidemiology,”</span> <em>Methods in Social Epidemiology</em>, 2006, 393–428.</span></span></span> makes another related and important point about the moral character of mediation analysis:</p>
<blockquote>
<p>Implicitly, the question of what mediates observed social effects informs our view of which types of inequalities are socially acceptable and which types require remediation by social policies. For example, a conclusion that women are “biologically programmed” to be depressed more than men may ameliorate the social obligation to try to reduce gender inequalities in depression. Yet if people get depressed whenever they are, say, sexually harassed—and women are more frequently sexually harassed than men—this suggests a very strong social obligation to reduce the depression disparity by reducing the sexual harassment disparity.</p>
</blockquote>
<p>Ending on a technical note, we currently do not have a method to estimate indirect effects. Estimating an indirect effect somehow requires us to <em>disable</em> the direct influence. There is no way of doing this with the do-operation that we’ve seen so far. However, we will shortly introduce <em>counterfactuals</em>, which among other applications will give us a way of estimating path-specific effects.</p>
</section>
<section id="structural-discrimination" class="level2">
<h2>Structural discrimination</h2>
<p>There’s an additional problem we neglected so far. Imagine a spiteful university administration that systematically defunds graduate programs that attract more female applicants. This structural pattern of discrimination is invisible from the causal model we drew. There is a kind of type mismatch here. Our model talks about individual applicants, their department preferences, and their outcomes. Put differently, individuals are the <em>units</em> of our investigation. University policy is not one of the mechanisms that our model exposes. We cannot <em>featurize</em> university policy to make it an attribute of the individual. As a result we cannot talk about university policy as a cause of discrimination in our model.</p>
<p>The model we chose commits us to an individualistic perspective that frames discrimination as the consequence of how decision makers respond to information about individuals. An analogy is helpful. In epidemiology, scientists can seek the cause of health outcomes in biomedical aspects and lifestyle choices of individuals, such as whether or not an individual smokes, exercises, maintains a balanced diet etc. The growing field of social epidemiology criticizes the view of individual choices as causes of health outcomes, and instead draws attention to social and structural causes<span class="citation" data-cites="krieger2011epidemiology">,<span><label for="sn-9" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-9" class="margin-toggle" /><span class="sidenote">Krieger, <span>“Epidemiology and the People’s Health: Theory and Context,”</span> 2011.</span></span></span> such as poverty and inequality.</p>
<p>Similarly, we can contrast the individualistic perspective on discrimination with structural discrimination. Causal modeling can in principle be used to study the causes of structural discrimination, as well. But it requires a different perspective than the one we chose for our Berkeley scenario.</p>
</section>
</section>
<section id="counterfactuals" class="level1">
<h1>Counterfactuals</h1>
<p>Fully specified structural causal models allow us to ask causal questions that are more delicate than the mere effect of an action. Specifically, we can ask <em>counterfactual</em> questions such as: Would I have avoided the traffic jam had I taken a different route this morning? Counterfactual questions are common and relevant for questions of discrimination. We can computer the answer to counterfactual questions given a structural causal model. The procedure for extracting the answer from the model looks a bit subtle at first. We’ll walk through the formal details starting from a simple example before returning to our discussion of discrimination.</p>
<section id="a-simple-counterfactual" class="level2">
<h2>A simple counterfactual</h2>
<p>To understand counterfactuals, we first need to convince ourselves that they aren’t quite as straightforward as a single substitution in our model.</p>
<p>Assume every morning we need to decide between two routes&nbsp;<span class="math inline">X=0</span> and&nbsp;<span class="math inline">X=1</span>. On bad traffic days, indicated by&nbsp;<span class="math inline">U=1</span>, both routes are bad. On good days, indicated by&nbsp;<span class="math inline">U=0</span>, the traffic on either route is good unless there was an accident on the route. Let’s say that&nbsp;<span class="math inline">U\sim B(1/2)</span> follows the distribution of an unbiased coin toss. Accidents occur independently on either route with probability&nbsp;<span class="math inline">1/2.</span> So, choose two Bernoulli random variables&nbsp;<span class="math inline">U_0, U_1\sim B(1/2)</span> that tell us if there is an accident on route&nbsp;<span class="math inline">0</span> and route&nbsp;<span class="math inline">1</span>, respectively. We reject all external route guidance and instead decide on which route to take uniformly at random. That is,&nbsp;<span class="math inline">X:=U_X\sim B(1/2)</span> is also an unbiased coin toss.</p>
<p>Introduce a variable&nbsp;<span class="math inline">Y\in\{0,1\}</span> that tells us whether the traffic on the chosen route is good (<span class="math inline">Y=0</span>) or bad (<span class="math inline">Y=1</span>). Reflecting our discussion above, we can express&nbsp;<span class="math inline">Y</span> as</p>
<p><span class="math display">
Y := X\cdot \max\{U, U_1\} + (1-X)\max\{U, U_0\} \,.
</span></p>
<p>In words, when&nbsp;<span class="math inline">X=0</span> the first term disappears and so traffic is determined by the larger of the two values&nbsp;<span class="math inline">U</span> and&nbsp;<span class="math inline">U_0</span>. Similarly, when&nbsp;<span class="math inline">X=1</span> traffic is determined by the larger of&nbsp;<span class="math inline">U</span> and&nbsp;<span class="math inline">U_1</span>.</p>
<figure>
<img src="assets/causal-counterf.svg" style="width:25.0%" alt="Causal diagram for our traffic scenario." /><figcaption aria-hidden="true">Causal diagram for our traffic scenario.</figcaption>
</figure>
<p>Now, suppose one morning we have&nbsp;<span class="math inline">X=1</span> and we observe bad traffic&nbsp;<span class="math inline">Y=1.</span> Would we have been better off taking the alternative route this morning?</p>
<p>A natural attempt to answer this question is to compute the likelihood of&nbsp;<span class="math inline">Y=0</span> after the do-operation&nbsp;<span class="math inline">X:=0</span>, that is,&nbsp;<span class="math inline">\mathbb{P}_{M[X:=0]}(Y=0).</span> A quick calculation reveals that this probability is&nbsp;<span class="math inline">\frac12 \cdot \frac12 = 1/4.</span> Indeed, given the substitution&nbsp;<span class="math inline">X:=0</span> in our model, for the traffic to be good we need that&nbsp;<span class="math inline">\max\{U, U_0\}=0.</span> This can only happen when both&nbsp;<span class="math inline">U=0</span> (probability&nbsp;<span class="math inline">1/2</span>) and&nbsp;<span class="math inline">U_0=0</span> (probability&nbsp;<span class="math inline">1/2</span>).</p>
<p>But this isn’t the correct answer to our question. The reason is that we took route&nbsp;<span class="math inline">X=1</span> and observed that&nbsp;<span class="math inline">Y=1.</span> From this observation, we can deduce that certain background conditions did not manifest for they are inconsistent with the observed outcome. Formally, this means that certain settings of the noise variables&nbsp;<span class="math inline">(U, U_0, U_1)</span> are no longer feasible given the observed event&nbsp;<span class="math inline">\{Y=1, X=1\}</span>. Specifically, if&nbsp;<span class="math inline">U</span> and&nbsp;<span class="math inline">U_1</span> had both been zero, we would have seen no bad traffic on route&nbsp;<span class="math inline">X=1</span>, but this is contrary to our observation. In fact, the available evidence&nbsp;<span class="math inline">\{Y=1, X=1\}</span> leaves only the following settings for&nbsp;<span class="math inline">U</span> and&nbsp;<span class="math inline">U_1</span>:</p>
<table>
<caption>Possible noise settings after observing evidence</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">U</span></th>
<th style="text-align: center;"><span class="math inline">U_1</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>We leave out&nbsp;<span class="math inline">U_0</span> from the table, since its distribution is unaffected by our observation. Each of the remaining three cases is equally likely, which in particular means that the event&nbsp;<span class="math inline">U=1</span> now has probability&nbsp;<span class="math inline">2/3.</span> In the absence of any additional evidence, recall,&nbsp;<span class="math inline">U=1</span> had probability&nbsp;<span class="math inline">1/2.</span> What this means is that the observed evidence&nbsp;<span class="math inline">\{Y=1, X=1\}</span> has biased the distribution of the noise variable&nbsp;<span class="math inline">U</span> toward&nbsp;<span class="math inline">1</span>. Let’s use the letter&nbsp;<span class="math inline">U&#39;</span> to refer to this biased version of&nbsp;<span class="math inline">U</span>. Formally,&nbsp;<span class="math inline">U&#39;</span> is distributed according to the distribution of&nbsp;<span class="math inline">U</span> conditional on the event&nbsp;<span class="math inline">\{Y=1, X=1\}.</span></p>
<p>Working with this biased noise variable, we can again entertain the effect of the action&nbsp;<span class="math inline">X:=0</span> on the outcome&nbsp;<span class="math inline">Y</span>. For&nbsp;<span class="math inline">Y=0</span> we need that&nbsp;<span class="math inline">\max\{U&#39;, U_0\}=0.</span> This means that&nbsp;<span class="math inline">U&#39;=0</span>, an event that now has probability&nbsp;<span class="math inline">1/3</span>, and&nbsp;<span class="math inline">U_0=0</span> (probability&nbsp;<span class="math inline">1/2</span> as before). Hence, we get the probability&nbsp;<span class="math inline">1/6=1/2\cdot 1/3</span> for the event that&nbsp;<span class="math inline">Y=0</span> under our do-operation&nbsp;<span class="math inline">X:=0</span>, and after updating the noise variables to account for the observation&nbsp;<span class="math inline">\{Y=1, X=1\}</span>.</p>
<p>To summarize, incorporating available evidence into our calculation decreased the probability of no traffic (<span class="math inline">Y=0</span>) when choosing route&nbsp;<span class="math inline">0</span> from&nbsp;<span class="math inline">1/4</span> to&nbsp;<span class="math inline">1/6</span>. The intuitive reason is that the evidence made it more likely that it was generally a bad traffic day, and even the alternative route would’ve been clogged. More formally, the event that we observed biases the distribution of exogenous noise variables.</p>
<p>We think of the result we just calculated as the <em>counterfactual</em> of choosing the alternative route given the route we chose had bad traffic.</p>
</section>
<section id="the-general-recipe" class="level2">
<h2>The general recipe</h2>
<p>We can generalize our discussion of computing counterfactuals from the previous example to a general procedure. There were three essential steps. First, we incorporated available observational evidence by biasing the exogenous noise variables through a conditioning operation. Second, we performed a do-operation in the structural causal model after we substituted the biased noise variables. Third, we computed the distribution of a target variable. These three steps are typically called <em>abduction</em>, <em>action</em>, and <em>prediction</em>, as can be described as follows.</p>
<div class="numenv Definition">
<span class="numenv Definition title">Definition 2.</span>
<p>Given a structural causal model&nbsp;<span class="math inline">M</span>, an observed event&nbsp;<span class="math inline">E</span>, an action&nbsp;<span class="math inline">X:= x</span> and target variable&nbsp;<span class="math inline">Y,</span> we define the <em>counterfactual</em> <span class="math inline">Y_{X:=x}(E)</span> by the following three step procedure:</p>
<ol type="1">
<li><strong>Abduction:</strong> Adjust noise variables to be consistent with the observed event. Formally, condition the joint distribution of <span class="math inline">U=(U_1,...,U_d)</span> on the event <span class="math inline">E</span>. This results in a biased distribution <span class="math inline">U&#39;</span>.</li>
<li><strong>Action:</strong> Perform do-intervention <span class="math inline">X:=x</span> in the structural causal model <span class="math inline">M</span> resulting in the model <span class="math inline">M&#39;=M[X:=x].</span></li>
<li><strong>Prediction:</strong> Compute target counterfactual <span class="math inline">Y_{X:=x}(E)</span> by using <span class="math inline">U&#39;</span> as the random seed in <span class="math inline">M&#39;.</span></li>
</ol>
</div>
<p>It’s important to realize that this procedure <em>defines</em> what a counterfactual is in a structural causal model. The notation&nbsp;<span class="math inline">Y_{X:=x}(E)</span> denotes the outcome of the procedure and is part of the definition. We haven’t encountered this notation before. Put in words, we interpret the formal counterfactual&nbsp;<span class="math inline">Y_{X:=x}(E)</span> as the value&nbsp;<span class="math inline">Y</span> would’ve taken had the variable&nbsp;<span class="math inline">X</span> been set to value&nbsp;<span class="math inline">x</span> in the circumstances described by the event&nbsp;<span class="math inline">E</span>.</p>
<p>In general, the counterfactual&nbsp;<span class="math inline">Y_{X:=x}(E)</span> is a random variable that varies with&nbsp;<span class="math inline">U&#39;</span>. But counterfactuals can also be deterministic. When the event&nbsp;<span class="math inline">E</span> narrows down the distribution of&nbsp;<span class="math inline">U</span> to a single point mass, called <em>unit</em>, the variable&nbsp;<span class="math inline">U&#39;</span> is constant and hence the counterfactual&nbsp;<span class="math inline">Y_{X:=x}(E)</span> reduces to a single number. In this case, it’s common to use the shorthand notation&nbsp;<span class="math inline">Y_{x}(u)=Y_{X:=x}(\{U=u\}),</span> where we make the variable&nbsp;<span class="math inline">X</span> implicit, and let&nbsp;<span class="math inline">u</span> refer to a single unit.</p>
<p>The motivation for the name <em>unit</em> derives from the common situation where the structural causal model describes a population of entities that form the atomic units of our study. It’s common for a unit to be an individual (or the description of a single individual). However, depending on application, the choice of units can vary. In our traffic example, the noise variables dictate which route we take and what the road conditions are.</p>
<p>Answers to counterfactual questions strongly depend on the specifics of the structural causal model, including the precise model of how the exogenous noise variables come into play. It’s possible to construct two models that have identical graph structures, and behave identically under interventions, yet give different answers to counterfactual queries.<span class="citation" data-cites="peters2017elements"><span><label for="sn-10" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-10" class="margin-toggle" /><span class="sidenote">Peters, Janzing, and Schölkopf, <em>Elements of Causal Inference</em> (MIT Press, 2017).</span></span></span></p>
</section>
<section id="potential-outcomes" class="level2">
<h2>Potential outcomes</h2>
<p>The <em>potential outcomes</em> framework is a popular formal basis for causal inference, which goes about counterfactuals differently. Rather than deriving them from a structural causal model, we assume their existence as ordinary random variables, albeit some unobserved.</p>
<p>Specifically, we assume that for every unit&nbsp;<span class="math inline">u</span> there exist random variables&nbsp;<span class="math inline">Y_x(u)</span> for every possible value of the assignment&nbsp;<span class="math inline">x</span>. In the potential outcomes model, it’s customary to think of a binary <em>treatment</em> variable&nbsp;<span class="math inline">X</span> so that&nbsp;<span class="math inline">x</span> assumes only two values,&nbsp;<span class="math inline">0</span> for <em>untreated</em>, and&nbsp;<span class="math inline">1</span> for <em>treated</em>. This gives us two potential outcome variables&nbsp;<span class="math inline">Y_0(u)</span> and&nbsp;<span class="math inline">Y_1(u)</span> for each unit&nbsp;<span class="math inline">u</span>. There is some potential for notational confusion here. Readers familiar with the potential outcomes model may be used to the notation “<span class="math inline">Y_i(0), Y_i(1)</span>” for the two potential outcomes corresponding to unit&nbsp;<span class="math inline">i</span>. In our notation the unit (or, more generally, set of units) appears in the parentheses and the subscript denotes the substituted value for the variable we intervene on.</p>
<p>The key point about the potential outcomes model is that we only observe the potential outcome&nbsp;<span class="math inline">Y_1(u)</span> for units that were treated. For untreated units we observe&nbsp;<span class="math inline">Y_0(u)</span>. In other words, we can never simultaneously observe both, although they’re both assumed to exist in a formal sense. Formally, the outcome&nbsp;<span class="math inline">Y(u)</span> for unit&nbsp;<span class="math inline">u</span> that we observe depends on the binary treatment&nbsp;<span class="math inline">T(u)</span> and is given by the expression:</p>
<p><span class="math display">Y(u)=Y_0(u)\cdot(1-T(u))+Y_1(u) \cdot T(u)</span></p>
<p>It’s often convenient to omit the parentheses from our notation for counterfactuals so that this expression would read&nbsp;<span class="math inline">Y=Y_0\cdot(1-T)+Y_1\cdot T</span>.</p>
<p>We can revisit our traffic example in this framework. The next table summarizes what information is observable in the potential outcomes model. We think of the route we choose as the treatment variable, and the observed traffic as reflecting one of the two potential outcomes.</p>
<table>
<caption>Traffic example in the potential outcomes model</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Route <span class="math inline">X</span></th>
<th style="text-align: center;">Outcome <span class="math inline">Y_0</span></th>
<th style="text-align: center;">Outcome <span class="math inline">Y_1</span></th>
<th style="text-align: right;">Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">?</td>
<td style="text-align: right;">1/8</td>
</tr>
<tr class="even">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">?</td>
<td style="text-align: right;">3/8</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">0</td>
<td style="text-align: right;">1/8</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;">3/8</td>
</tr>
</tbody>
</table>
<p>Often this information comes in the form of samples. For example, we might observe the traffic on different days. With sufficiently many samples, we can estimate the above frequencies with arbitrary accuracy.</p>
<table>
<caption>Traffic data in the potential outcomes model</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: center;">Route <span class="math inline">X</span></th>
<th style="text-align: center;">Outcome <span class="math inline">Y_0</span></th>
<th style="text-align: center;">Outcome <span class="math inline">Y_1</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">?</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">?</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">?</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
</tr>
</tbody>
</table>
<p>A typical query in the potential outcomes model is the <em>average treatment effect</em> <span class="math inline">\mathbb{E}[Y_1 - Y_0].</span> Here the expectation is taken over the properly weighted units in our study. If units correspond to equally weighted individuals, the expectation is an average over these individuals.</p>
<p>In our original traffic example, there were&nbsp;<span class="math inline">16</span> units corresponding to the background conditions given by the four binary variables&nbsp;<span class="math inline">U, U_0, U_1, U_X</span>. When the units in the potential outcome model agree with those of a structural causal model, then causal effects computed in the potential outcomes model agree with those computed in the structural equation model. The two formal frameworks are perfectly consistent with each other.</p>
<p>As is intuitive from the table above, causal inference in the potential outcomes framework can be thought of as filling in the missing entries (“?”) in the table above. This is sometimes called <em>missing data imputation</em> and there are numerous statistical methods for this task. If we could <em>reveal</em> what’s behind the question marks, estimating the average treatment effect would be as easy as counting rows.</p>
<p>There is a set of established conditions under which causal inference becomes possible:</p>
<ol type="1">
<li><strong>Stable Unit Treatment Value Assumption</strong> (SUTVA): The treatment that one unit receives does not change the effect of treatment for any other unit.</li>
<li><strong>Consistency</strong>: Formally, <span class="math inline">Y=Y_0(1-T)+Y_1T.</span> That is, <span class="math inline">Y=Y_0</span> if <span class="math inline">T=0</span> and <span class="math inline">Y=Y_1</span> if <span class="math inline">T=1</span>. In words, the outcome <span class="math inline">Y</span> agrees with the potential outcome corresponding to the treatment indicator.</li>
<li><strong>Ignorability</strong>: The potential outcomes are independent of treatment given some deconfounding variables <span class="math inline">Z</span>, i.e., <span class="math inline">T\bot (Y_0, Y_1)\mid Z</span>. In words, the potential outcomes are conditionally independent of treatment given some set of deconfounding variables.</li>
</ol>
<p>The first two assumptions automatically hold for counterfactual variables derived from structural causal models according to the procedure described above. This assumes that the units in the potential outcomes framework correspond to the atomic values of the background variables in the structural causal model.</p>
<p>The third assumption is a major one. It’s easiest to think of it as aiming to formalize the guarantees of a perfectly executed randomized controlled trial. The assumption on its own cannot be verified or falsified, since we never have access to samples with both potential outcomes manifested. However, we can verify if the assumption is consistent with a given structural causal model by checking if the set&nbsp;<span class="math inline">Z</span> blocks all backdoor paths from treatment&nbsp;<span class="math inline">T</span> to outcome&nbsp;<span class="math inline">Y</span>.</p>
<p>There’s no tension between structural causal models and potential outcomes and there’s no harm in having familiarity with both. It nonetheless makes sense to say a few words about the differences of the two approaches.</p>
<p>We can derive potential outcomes from a structural causal model as we did above, but we cannot derive a structural causal model from potential outcomes alone. A structural causal model in general encodes more assumptions about the relationships of the variables. This has several consequences. On the one hand, a structural causal model gives us a broader set of formal concepts (causal graphs, mediating paths, counterfactuals for every variable, and so on). On the other hand, coming up with a plausibly valid structural causal model is often a daunting task that might require knowledge that is simply not available. We will dive deeper into questions of validity below. Difficulty to come up with a plausible causal model often exposes unsettled substantive questions that require resolution first.</p>
<p>The potential outcomes model, in contrast, is generally easier to apply. There’s a broad set of statistical estimators of causal effects that can be readily applied to observational data. But the ease of application can also lead to abuse. The assumptions underpinning the validity of such estimators are experimentally unverifiable. Frivolous application of causal effect estimators in situations where crucial assumptions do not hold can lead to false results, and consequently to ineffective or harmful interventions.</p>
</section>
</section>
<section id="counterfactual-discrimination-analysis" class="level1">
<h1>Counterfactual discrimination analysis</h1>
<p>Counterfactuals serve at least two purposes for us. On the technical side, counterfactuals give us a way to compute path-specific causal effects. This allows us to make path analysis a quantitative matter. On the conceptual side, counterfactuals let us engage with the important normative debate about whether discrimination can be captured by counterfactual criteria. We will discuss each of these in turn.</p>
<section id="quantitative-path-analysis" class="level2">
<h2>Quantitative path analysis</h2>
<p>Mediation analysis is a venerable subject dating back decades<span class="citation" data-cites="baron1986moderator">.<span><label for="sn-11" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-11" class="margin-toggle" /><span class="sidenote">Baron and Kenny, <span>“The Moderator–Mediator Variable Distinction in Social Psychological Research: Conceptual, Strategic, and Statistical Considerations.”</span> <em>Journal of Personality and Social Psychology</em> 51, no. 6 (1986): 1173.</span></span></span> Generally speaking, the goal of mediation analysis is to identify a mechanism through which a cause has an effect. We will review some recent developments and how they relate to questions of discrimination.</p>
<p>In the language of our formal framework, mediation analysis aims to decompose a total causal effect into path-specific components. We will illustrate the concepts in the basic three variable case of a mediator, although the ideas extend to more complicated structures.</p>
<figure>
<img src="assets/causal-chain.svg" style="width:30.0%" alt="Causal graph with mediator Z." /><figcaption aria-hidden="true">Causal graph with mediator <span class="math inline">Z</span>.</figcaption>
</figure>
<p>There are two different paths from&nbsp;<span class="math inline">X</span> to&nbsp;<span class="math inline">Y</span>. A direct path and a path through the mediator&nbsp;<span class="math inline">Z</span>. The conditional expectation&nbsp;<span class="math inline">\mathbb{E}[Y\mid X=x]</span> lumps together influence from both paths. If there were another confounding variable in our graph influencing both&nbsp;<span class="math inline">X</span> and&nbsp;<span class="math inline">Y</span>, then the conditional expectation would also include whatever correlation is the result of confounding. We can eliminate the confounding path by virtue of the do-operator&nbsp;<span class="math inline">\mathbb{E}[Y\mid \mathrm{do}(X:=x)]</span>. This gives us the total effect of the action&nbsp;<span class="math inline">X:=x</span> on&nbsp;<span class="math inline">Y</span>. But the total effect still conflates the two causal pathways, the direct effect and the indirect effect. We will now see how we can identify the direct and indirect effects separately.</p>
<p>The direct effect we already dealt with earlier as it did not require any counterfactuals. Recall, we can hold the mediator fixed at level&nbsp;<span class="math inline">Z:=z</span> and consider the effect of treatment&nbsp;<span class="math inline">X:=1</span> compared with no treatment&nbsp;<span class="math inline">X:=0</span> as follows:</p>
<p><span class="math display">
\mathbb{E}\left[ Y \mid \mathrm{do}(X:=1, Z:=z) \right]
- \mathbb{E}\left[ Y \mid \mathrm{do}(X:=0, Z:=z) \right]
\,.
</span></p>
<p>We can rewrite this expression in terms of counterfactuals equivalently as:</p>
<p><span class="math display">
\mathbb{E}\left[ Y_{X:=1, Z:=z} - Y_{X:=0, Z:=z} \right]
\,.
</span></p>
<p>To be clear, the expectation is taken over the background variables in our structural causal models. In other words, the counterfactuals inside the expectation are invoked with an elementary setting&nbsp;<span class="math inline">u</span> of the background variables, i.e.,&nbsp;<span class="math inline">Y_{X:=1, Z:=z}(u) - Y_{X:=0,Z:=x}(u)</span> and the expectation averages over all possible settings.</p>
<p>The formula for the direct effect above is usually called <em>controlled direct effect</em>, since it requires setting the mediating variable to a specified level. Sometimes it is desirable to allow the mediating variable to vary as it would had no treatment occurred. This too is possible with counterfactuals and it leads to a notion called <em>natural direct effect</em>, defined as:</p>
<p><span class="math display">
\mathbb{E}\left[ Y_{X:=1, Z:=Z_{X:=0}} - Y_{X:=0, Z:=Z_{X:=0}} \right]
\,.
</span></p>
<p>The counterfactual&nbsp;<span class="math inline">Y_{X:=1, Z:=Z_{X:=0}}</span> is the value that&nbsp;<span class="math inline">Y</span> would obtain had&nbsp;<span class="math inline">X</span> been set to&nbsp;<span class="math inline">1</span> and had&nbsp;<span class="math inline">Z</span> been set to the value&nbsp;<span class="math inline">Z</span> would’ve assumed had&nbsp;<span class="math inline">X</span> been set to&nbsp;<span class="math inline">0</span>.</p>
<p>The advantage of this slightly mind-bending construction is that it gives us an analogous notion of <em>natural indirect effect</em>:</p>
<p><span class="math display">
\mathbb{E}\left[ Y_{X:=0, Z:=Z_{X:=1}} - Y_{X:=0, Z:=Z_{X:=0}} \right]
\,.
</span></p>
<p>Here we hold the treatment variable constant at level&nbsp;<span class="math inline">X:=0</span>, but let the mediator variable change to the value it would’ve attained had treatment&nbsp;<span class="math inline">X:=1</span> occurred.</p>
<p>In our three node example, the effect of&nbsp;<span class="math inline">X</span> on&nbsp;<span class="math inline">Y</span> is unconfounded. In the absence of confounding, the natural indirect effect corresponds to the following statement of conditional probability (involving neither counterfactuals nor do-interventions):</p>
<p><span class="math display">
\sum_z \mathbb{E}\left[ Y\mid X=0, Z=z\right]\big(
\mathbb{P}(Z=z\mid X=1)
-\mathbb{P}(Z=z\mid X=0)
\big)\,.
</span></p>
<p>In this case, we can estimate the natural direct and indirect effect from observational data.</p>
<p>The technical possibilities go beyond the case discussed here. In principle, counterfactuals allow us to compute all sorts of path-specific effects even in the presence of (observed) confounders. We can also design decision rules that eliminate path-specific effects we deem undesirable.</p>
</section>
<section id="counterfactual-discrimination-criteria" class="level2">
<h2>Counterfactual discrimination criteria</h2>
<p>Beyond their application to path analysis, counterfactuals can also be used as a tool to put forward normative fairness criteria. Consider the typical setup of Chapter 3. We have features&nbsp;<span class="math inline">X</span>, a sensitive attribute&nbsp;<span class="math inline">A</span>, an outcome variable&nbsp;<span class="math inline">Y</span> and a predictor&nbsp;<span class="math inline">\hat Y</span>.</p>
<p>One criterion that is technically natural would say the following: For every possible demographic described by the event&nbsp;<span class="math inline">E:=\{X:=x, A:=a\}</span> and every possible setting&nbsp;<span class="math inline">a&#39;</span> of&nbsp;<span class="math inline">A</span> we ask that the counterfactual <span class="math inline">\hat Y_{A:=a}(E)</span> and the counterfactual&nbsp;<span class="math inline">\hat Y_{A:=a&#39;}(E)</span> follow the same distribution.</p>
<p>Introduced as <em>counterfactual fairness</em><span class="citation" data-cites="kusner2017counterfactual">,<span><label for="sn-12" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-12" class="margin-toggle" /><span class="sidenote">Kusner et al., <span>“Counterfactual Fairness,”</span> in <em>Advances in Neural Information Processing Systems</em>, 2017, 4069–79.</span></span></span> we refer to this condition as <em>counterfactual demographic parity</em>, since it’s closely related to the observational criterion <em>conditional demographic parity</em>. Recall, conditional demographic parity requires that in each demographic defined by a feature setting&nbsp;<span class="math inline">X=x</span>, the sensitive attribute is independent of the predictor. Formally, we have the conditional independence relation&nbsp;<span class="math inline">\hat Y \bot A \mid X</span>. In the case of a binary predictor, this condition is equivalent to requiring for all feature settings&nbsp;<span class="math inline">x</span> and groups&nbsp;<span class="math inline">a, a&#39;</span>: <span class="math display">
\mathbb{E}[\hat Y \mid X=x, A=a]
= \mathbb{E}[\hat Y \mid X=x, A=a&#39;]
</span> The easiest way to satisfy counterfactual demographic parity is for the predictor&nbsp;<span class="math inline">\hat Y</span> to only use non-descendants of&nbsp;<span class="math inline">A</span> in the causal graph. This is analogous to the statistical condition of only using features that are independent of&nbsp;<span class="math inline">A</span>.</p>
<p>In the same way that we defined a counterfactual analog of demographic parity, we can explore causal analogs of other statistical criteria in Chapter 3. In doing so, we need to be careful in separating technical questions about the difference between observational and causal criteria from the normative content of the criterion. Just because a causal variant of a criterion might get around some statistical issues of non-causal correlations does not mean that the causal criterion resolves normative concerns or questions with its observational cousin.</p>
</section>
<section id="counterfactuals-in-the-law" class="level2">
<h2>Counterfactuals in the law</h2>
<p>We’ll now scratch the surface of a deep subject in legal scholarship that we return to in Chapter 6 after developing greater familiarity with the legal background. The subject is the relationship of causal counterfactual claims and legal cases of discrimination. Many technical scholars see support for a counterfactual interpretation of United States discrimination law in various rulings by judges that seemed to have invoked counterfactual language. Here’s a quote from a popular textbook on causal inference<span class="citation" data-cites="pearl2016causal">:<span><label for="sn-13" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-13" class="margin-toggle" /><span class="sidenote">Pearl, Glymour, and Jewell, <em>Causal Inference in Statistics: A Primer</em> (Wiley, 2016).</span></span></span></p>
<blockquote>
<p>U.S. courts have issued clear directives as to what constitutes employment discrimination. According to law makers, “The central question in any employment-discrimination case is whether the employer would have taken the same action had the employee been of a different race (age, sex, religion, national origin etc.) and everything else had been the same.” (In Carson vs Bethlehem Steel Corp., 70 FEP Cases 921, 7th Cir. (1996).)</p>
</blockquote>
<p>Unfortunately, the situation is not so simple. This quote invoked here—and in several other technical papers on the topic—expresses the opinion of judges in the 7th Circuit Court at the time. This court is one of thirteen United States courts of appeals. The case has little precedential value; the quote cannot be considered a definitive statement on what employment discrimination means under either Title VII or Equal Protection law.</p>
<p>More significant in U.S. jurisprudence is the standard of “but-for causation” that has gained support through a 2020 U.S. Supreme Court decision relating to sex discrimination in the case Bostock v. Clayton County. In reference to the Title VII statute about employment discrimination in the Civil Rights Act of 1964, the court argued:</p>
<blockquote>
<p>While the statute’s text does not expressly discuss causation, it is suggestive. The guarantee that each person is entitled to the ‘same right … as is enjoyed by White citizens’ directs our attention to the counterfactual—what would have happened if the plaintiff had been White? This focus fits naturally with the ordinary rule that a plaintiff must prove but-for causation.</p>
</blockquote>
<p>Although the language of counterfactuals appears here, the notion of but-for causation may not effectively correspond to a correct causal counterfactual. Expanding on how to interpret but-for causation, the court noted:</p>
<blockquote>
<p>a but-for test directs us to change one thing at a time and see if the outcome changes. If it does, we have found a but-for cause.</p>
</blockquote>
<p>Changing one attribute while holding all others fixed is not in general a correct way of computing counterfactuals in a causal graph. This important issue was central to an major discrimination lawsuit.</p>
</section>
<section id="harvard-college-admissions" class="level2">
<h2>Harvard college admissions</h2>
<p>In a trial dating back to 2015, the plaintiff <em>Students for Fair Admissions</em> (SFFA) allege discrimination in Harvard undergraduate admissions against Asian-Americans. Plaintiff SFFA is an offshoot of a legal defense fund which aims to end the use of race in voting, education, contracting, and employment.</p>
<p>The trial entailed unprecedented discovery regarding higher education admissions processes and decision-making, including statistical analyses of individual-level applicant data from the past five admissions cycles.</p>
<p>The plaintiff’s expert report by Peter S. Arcidiacono, Professor of Economics at Duke University, claims:</p>
<blockquote>
<p>Race plays a significant role in admissions decisions. Consider the example of an Asian-American applicant who is male, is not disadvantaged, and has other characteristics that result in a 25% chance of admission. Simply changing the race of the applicant to white—and leaving all his other characteristics the same—would increase his chance of admission to 36%. Changing his race to Hispanic (and leaving all other characteristics the same) would increase his chance of admission to 77%. Changing his race to African-American (again, leaving all other characteristics the same) would increase his chance of admission to 95%.</p>
</blockquote>
<p>The plaintiff’s charge, summarized above, is based technically on the argument that conditional statistical parity is not satisfied by a model of Harvard’s admissions decisions. Harvard’s decision process isn’t codified as a formal decision rule. Hence, to talk about Harvard’s decision rule formally, we first need to model Harvard’s decision rule. The plaintiff’s expert did so by fitting a logistic regression model against Harvard’s past admissions decisions in terms of variables deemed relevant for the admission decision.</p>
<p>Formally, denote by&nbsp;<span class="math inline">\hat Y</span> the model of Harvard’s admissions decisions, by&nbsp;<span class="math inline">X</span> a set of applicant features deemed relevant for admission, and denoting by&nbsp;<span class="math inline">A</span> the applicant’s reported race we have that <span class="math display">
\mathbb{E}[\hat Y \mid X=x, A=a]
&lt; \mathbb{E}[\hat Y \mid X=x, A=a&#39;]-\delta\,,
</span> for some groups&nbsp;<span class="math inline">a,a&#39;</span> and some significant value of&nbsp;<span class="math inline">\delta&gt;0</span>.</p>
<p>The violation of this condition certainly depends on which features we deem relevant for admissions, formally, which features&nbsp;<span class="math inline">X</span> we should condition on. Indeed, this point is to a large extent the basis of the response of the defendant’s expert David Card, Professor of Economics at the University of California, Berkeley. Card argues that under a different reasonable choice of&nbsp;<span class="math inline">X</span>, one that includes among other features the applicant’s interview performance and the year they applied in, the observed disparity disappears.</p>
<p>The selection and discussion of what constitute relevant features is certainly important for the interpretation of conditional statistical parity. But arguably a bigger question is whether a violation of conditional statistical parity constitutes evidence of discrimination in the first place. This isn’t merely a question of having selected the right features to condition on.</p>
<p>What is it the plaintiff’s expert report means by “changing his race?” The literal interpretation is to “flip” the race attribute in the input to the model without changing any of the other features of the input. But a formal interpretation in terms of attribute swapping is not necessarily what triggers our moral intuition. As we know now, attribute flipping generally does not produce valid counterfactuals. Indeed, if we assume a causal graph in which some of the relevant features are influenced by race, then computing counterfactuals with respect to race would require adjusting downstream features. Changing the race attribute without a change in any other attribute only corresponds to a counterfactual in the case where race does not have any descendant nodes—an implausible assumption.</p>
<p>Attribute flipping is often mistakenly given a counterfactual causal interpretation. Obtaining valid counterfactuals is in general substantially more involved than flipping a single attribute independently of the others. In particular, we cannot meaningfully talk about counterfactuals without bringing clarity to what exactly we refer to in our causal model and how we can produce <em>valid</em> causal models. We turn to this important topic next.</p>
</section>
</section>
<section id="validity-of-causal-modeling" class="level1">
<h1>Validity of causal modeling</h1>
<p>Consider a claim of employment discrimination of the kind: <em>The company’s hiring practices discriminated against applicants of a certain religion.</em> Suppose we want to interrogate this claim using the formal machinery developed in this chapter. At the outset, this requires that we formally introduce an attributed corresponding to the “religious affiliation” of an individual.</p>
<p>Our first attempt is to model <em>religious affiliation</em> as a personal trait or characteristic that someone either does or does not possess. This trait, call it&nbsp;<span class="math inline">A</span>, may influence choices relating to one’s appearance, social practices, and variables relevant to the job, such as, the person’s level of education&nbsp;<span class="math inline">Z</span>. So, we might like to start with a model such as the following:</p>
<figure>
<img src="assets/causal-race1.svg" style="width:25.0%" alt="Religion as a root node." /><figcaption aria-hidden="true">Religion as a root node.</figcaption>
</figure>
<p>Religious affiliation&nbsp;<span class="math inline">A</span> is a source node in this graph, which influences the person’s level of education&nbsp;<span class="math inline">Z</span>. Members of certain religions may be steered away from or encouraged towards obtaining a higher level of education by their social peer group. This story is similar to how in our Berkeley admissions graph <em>sex</em> influences <em>department choice</em>.</p>
<p>This view of religion places burden on understanding the possible indirect pathways, such as&nbsp;<span class="math inline">A\rightarrow Z\rightarrow Y</span>, through which religion can influence the outcome. There may be insufficient understanding of how a religious affiliation affects numerous other relevant variables throughout life. If we think of religion as a source node in a causal graph, changing it will potentially affect all downstream nodes. For each such downstream node we would need a clear understanding of the mechanisms by which religion influence the node. Where would such <em>scientific knowledge</em> of such relationships come from?</p>
<p>But the causal story around religion might also be different. It could be that obtaining a higher level of education causes an individual to lose their religious beliefs. In fact, this modeling choice has been put forward in technical work on this topic.<span class="citation" data-cites="zhang2018fairness"><span><label for="sn-14" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-14" class="margin-toggle" /><span class="sidenote">Zhang and Bareinboim, <span>“Fairness in Decision-Making — the Causal Explanation Formula,”</span> in <em>Proc. <span class="math inline">32</span>Nd <span>AAAI</span></em>, 2018.</span></span></span> Empirically, data from the United States General Social Survey show that the fraction of respondents changing their reported religion at least once during a 4-year period ranged from about 20% to about 40%.<span class="citation" data-cites="egan2020identity"><span><label for="sn-15" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-15" class="margin-toggle" /><span class="sidenote">Egan, <span>“Identity as Dependent Variable: How Americans Shift Their Identities to Align with Their Politics,”</span> <em>American Journal of Political Science</em> 64, no. 3 (2020): 699–716.</span></span></span> Identities associated with sexuality and social class were found to be even more unstable. Changing one’s identity to better align with one’s politics appeared to explain some of this shift. From this perspective, religious affiliation is influenced by level of education and so the graph might look like this:</p>
<figure>
<img src="assets/causal-race2.svg" style="width:25.0%" alt="Religion as ancestor." /><figcaption aria-hidden="true">Religion as ancestor.</figcaption>
</figure>
<p>This view of religion forces us to correctly identify the variables that influence religious affiliation and are also relevant to the decision. After all, these are the confounders between religion and outcome. Perhaps it is not just level of education, but also socioeconomic status and other factors that have a similar confounding influence.</p>
<p>What is troubling is that in our first graph education is a mediator, while in our second graph it is a confounder. The difference is important; to quote Pearl:</p>
<blockquote>
<p>As you surely know by now, mistaking a mediator for a confounder is one of the deadliest sins in causal inference and may lead to the most outrageous error. The latter invites adjustment; the former forbids it.<span class="citation" data-cites="pearl2018book"><span><label for="sn-16" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-16" class="margin-toggle" /><span class="sidenote">Pearl and Mackenzie, <em>The Book of Why</em>.</span></span></span></p>
</blockquote>
<p>The point is not that these are the only two possible modeling choices for how religious affiliation might interact with decision making processes. Rather, the point is that there exist multiple plausible choices. Either of our modeling choices follows a natural causal story. Identifying which one is justified is no easy task. It’s also not a task that we can circumvent by appeal to some kind of pragmatism. Different modeling choices can lead to completely different claims and consequences.</p>
<p>In order to create a valid causal model, we need to provide clarity about what the thing is that each node references, and what relationships exist between these things. This is a problem of ontology and metaphysics. But we also need to know facts about the things we reference in causal models. This is a problem is epistemology, the theory of knowledge.</p>
<p>These problems might seem mundane for some objects of study. We might have strong scientifically justified beliefs on how certain mechanical parts in an airplane interact. We can use this knowledge to reliably diagnose the cause of an airplane crash. In other domains, especially ones relevant to disputes about discrimination, our subject matter knowledge is less stable and subject to debate.</p>
<section id="social-construction-of-categories" class="level2">
<h2>Social construction of categories</h2>
<p>The difficulties we encountered in our motivating example arise routinely when making causal statements involving human kinds and categories, such as, race, religion, or gender, and how these interact with consequential decisions.</p>
<p>Consider the case of <em>race</em>. The metaphysics of race is a complex subject, highly debated, featuring a range of scholarly accounts today. A book by Glasgow, Haslanger, Jeffers, and Spencer represents four contemporary philosophical views of what race is.<span class="citation" data-cites="glasgow2019what"><span><label for="sn-17" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-17" class="margin-toggle" /><span class="sidenote">Glasgow et al., <span>“What Is Race?: Four Philosophical Views,”</span> 2019.</span></span></span> The construction of racial categories and racial classification of individuals is inextricably tied to a long history of oppression, segregation, and discriminatory practices.<span class="citation" data-cites="bowker2000sorting fields2014racecraft benjamin2019race"><span><label for="sn-18" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-18" class="margin-toggle" /><span class="sidenote">Bowker and Leigh Star, <em>Sorting Things Out: Classification and Its Consequences</em> (MIT Press, 2000); Fields and Fields, <em>Racecraft: The Soul of Inequality in American Life</em> (Verso, 2014); Benjamin, <em>Race After Technology</em> (Polity, 2019).</span></span></span></p>
<p>In the technical literature around discrimination and causality, it’s common for researchers to model <em>race</em> as a source node in a causal graph, which is to say that race has no incoming arrows. As a source node it can directly and indirectly influence an outcome variable, say, <em>getting a job offer</em>. Implicit in this modeling choice is a kind of naturalistic perspective that views race as a biologically grounded trait, similar to <em>sex</em>. The trait exists at the beginning of one’s life. Other variables that come later in life, education and income, for example, thus become ancestors in the causal graph.</p>
<p>This view of race challenges us to identify all the possible indirect pathways through which race can influence the outcome. But it’s not just this modeling challenge that we need to confront. The view of race as a biologically grounded trait stands in contrast with the <em>social constructivist</em> account of race.<span class="citation" data-cites="hacking2000the haslanger2012resisting mallon2018construction glasgow2019what"><span><label for="sn-19" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-19" class="margin-toggle" /><span class="sidenote">Hacking, <em>The Social Construction of What?</em> (Harvard University Press, 2000); Haslanger, <em>Resisting Reality: Social Construction and Social Critique</em> (Oxford University Press, 2012); Mallon, <em>The Construction of Human Kinds</em> (Oxford University Press, 2018); Glasgow et al., <span>“What Is Race?”</span></span></span></span> In this view, roughly speaking, race has no strong biological grounding but rather is a social construct. Race stems from a particular classification of individuals by society, and the shared experiences that stem from the classification. As such, the surrounding social system of an individual influences what race is and how it is perceived. In the constructivist view, <em>race</em> is a socially constructed category that individuals are assigned to.</p>
<p>The challenge with adopting this view is that it is difficult to tease out a set of nodes that faithfully represent the influence that society has on race, and perceptions of race. The social constructivist perspective does not come with a simple operational guide for identifying causal structures. In particular, socially constructed categories often lack the kind of modularity that a causal diagram requires. Suppose that group membership is constructed from a set of social facts about the group and practices of individuals within the group. We might have some understanding of how these facts and practices constitutively identify group membership. But we may not have an understanding of how each factor individually interacts with each other factor, or whether such a decomposition is even possible.<span class="citation" data-cites="cartwright2006hunting"><span><label for="sn-20" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-20" class="margin-toggle" /><span class="sidenote">Cartwright, <em>Hunting Causes and Using Them, Too</em> (Cambridge University Press, 2006).</span></span></span></p>
</section>
<section id="ontological-instability" class="level2">
<h2>Ontological instability</h2>
<p>The previous arguments notwithstanding, pragmatist might accuse our discussion of adding unnecessary complexity to what might seem like a matter of common sense to some. Surely, we could also find subtlety in other characteristics, such as, smoking habits or physical exercise. How is race different from other things we reference in causal models?</p>
<p>An important difference is a matter of ontological stability. When we say <em>rain caused the grass to be wet</em> we also refer to an implicit understanding of what rain is, what grass is, and what wet means. However, we find that acceptable in this instance, because all three things we refer to in our causal statement have <em>stable enough</em> ontologies. We know what we reference when we invoke them. To be sure, there could be subtleties in what we call grass. Perhaps the colloquial term <em>grass</em> does not correspond to a precise botanical category, or one that has changed over time and will again change in the future. However, by making the causal claim, we implicitly assert that these subtleties are irrelevant for the claim we made. We know that grass is a plant and that other plants would also get wet from rain. In short, we believe the ontologies we reference are <em>stable enough</em> for the claim we make.</p>
<p>This is not always an easy judgment to make. There are, broadly speaking, at least two sources of ontological instability. One stems from the fact that the world changes over time. Both social progress, political events, and our own epistemic activities may obsolete theories, create new categories, or disrupt existing ones.<span class="citation" data-cites="mallon2018construction"><span><label for="sn-21" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-21" class="margin-toggle" /><span class="sidenote">Mallon, <em>The Construction of Human Kinds</em>.</span></span></span> Hacking’s work describes another important source of instability. Categories lead people who putatively fall into such categories to change their behavior in possibly unexpected ways. Individuals might conform or disconform to the categories they are confronted with. As a result, the responses of people, individually or collectively, invalidate the theory underlying the categorization. Hacking calls this a “looping effect.”<span class="citation" data-cites="hacking2006making"><span><label for="sn-22" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-22" class="margin-toggle" /><span class="sidenote">Hacking, <span>“Making up People,”</span> <em>London Review of Books</em> 28, no. 16 (2006).</span></span></span> As such, social categories are moving targets that need constant revision.</p>
</section>
<section id="certificates-of-ontological-stability" class="level2">
<h2>Certificates of ontological stability</h2>
<p>The debate around human categories in causal models is by no means new. But it often surfaces in a seemingly unrelated, yet long-standing discussion around causation and manipulation. One school of thought in causal inference aligns with the mantra <em>no causation without manipulation</em>, a view expressed by Holland in an influential article from 1986:</p>
<blockquote>
<p>Put as bluntly and as contentiously as possible, in this article I take the position that causes are only those things that could, in principle, be treatments in experiments.<span class="citation" data-cites="holland1986statistics"><span><label for="sn-23" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-23" class="margin-toggle" /><span class="sidenote">Holland, <span>“Statistics and Causal Inference,”</span> <em>Journal of the American Statistical Association (JASA)</em> 81 (1986): 945–70.</span></span></span></p>
</blockquote>
<p>Holland goes further by arguing that statements involving “attributes” are necessarily statements of association:</p>
<blockquote>
<p>The only way for an attribute to change its value is for the unit to change in some way and no longer be the same unit. Statements of “causation” that involve attributes as “causes” are always statements of association between the values of an attribute and a response variable across the units in a population.<span class="citation" data-cites="holland1986statistics"><span><label for="sn-24" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-24" class="margin-toggle" /><span class="sidenote">Holland.</span></span></span></p>
</blockquote>
<p>To give an example, Holland maintains that the sentence “She did well on the exam because she is a woman” means nothing but “the performance of women on the exam exceeds, in some sense, that of men.”<span class="citation" data-cites="holland1986statistics"><span><label for="sn-25" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-25" class="margin-toggle" /><span class="sidenote">Holland.</span></span></span></p>
<p>If we believed that there is no causation without manipulation, we would have to refrain from including immutable characteristics in causal models altogether. After all, there is by definition no experimental mechanism that turns immutable attributes into treatments.</p>
<p>Holland’s view remains popular among practitioners of the potential outcomes model. The assumptions common in the potential outcomes model are easiest to conceptualize by analogy with a well-designed randomized trial. Practitioners in this framework are therefore used to conceptualizing causes as things that could, in principle, be a treatment in randomized controlled trials.</p>
<p>The desire or need to make causal statements involving race in one way or the other not only arises in the context of discrimination. Epidemiologists encounter the same difficulties when confronting health disparities<span class="citation" data-cites="jackson2018decomposition vanderweele2014on">,<span><label for="sn-26" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-26" class="margin-toggle" /><span class="sidenote">Jackson and VanderWeele, <span>“Decomposition Analysis to Identify Intervention Targets for Reducing Disparities,”</span> <em>Epidemiology</em>, 2018, 825–35; VanderWeele and Robinson, <span>“On Causal Interpretation of Race in Regressions Adjusting for Confounding and Mediating Variables,”</span> <em>Epidemiology</em>, 2014.</span></span></span> as do social scientists when reasoning about inequality in poverty, crime, and education.</p>
<p>Practitioners facing the need of making causal statements about race often turn to a particular conceptual trick. The idea is to change object of study from the <em>effect of race</em> to the effect of <em>perceptions of race</em>.<span class="citation" data-cites="greiner2011causal"><span><label for="sn-27" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-27" class="margin-toggle" /><span class="sidenote">Greiner and Rubin, <span>“Causal Effects of Perceived Immutable Characteristics,”</span> <em>The Review of Economics and Statistics</em> 93, no. 3 (2011): 775–85.</span></span></span> What this boils down to is that we change the units of the study from individuals with a race attribute to <em>decision makers</em>. The treatment becomes <em>exposure to race</em> through some observable trait, like the name on a CV in a job application setting. The target of the study is then how decision makers respond to such <em>racial stimuli</em> in the decision-making process. The hope behind this maneuver is that exposure to race, unlike race itself, may be something that we can control, manipulate, and experiment with.</p>
<p>While this approach superficially avoids the difficulty of conceptualizing manipulation of immutable characteristics, it shifts the burden elsewhere. We now have to sort out all the different ways in which we think that race could possibly be perceived: through names, speech, style, and all sorts of other characteristics and combinations thereof. But not only that. To make a counterfactual statements viz-a-viz <em>exposure to race</em>, we would have to be able to create the authentic background conditions under which all these perceptible characteristics would’ve come out in a manner that’s consistent with a different racial category. There is no way to construct such counterfactuals accurately without a clear understanding of what we mean by the category of race.<span class="citation" data-cites="kohlerhausmann2019eddie">.<span><label for="sn-28" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-28" class="margin-toggle" /><span class="sidenote">Kohler-Hausmann, <span>“Eddie Murphy and the Dangers of Counterfactual Causal Thinking about Detecting Racial Discrimination,”</span> <em>SSRN</em>, 2019.</span></span></span> Just as we cannot talk about witchcraft in a valid causal model for lack of any scientific basis, we also cannot talk about perceptions of witchcraft in a valid causal model for the very same reason. Similarly, if we lack the ontological and epistemic basis for talking about race in a valid causal model, there is no easy remedy to be found in moving to perceptions of race.</p>
<p>In opposition to Holland’s view, other scholars, including Pearl, argue that causation does not require manipulability but rather an understanding of <em>interactions</em>. We can reason about hypothetical Volcano eruptions without being able to manipulate Volcanoes. We can explain the mechanism that causes tides without being able to manipulate the moon by any feasible intervention. What is required is an understanding of the ways in which a variable interacts with other variables in the model. Structural equations in a causal model are <em>response functions</em>. We can think of a node in a causal graph as receiving messages from its parent nodes and responding to those messages. Causality is thus about who <em>listens</em> to whom. We can form a causal model once we know how the nodes in it interact.</p>
<p>But as we saw the conceptual shift to <em>interaction</em>—who <em>listens</em> to whom—by no means makes it straightforward to come up with valid causal models. If causal models organize available scientific or empirical information, there are inevitably limitations to what constructs we can include in a causal model without running danger of divorcing the model from reality. Especially in sociotechnical systems, scientific knowledge may not be available in terms of precise modular response functions.</p>
<p>We take the position that causes need not be experimentally manipulable. However, our discussion motivates that constructs referenced in causal models need a certificate of ontological and epistemic stability. Manipulation can be interpreted as a somewhat heavy-handed approach to clarify the ontological nature of a node by specifying an explicit experimental mechanism for manipulating the node. This is one way, but not the only way, to clarify what it is that the node references.</p>
</section>
</section>
<section id="chapter-notes" class="level1">
<h1>Chapter notes</h1>
<p>There are several introductory textbooks on the topic of causality. For a short introduction to causality turn to the primer by Pearl, Glymour, and Jewell<span class="citation" data-cites="pearl2016causal">,<span><label for="sn-29" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-29" class="margin-toggle" /><span class="sidenote">Pearl, Glymour, and Jewell, <em>Causal Inference in Statistics</em>.</span></span></span> or the more comprehensive textbook by Pearl<span class="citation" data-cites="pearl2009causality">.<span><label for="sn-30" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-30" class="margin-toggle" /><span class="sidenote">Pearl, <em>Causality</em>.</span></span></span> At the technical level, Pearl’s text emphasizes causal graphs and structural causal models. Our exposition of Simpson’s paradox and the UC Berkeley was influenced by Pearl’s discussion, updated for a new popular audience book<span class="citation" data-cites="pearl2018book">.<span><label for="sn-31" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-31" class="margin-toggle" /><span class="sidenote">Pearl and Mackenzie, <em>The Book of Why</em>.</span></span></span> All of these texts touch on the topic of discrimination. In these books, Pearl takes the position that discrimination corresponds to the direct effect of the sensitive category on a decision.</p>
<p>The technically-minded reader will enjoy complementing Pearl’s book with the an open access text by Peters, Janzing, and Schölkopf<span class="citation" data-cites="peters2017elements"><span><label for="sn-32" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-32" class="margin-toggle" /><span class="sidenote">Peters, Janzing, and Schölkopf, <em>Elements of Causal Inference</em>.</span></span></span> that is also <a href="https://mitpress.mit.edu/books/elements-causal-inference">available online</a>. The text emphasizes two variable causal models and applications to machine learning. See Spirtes, Glymour and Scheines<span class="citation" data-cites="spirtes2000causation"><span><label for="sn-33" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-33" class="margin-toggle" /><span class="sidenote">Spirtes et al., <em>Causation, Prediction, and Search</em> (MIT Press, 2000).</span></span></span> for a general introduction based on causal graphs with an emphasis on <em>graph discovery</em>, i.e., inferring causal graphs from observational data.</p>
<p>Morgan and Winship<span class="citation" data-cites="morgan2014counterfactuals"><span><label for="sn-34" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-34" class="margin-toggle" /><span class="sidenote">Morgan and Winship, <em>Counterfactuals and Causal Inference</em> (Cambridge University Press, 2014).</span></span></span> focus on applications in the social sciences. Imbens and Rubin<span class="citation" data-cites="imbens2015causal"><span><label for="sn-35" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-35" class="margin-toggle" /><span class="sidenote">Imbens and Rubin, <em>Causal Inference for Statistics, Social, and Biomedical Sciences</em> (Cambridge University Press, 2015).</span></span></span> give a comprehensive overview of the technical repertoire of causal inference in the potential outcomes model. Angrist and Pischke<span class="citation" data-cites="angrist2009mostly"><span><label for="sn-36" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-36" class="margin-toggle" /><span class="sidenote">Angrist and Jörn-Steffen, <em>Mostly Harmless Econometrics: An Empiricist’s Companion</em> (Princeton University Press, 2009).</span></span></span> focus on causal inference and potential outcomes in econometrics.</p>
<p>Hernan and Robins<span class="citation" data-cites="hernan2019causal"><span><label for="sn-37" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-37" class="margin-toggle" /><span class="sidenote">Hernán and Robins, <em>Causal Inference</em> (Boca Raton: Chapman &amp; Hall/CRC, forthcoming, 2019).</span></span></span> give another detailed introduction to causal inference that draws on the authors’ experience in epidemiology.</p>
<p>Pearl<span class="citation" data-cites="pearl2009causality"><span><label for="sn-38" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-38" class="margin-toggle" /><span class="sidenote">Pearl, <em>Causality</em>.</span></span></span> already considered the example of gender discrimination in UC Berkeley graduate admissions that we discussed at length. In his discussion, he implicitly advocates for a view of discussing discrimination based on the causal graphs by inspecting which paths in the graph go from the sensitive variable to the decision point. The UC Berkeley example has been discussed in various other writings, such as Pearl’s discussion in the Book of Why<span class="citation" data-cites="pearl2018book">.<span><label for="sn-39" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-39" class="margin-toggle" /><span class="sidenote">Pearl and Mackenzie, <em>The Book of Why</em>.</span></span></span> However, the development in this chapter differs significantly in its arguments and conclusions.</p>
<p>For clarifications regarding the popular interpretation of Simpson’s original article<span class="citation" data-cites="simpson1951interpretation">,<span><label for="sn-40" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-40" class="margin-toggle" /><span class="sidenote">Simpson, <span>“The Interpretation of Interaction in Contingency Tables,”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 13, no. 2 (1951): 238–41.</span></span></span> see Hernan’s article<span class="citation" data-cites="hernan2011simpson"><span><label for="sn-41" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-41" class="margin-toggle" /><span class="sidenote">Hernán, Clayton, and Keiding, <span>“<span class="nocase">The Simpson’s paradox unraveled</span>,”</span> <em>International Journal of Epidemiology</em> 40, no. 3 (March 2011): 780–85, <a href="https://doi.org/10.1093/ije/dyr041" role="doc-biblioref">https://doi.org/10.1093/ije/dyr041</a>.</span></span></span> and Pearl’s text<span class="citation" data-cites="pearl2009causality">.<span><label for="sn-42" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-42" class="margin-toggle" /><span class="sidenote">Pearl, <em>Causality</em>.</span></span></span></p>
<p>The topic of causal reasoning and discrimination gained significant momentum in the computer science and statistics community around 2017. Zhang, Wu, and Wu<span class="citation" data-cites="zhang2017causal"><span><label for="sn-43" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-43" class="margin-toggle" /><span class="sidenote">Zhang, Wu, and Wu, <span>“A Causal Framework for Discovering and Removing Direct and Indirect Discrimination,”</span> in <em>Proc. <span class="math inline">26</span>Th <span>IJCAI</span></em>, 2017, 3929–35.</span></span></span> previously considered discrimination analysis via path-specific causal effects. Kusner, Loftus, Russell, and Silva<span class="citation" data-cites="kusner2017counterfactual"><span><label for="sn-44" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-44" class="margin-toggle" /><span class="sidenote">Kusner et al., <span>“Counterfactual Fairness.”</span></span></span></span> introduced a notion of <em>counterfactual fairness</em>. The authors extend this line of thought in another work.<span class="citation" data-cites="russell2017when"><span><label for="sn-45" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-45" class="margin-toggle" /><span class="sidenote">Russell et al., <span>“When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness,”</span> in <em>Advances in Neural Information Processing Systems</em>, 2017, 6417–26.</span></span></span> Chiappa introduces a path-specific notion of counterfactual fairness.<span class="citation" data-cites="chiappa2019path"><span><label for="sn-46" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-46" class="margin-toggle" /><span class="sidenote">Chiappa, <span>“Path-Specific Counterfactual Fairness,”</span> in <em>Proc. <span class="math inline">33</span>Rd AAAI</em>, vol. 33, 2019, 7801–8.</span></span></span> Kilbertus et al.<span class="citation" data-cites="kilbertus2017avoiding"><span><label for="sn-47" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-47" class="margin-toggle" /><span class="sidenote">Kilbertus et al., <span>“Avoiding Discrimination Through Causal Reasoning,”</span> in <em>Advances in Neural Information Processing Systems</em>, 2017, 656–66.</span></span></span> distinguish between two graphical causal criteria, called <em>unresolved discrimination</em> and <em>proxy discrimination</em>. Both notions correspond to either allowing or disallowing paths in causal models. Razieh and Shpitser<span class="citation" data-cites="razieh2018fair"><span><label for="sn-48" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-48" class="margin-toggle" /><span class="sidenote">Nabi and Shpitser, <span>“Fair Inference on Outcomes,”</span> in <em>Proc. <span class="math inline">32</span>Nd <span>AAAI</span></em>, 2018, 1931–40.</span></span></span> conceptualize discrimination as the influence of the sensitive attribute on the outcome along certain <em>disallowed</em> causal paths. Chiappa and Isaac<span class="citation" data-cites="chiappa2019causal"><span><label for="sn-49" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-49" class="margin-toggle" /><span class="sidenote">Chiappa and Isaac, <span>“A Causal Bayesian Networks Viewpoint on Fairness,”</span> <em>Arxiv.org</em> arXiv:1907.06430 (2019).</span></span></span> give a tutorial on causality and fairness with an emphasis on the COMPAS debate. Kasirzadeh and Smart extend on the discussion about the difficulties with constructing causal counterfactual claims about social categories in the context of machine learning problems.<span class="citation" data-cites="kasirzadeh2021use"><span><label for="sn-50" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-50" class="margin-toggle" /><span class="sidenote">Kasirzadeh and Smart, <span>“The Use and Misuse of Counterfactuals in Ethical Machine Learning,”</span> in <em>Conference on Fairness, Accountability, and Transparency</em>, 2021, 228–36.</span></span></span></p>
<p>There is also extensive relevant scholarship in other disciplines that we cannot fully survey here. Of relevance is the vast literature in epidemiology on health disparities. In particular, epidemiologists have grappled with race and gender in causal models. See, for example, the article by VanderWeele and Robinson<span class="citation" data-cites="vanderweele2014on">,<span><label for="sn-51" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-51" class="margin-toggle" /><span class="sidenote">VanderWeele and Robinson, <span>“On Causal Interpretation of Race in Regressions Adjusting for Confounding and Mediating Variables.”</span></span></span></span> as well as Krieger’s comment on the article<span class="citation" data-cites="krieger2014on">,<span><label for="sn-52" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-52" class="margin-toggle" /><span class="sidenote">Krieger, <span>“On the Causal Interpretation of Race,”</span> <em>Epidemiology</em> 25, no. 6 (2014): 937.</span></span></span> and Krieger’s article on discrimination and health inequalities<span class="citation" data-cites="krieger2014discrimination"><span><label for="sn-53" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-53" class="margin-toggle" /><span class="sidenote">Krieger, <span>“Discrimination and Health Inequities,”</span> <em>International Journal of Health Services</em> 44, no. 4 (2014): 643–710.</span></span></span> for a starting point.</p>
<p>We retrieved the data about UC Berkeley admissions from <code>http://www.randomservices.org/random/data/Berkeley.html</code> on Dec 27, 2018. There is some discrepancy with the data displayed on the Wikipedia page for Simpson’s paradox, which does not affect our discussion.</p>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-angrist2009mostly" class="csl-entry" role="doc-biblioentry">
Angrist, Joshua D., and Pischke Jörn-Steffen. <em>Mostly Harmless Econometrics: An Empiricist’s Companion</em>. Princeton University Press, 2009.
</div>
<div id="ref-baron1986moderator" class="csl-entry" role="doc-biblioentry">
Baron, Reuben M, and David A Kenny. <span>“The Moderator–Mediator Variable Distinction in Social Psychological Research: Conceptual, Strategic, and Statistical Considerations.”</span> <em>Journal of Personality and Social Psychology</em> 51, no. 6 (1986): 1173.
</div>
<div id="ref-benjamin2019race" class="csl-entry" role="doc-biblioentry">
Benjamin, Ruha. <em>Race After Technology</em>. Polity, 2019.
</div>
<div id="ref-berkson2014limitations" class="csl-entry" role="doc-biblioentry">
Berkson, Joseph. <span>“Limitations of the Application of Fourfold Table Analysis to Hospital Data.”</span> <em>International Journal of Epidemiology</em> 43, no. 2 (2014): 511–15.
</div>
<div id="ref-bickel1975sex" class="csl-entry" role="doc-biblioentry">
Bickel, Peter J, Eugene A Hammel, J William O’Connell et al. <span>“Sex Bias in Graduate Admissions: Data from Berkeley.”</span> <em>Science</em> 187, no. 4175 (1975): 398–404.
</div>
<div id="ref-bowker2000sorting" class="csl-entry" role="doc-biblioentry">
Bowker, Geoffrey C., and Susan Leigh Star. <em>Sorting Things Out: Classification and Its Consequences</em>. MIT Press, 2000.
</div>
<div id="ref-cartwright2006hunting" class="csl-entry" role="doc-biblioentry">
Cartwright, Nancy. <em>Hunting Causes and Using Them, Too</em>. Cambridge University Press, 2006.
</div>
<div id="ref-chiappa2019path" class="csl-entry" role="doc-biblioentry">
Chiappa, Silvia. <span>“Path-Specific Counterfactual Fairness.”</span> In <em>Proc. <span class="math inline">33</span>Rd AAAI</em>, 33:7801–8, 2019.
</div>
<div id="ref-chiappa2019causal" class="csl-entry" role="doc-biblioentry">
Chiappa, Silvia, and William S. Isaac. <span>“A Causal Bayesian Networks Viewpoint on Fairness.”</span> <em>Arxiv.org</em> arXiv:1907.06430 (2019).
</div>
<div id="ref-deaton2018understanding" class="csl-entry" role="doc-biblioentry">
Deaton, Angus, and Nancy Cartwright. <span>“Understanding and Misunderstanding Randomized Controlled Trials.”</span> <em>Social Science &amp; Medicine</em> 210 (2018): 2–21.
</div>
<div id="ref-egan2020identity" class="csl-entry" role="doc-biblioentry">
Egan, Patrick J. <span>“Identity as Dependent Variable: How Americans Shift Their Identities to Align with Their Politics.”</span> <em>American Journal of Political Science</em> 64, no. 3 (2020): 699–716.
</div>
<div id="ref-fields2014racecraft" class="csl-entry" role="doc-biblioentry">
Fields, Karen E., and Barbara J. Fields. <em>Racecraft: The Soul of Inequality in American Life</em>. Verso, 2014.
</div>
<div id="ref-glasgow2019what" class="csl-entry" role="doc-biblioentry">
Glasgow, Joshua, Sally Haslanger, Chike Jeffers, and Quayshawn Spencer. <span>“What Is Race?: Four Philosophical Views,”</span> 2019.
</div>
<div id="ref-glymour2006using" class="csl-entry" role="doc-biblioentry">
Glymour, M Maria. <span>“Using Causal Diagrams to Understand Common Problems in Social Epidemiology.”</span> <em>Methods in Social Epidemiology</em>, 2006, 393–428.
</div>
<div id="ref-greiner2011causal" class="csl-entry" role="doc-biblioentry">
Greiner, D. James, and Donald B. Rubin. <span>“Causal Effects of Perceived Immutable Characteristics.”</span> <em>The Review of Economics and Statistics</em> 93, no. 3 (2011): 775–85.
</div>
<div id="ref-hacking2006making" class="csl-entry" role="doc-biblioentry">
Hacking, Ian. <span>“Making up People.”</span> <em>London Review of Books</em> 28, no. 16 (2006).
</div>
<div id="ref-hacking2000the" class="csl-entry" role="doc-biblioentry">
———. <em>The Social Construction of What?</em> Harvard University Press, 2000.
</div>
<div id="ref-hall2017predictors" class="csl-entry" role="doc-biblioentry">
Hall, Anna B. AND Cook, Joshua D. AND O’Connell. <span>“Predictors of Student Productivity in Biomedical Graduate School Applications.”</span> <em>PLOS ONE</em> 12, no. 1 (January 2017): 1–14.
</div>
<div id="ref-haslanger2012resisting" class="csl-entry" role="doc-biblioentry">
Haslanger, Sally. <em>Resisting Reality: Social Construction and Social Critique</em>. Oxford University Press, 2012.
</div>
<div id="ref-hernan2011simpson" class="csl-entry" role="doc-biblioentry">
Hernán, Miguel A, David Clayton, and Niels Keiding. <span>“<span class="nocase">The Simpson’s paradox unraveled</span>.”</span> <em>International Journal of Epidemiology</em> 40, no. 3 (March 2011): 780–85. <a href="https://doi.org/10.1093/ije/dyr041">https://doi.org/10.1093/ije/dyr041</a>.
</div>
<div id="ref-hernan2019causal" class="csl-entry" role="doc-biblioentry">
Hernán, Miguel, and James Robins. <em>Causal Inference</em>. Boca Raton: Chapman &amp; Hall/CRC, forthcoming, 2019.
</div>
<div id="ref-holland1986statistics" class="csl-entry" role="doc-biblioentry">
Holland, Paul W. <span>“Statistics and Causal Inference.”</span> <em>Journal of the American Statistical Association (JASA)</em> 81 (1986): 945–70.
</div>
<div id="ref-humphrey2002postmenopausal" class="csl-entry" role="doc-biblioentry">
Humphrey, Linda L., Benjamin K. S. Chan, and Harold C. Sox. <span>“<span class="nocase">Postmenopausal Hormone Replacement Therapy and the Primary Prevention of Cardiovascular Disease</span>.”</span> <em>Annals of Internal Medicine</em> 137, no. 4 (August 2002): 273–84.
</div>
<div id="ref-imbens2015causal" class="csl-entry" role="doc-biblioentry">
Imbens, Guido W., and Donald B. Rubin. <em>Causal Inference for Statistics, Social, and Biomedical Sciences</em>. Cambridge University Press, 2015.
</div>
<div id="ref-jackson2018decomposition" class="csl-entry" role="doc-biblioentry">
Jackson, John W., and Tyler J. VanderWeele. <span>“Decomposition Analysis to Identify Intervention Targets for Reducing Disparities.”</span> <em>Epidemiology</em>, 2018, 825–35.
</div>
<div id="ref-kasirzadeh2021use" class="csl-entry" role="doc-biblioentry">
Kasirzadeh, Atoosa, and Andrew Smart. <span>“The Use and Misuse of Counterfactuals in Ethical Machine Learning.”</span> In <em>Conference on Fairness, Accountability, and Transparency</em>, 228–36, 2021.
</div>
<div id="ref-kilbertus2017avoiding" class="csl-entry" role="doc-biblioentry">
Kilbertus, Niki, Mateo Rojas-Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and Bernhard Schölkopf. <span>“Avoiding Discrimination Through Causal Reasoning.”</span> In <em>Advances in Neural Information Processing Systems</em>, 656–66, 2017.
</div>
<div id="ref-kohlerhausmann2019eddie" class="csl-entry" role="doc-biblioentry">
Kohler-Hausmann, Issa. <span>“Eddie Murphy and the Dangers of Counterfactual Causal Thinking about Detecting Racial Discrimination.”</span> <em>SSRN</em>, 2019.
</div>
<div id="ref-krieger2014discrimination" class="csl-entry" role="doc-biblioentry">
Krieger, Nancy. <span>“Discrimination and Health Inequities.”</span> <em>International Journal of Health Services</em> 44, no. 4 (2014): 643–710.
</div>
<div id="ref-krieger2011epidemiology" class="csl-entry" role="doc-biblioentry">
———. <span>“Epidemiology and the People’s Health: Theory and Context,”</span> 2011.
</div>
<div id="ref-krieger2014on" class="csl-entry" role="doc-biblioentry">
———. <span>“On the Causal Interpretation of Race.”</span> <em>Epidemiology</em> 25, no. 6 (2014): 937.
</div>
<div id="ref-kusner2017counterfactual" class="csl-entry" role="doc-biblioentry">
Kusner, Matt J., Joshua R. Loftus, Chris Russell, and Ricardo Silva. <span>“Counterfactual Fairness.”</span> In <em>Advances in Neural Information Processing Systems</em>, 4069–79, 2017.
</div>
<div id="ref-mallon2018construction" class="csl-entry" role="doc-biblioentry">
Mallon, Ron. <em>The Construction of Human Kinds</em>. Oxford University Press, 2018.
</div>
<div id="ref-moneta-koehler2017the" class="csl-entry" role="doc-biblioentry">
Moneta-Koehler, Abigail M. AND Petrie, Liane AND Brown. <span>“The Limitations of the GRE in Predicting Success in Biomedical Graduate School.”</span> <em>PLOS ONE</em> 12, no. 1 (January 2017): 1–17.
</div>
<div id="ref-morgan2014counterfactuals" class="csl-entry" role="doc-biblioentry">
Morgan, Stephen L., and Christopher Winship. <em>Counterfactuals and Causal Inference</em>. Cambridge University Press, 2014.
</div>
<div id="ref-razieh2018fair" class="csl-entry" role="doc-biblioentry">
Nabi, Razieh, and Ilya Shpitser. <span>“Fair Inference on Outcomes.”</span> In <em>Proc. <span class="math inline">32</span>Nd <span>AAAI</span></em>, 1931–40, 2018.
</div>
<div id="ref-pearl2009causality" class="csl-entry" role="doc-biblioentry">
Pearl, Judea. <em>Causality</em>. Cambridge University Press, 2009.
</div>
<div id="ref-pearl2016causal" class="csl-entry" role="doc-biblioentry">
Pearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. <em>Causal Inference in Statistics: A Primer</em>. Wiley, 2016.
</div>
<div id="ref-pearl2018book" class="csl-entry" role="doc-biblioentry">
Pearl, Judea, and Dana Mackenzie. <em>The Book of Why: The New Science of Cause and Effect</em>. Basic Books, 2018.
</div>
<div id="ref-peters2017elements" class="csl-entry" role="doc-biblioentry">
Peters, Jonas, Dominik Janzing, and Bernhard Schölkopf. <em>Elements of Causal Inference</em>. MIT Press, 2017.
</div>
<div id="ref-russell2017when" class="csl-entry" role="doc-biblioentry">
Russell, Chris, Matt J. Kusner, Joshua R. Loftus, and Ricardo Silva. <span>“When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness.”</span> In <em>Advances in Neural Information Processing Systems</em>, 6417–26, 2017.
</div>
<div id="ref-simpson1951interpretation" class="csl-entry" role="doc-biblioentry">
Simpson, Edward H. <span>“The Interpretation of Interaction in Contingency Tables.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 13, no. 2 (1951): 238–41.
</div>
<div id="ref-spirtes2000causation" class="csl-entry" role="doc-biblioentry">
Spirtes, Peter, Clark N Glymour, Richard Scheines, David Heckerman, Christopher Meek, Gregory Cooper, and Thomas Richardson. <em>Causation, Prediction, and Search</em>. MIT Press, 2000.
</div>
<div id="ref-vanderweele2014on" class="csl-entry" role="doc-biblioentry">
VanderWeele, Tyler J., and Whitney R. Robinson. <span>“On Causal Interpretation of Race in Regressions Adjusting for Confounding and Mediating Variables.”</span> <em>Epidemiology</em>, 2014.
</div>
<div id="ref-zhang2018fairness" class="csl-entry" role="doc-biblioentry">
Zhang, Junzhe, and Elias Bareinboim. <span>“Fairness in Decision-Making — the Causal Explanation Formula.”</span> In <em>Proc. <span class="math inline">32</span>Nd <span>AAAI</span></em>, 2018.
</div>
<div id="ref-zhang2017causal" class="csl-entry" role="doc-biblioentry">
Zhang, Lu, Yongkai Wu, and Xintao Wu. <span>“A Causal Framework for Discovering and Removing Direct and Indirect Discrimination.”</span> In <em>Proc. <span class="math inline">26</span>Th <span>IJCAI</span></em>, 3929–35, 2017.
</div>
</div>
</section>

<div id="lastupdate">
Last updated: Tue Nov 15 16:35:02 CET 2022
</div>
</article>


<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>

</body>
</html>
